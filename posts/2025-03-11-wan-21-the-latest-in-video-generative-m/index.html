<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Wan 21 The Latest in Video Generative Models</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Wan 21 The Latest in Video Generative Models</h1>
			<b><time>11.03.2025 16:46</time></b>
		       

			<div>
				<h1 id="wan-21-the-latest-in-video-generative-models">Wan 21 The Latest in Video Generative Models</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Video generative models have made tremendous strides recently. Although advancements in language modeling are impressive, with their capacity to tackle more intricate tasks, generating realistic videos poses a unique challenge. As humans, our brains have evolved over millions of years to instinctively detect even the slightest visual inconsistencies, making realistic video generation a remarkably complex task. In a previous article, we discussed <a href="/community/tutorials/hunyuanvideo-gpu-droplets">HunyuanVideo</a>, a leading open-source vision language model that has caught up to impressive closed source models like <a href="https://openai.com/sora/">Sora</a> and <a href="https://deepmind.google/technologies/veo/veo-2/">Veo 2</a>.</p>
<p>Beyond the typical use case of entertainment, areas of increased research interest for video generation models include <a href="https://phys.org/news/2025-01-video-generative-molecular-world.html">predicting protein folding dynamics</a> and modeling <a href="https://openai.com/index/video-generation-models-as-world-simulators/">real-world environments</a> for embodied intelligence (ex: robotics, self-driving cars). Advances in video generation models could be instrumental for scientific research and our ability to develop complex physical systems.</p>
<h2 id="introducing-wan-21"><a href="#introducing-wan-2-1">Introducing Wan 2.1</a><a href="#introducing-wan-2-1"></a></h2>
<p>On February 26th, 2025, a collection of open-source video foundation models Wan 2.1 were released. The series consists of four distinct models divided into two categories: text-to-video and image-to-video. The text-to-video category includes <a href="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B">T2V-14B</a> and <a href="https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B">T2V-1.3B</a>, while the image-to-video category features <a href="https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P">I2V-14B-720P</a> and <a href="https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P">I2V-14B-480P</a>. These models range in size from 1.3 billion to 14 billion parameters. The larger 14B model particularly excels in scenarios requiring high motion, producing videos at 720p resolution with realistic physics. Meanwhile, the smaller 1.3B model offers an excellent compromise between quality and efficiency, allowing users to generate 480p videos on standard hardware in approximately four minutes.</p>
<p>On February 27th, 2025, Wan2.1 was <a href="https://comfyanonymous.github.io/ComfyUI_examples/wan/">integrated</a> into <a href="https://www.comfy.org/">ComfyUI</a>, an open source node-based interface for creating images, videos, and audio with GenAI, and on March 3rd, 2025, Wan2.1’s <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/wan/pipeline_wan.py">T2V</a> and <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/wan/pipeline_wan_i2v.py">I2V</a> were integrated into <a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/wan">Diffusers</a>, a popular Python library developed by Hugging Face that provides tools and implementations for diffusion models.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/p2svsefficiency.png" alt="peak signal-to-noise ratio (PSNR) vs. efficiency" />
</figure>


 In this figure, we can see that with fewer parameters, Wan-VAE achieves a higher efficiency (frame/latency) and a comparable peak signal-to-noise ratio (PSNR) as Hunyuan video.</p>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<p>There are two parts to this tutorial. (1) An overview covering the model architecture and training methodology and (2) an implementation where we run the model. Note that the overview section of this article might receive an update once Wan 2.1’s full technical report is released. For the first part of the tutorial, an understanding of deep learning fundamentals is critical for following along with the theory. Some exposure to concepts discussed in this tutorial may be helpful (ex: autoencoders, diffusion transformers, flow matching). To complete the second part of this tutorial, a GPU is required. If you don’t have access to a GPU, consider <a href="https://cloud.digitalocean.com/registrations/new">signing up</a> for a DigitalOcean account to utilize a GPU Droplet. Feel free to skip the overview section if you’re only interested in implementing Wan 2.1.</p>
<h2 id="overview"><a href="#overview">Overview</a><a href="#overview"></a></h2>
<h3 id="a-refresher-on-autoencoders"><a href="#a-refresher-on-autoencoders">A Refresher on Autoencoders</a><a href="#a-refresher-on-autoencoders"></a></h3>
<p>An <a href="https://www.deeplearningbook.org/contents/autoencoders.html">Autoencoder</a> is a neural network designed to replicate its input as its output. For instance, an autoencoder can convert a handwritten digit image into a compact, lower-dimensional representation known as a <strong>latent representation</strong>, then reconstruct the original image. Through this process, it <strong>learns</strong> to compress data efficiently while minimizing errors in reconstructing an image. <a href="https://arxiv.org/pdf/1906.02691">Variational Autoencoders (VAEs)</a>, on the other hand, encode data into a continuous, probabilistic latent space rather than a fixed, discrete representation as with traditional autoencoders. This allows for the generation of new, diverse data samples and smooth interpolation between them, critical for tasks like image and video generation.</p>
<h3 id="a-refresher-on-causal-convolutions"><a href="#a-refresher-on-causal-convolutions">A Refresher on Causal Convolutions</a><a href="#a-refresher-on-causal-convolutions"></a></h3>
<p><a href="https://paperswithcode.com/method/causal-convolution">Causal convolutions</a> are a type of convolution specifically designed for temporal data, ensuring that the model’s predictions at any given timestep t are only dependent on past timesteps (t-1, t-2, …) and not on any future timesteps (t+1, t+2, …).</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/standard%20vs%20causal%20convolution.png" alt="Standard vs Causal Convolution" />
</figure>


 <a href="https://www.researchgate.net/publication/350311379_MoViNets_Mobile_Video_Networks_for_Efficient_Video_Recognition">(Source)</a> Standard (left) vs. Causal (right) Convolutions</p>
<p>Causal convolutions are applied in different dimensions to various data types.</p>
<ul>
<li>1D: Audio</li>
<li>2D: Image</li>
<li>3D: Video</li>
</ul>
<h3 id="wan-vae-a-3d-causal-variational-autoencoder"><a href="#wan-vae-a-3d-causal-variational-autoencoder">Wan-VAE: A 3D Causal Variational Autoencoder</a><a href="#wan-vae-a-3d-causal-variational-autoencoder"></a></h3>
<p>A 3D Causal Variational Autoencoder, as implemented with Wan 2.1, is an advanced type of VAE that incorporates 3D causal convolutions, allowing it to handle <strong>both spatial and temporal dimensions</strong> in <strong>video sequences</strong>.</p>
<p>This novel 3D causal VAE architecture, termed <strong>Wan-VAE</strong>, can efficiently encode and decode 1080P videos of unlimited length while preserving historical temporal information, making it suitable for video generation tasks.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/wan%20vae.png" alt="WAN-VAE" />
</figure>


</p>
<h3 id="feature-cache-and-chunking"><a href="#feature-cache-and-chunking">Feature Cache and Chunking</a><a href="#feature-cache-and-chunking"></a></h3>
<p>Processing long videos in one pass can lead to GPU memory overflow due to high-resolution frame data and temporal dependencies. Thus, the causal convolution module has a feature cache mechanism to provide historical data without retaining full video in memory. Here, video sequence frames are structured in a “1 + T” input format (1 initial frame + T subsequent frames), dividing the video into “1 + T/4” chunks.</p>
<p>For example: A 17-frame video (T=16) becomes 1 + 16/4 = 5 chunks.</p>
<p>Each encoding/decoding operation processes a single video chunk at a time, which corresponds to a single latent representation. To reduce the risk of GPU memory overflow, the number of frames in each processing chunk is limited to a maximum of 4. This frame limit is determined by the temporal compression ratio, which measures the compression of the time dimension.</p>
<h3 id="text-to-video-t2v-architecture"><a href="#text-to-video-t2v-architecture">Text-to-Video (T2V) Architecture</a><a href="#text-to-video-t2v-architecture"></a></h3>
<p>The T2V models generate videos from text prompts.</p>
<p>Component</p>
<p>Description</p>
<p>Diffusion Transformers (DiT) + Flow Matching 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/dit.png" alt="dit" />
</figure>


</p>
<p>Wan AI 2.1 is built on the principles of mainstream Diffusion Transformers, incorporating the Flow Matching framework to achieve advanced capabilities. Diffusion Transformers (DiTs) specifically refer to transformer architectures applied to diffusion models, which are models that generate data by adding and then removing noise from training data. <a href="https://arxiv.org/pdf/2210.02747">Flow Matching</a> is where a neural network is directly trained to predict smooth, continuous transformations between simple and complex data distributions. This results in more stable training, faster inference, and improved performance compared to conventional diffusion-based approaches.</p>
<p>T5 Encoder and Cross-Attention for Text Processing 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/cross%20attention.png" alt="T5 and cross attention" />
</figure>


</p>
<p>Wan2.1 employs the <a href="https://huggingface.co/docs/transformers/model_doc/t5">T5 (Text-To-Text Transfer Transformer)</a> Encoder (<a href="https://huggingface.co/docs/transformers/v4.34.0/model_doc/umt5">UMT5</a>) to embed text into the vision model. Within each transformer, a <a href="https://medium.com/@sachinsoni600517/cross-attention-in-transformer-f37ce7129d78">cross-attention</a> mechanism is utilized. This attention variant enhances the model’s ability to process multilingual text inputs (English and Chinese) and align the text prompts with visual outputs.</p>
<p>Time Embeddings 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/timestep.png" alt="Time embeddings" />
</figure>


</p>
<p>Time embeddings in Wan 2.1 serve as numerical representations that capture temporal patterns in data. These embeddings are processed through a shared MLP (consisting of a Linear layer and SiLU activation) used across all transformer blocks. The shared MLP reduces parameter count and boosts efficiency, while individual transformer blocks develop unique biases for specialized processing. This dual approach allows for both parameter efficiency and specialized processing, as each block can focus on different aspects of the input data, enhancing the model’s overall performance.</p>
<h3 id="image-2-video-i2v-architecture"><a href="#image-2-video-i2v-architecture">Image-2-Video (I2V) Architecture</a><a href="#image-2-video-i2v-architecture"></a></h3>
<p>The I2V models generate videos from images using text prompts.</p>
<p>Component</p>
<p>Description</p>
<p><strong>Condition Image</strong></p>
<p>Video synthesis is controlled with a condition image as the first frame</p>
<p><strong>Guidance Frames</strong></p>
<p>Frames filled with zeros (guidance frames) are concatenated with the previously generated condition image along the temporal axis</p>
<p><strong>Condition Latent Representation</strong></p>
<p>A 3D VAE is used to compress the guidance frames into a condition latent representation</p>
<p><strong>Binary Mask</strong></p>
<p>A binary mask is added (1 for preserved frames, 0 for frames to generate). This mask, spatially aligned with the condition latent representation, extends temporally to match the target video’s length</p>
<p><strong>Mask Rearrangement</strong></p>
<p>The binary mask is then reshaped to align with the VAE’s temporal stride, ensuring seamless integration with the latent representation</p>
<p><strong>DiT Model Input</strong></p>
<p>The noise latent representation, condition latent representation, and the rearranged binary mask are combined by concatenating them along the channel axis. This combined input is then fed into the DiT model</p>
<p><strong>Channel Projection</strong></p>
<p>Due to the increased channel count compared to T2V models, a supplementary projection layer, initialized with zeros, is implemented to adapt the input for the I2V DiT model</p>
<p><strong>CLIP Image Encoder</strong></p>
<p>A CLIP image encoder extracts feature representations from the condition image, capturing its visual essence</p>
<p><strong>Global Context MLP</strong></p>
<p>These extracted features are projected by a three-layer MLP, generating a global context that encapsulates the image’s overall information</p>
<p><strong>Decoupled Cross-Attention</strong></p>
<p>This global context is then injected into the DiT model via decoupled cross-attention, allowing the model to leverage the condition image’s features throughout the video generation process</p>
<h2 id="implementation"><a href="#implementation">Implementation</a><a href="#implementation"></a></h2>
<p>Wan 2.1 offers flexible implementation <a href="https://github.com/Wan-Video/Wan2.1?tab=readme-ov-file#quickstart">options</a>. In this tutorial, we’ll utilize Comfy UI to showcase a seamless way to run the Wan 2.1 I2V model. Before following along with this tutorial, set up a <a href="/products/gpu-droplets">GPU Droplet</a> and find a picture you’d like to convert into a video.</p>
<p>For optimal performance, we recommend selecting an “AI/ML Ready” OS and utilizing a single NVIDIA H100 GPU for this project. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/ai-ml.png" alt="choose an image - ai/ml" />
</figure>


</p>
<h4 id="step-0-install-python-and-pip">Step 0: Install Python and Pip</h4>
<pre tabindex="0"><code>apt install python3-pip
</code></pre><h4 id="step-1-install-comfyui">Step 1: Install ComfyUI</h4>
<pre tabindex="0"><code>pip install comfy-cli
comfy install
</code></pre><p>Select nvidia when prompted “What GPU do you have?”</p>
<h4 id="step-2-download-the-necessary-models">Step 2: Download the necessary models</h4>
<pre tabindex="0"><code>cd comfy/ComfyUI/models
wget -P diffusion_models https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp8_e4m3fn.safetensors
wget -P text_encoders https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors
wget -P clip_vision https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors
wget -P vae https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors
</code></pre><h4 id="step-3-launch-comfyui">Step 3: Launch ComfyUI</h4>
<pre tabindex="0"><code>comfy launch
</code></pre><p>You’ll see a URL in the console output. You’ll need this URL in a later step to access the GUI. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/gui%20link.png" alt="gui link" />
</figure>


</p>
<h4 id="step-4-open-vscode">Step 4: Open VSCode</h4>
<p>In VSCode, click on “Connect to…” in the Start menu. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/connect%20to.png" alt="connect" />
</figure>


</p>
<p>Choose “Connect to Host…”. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/host.png" alt="host" />
</figure>


</p>
<h4 id="step-5-connect-to-your-gpu-droplet">Step 5: Connect to your GPU Droplet</h4>
<p>Click “Add New SSH Host…” and enter the SSH command to connect to your droplet. This command is usually in the format ssh root@[your_droplet_ip_address]. Press Enter to confirm, and a new VSCode window will open, connected to your droplet.</p>
<p>You can find your droplet’s IP address on the GPU droplet page. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/ip.png" alt="gpu droplet IP" />
</figure>


</p>
<h4 id="step-6-access-the-comfyui-gui">Step 6: Access the ComfyUI GUI</h4>
<p>In the new VSCode window connected to your droplet, type &gt;sim and select “Simple Browser: Show”. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/show%20simple.png" alt="show simple browser" />
</figure>


</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/simple%20browser.png" alt="paste simple browser" />
</figure>


</p>
<p>Copy the ComfyUI GUI URL from your web console (from Step 3) and paste it into the Simple Browser. Press Enter, and the ComfyUI GUI will open. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/gui%20window.png" alt="GUI window" />
</figure>


</p>
<h4 id="step-7-update-the-comfyui-manager">Step 7: Update the ComfyUI Manager</h4>
<p>Click the Manager button in the top right corner. In the menu that pops up, click Update ComfyUI. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/update%20comfy%20ui.png" alt="update manager" />
</figure>


 You’ll be prompted to restart ComfyUI. Click “Restart” and refresh your browser if needed.</p>
<h4 id="step-8-load-a-workflow">Step 8: Load a Workflow</h4>
<p>Download the <a href="https://comfyanonymous.github.io/ComfyUI_examples/wan/">workflow</a> of your choice in Json format (here, we’re using the I2V workflow). 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/Workflow.png" alt="workflow" />
</figure>


</p>
<h4 id="step-9-install-missing-nodes">Step 9: Install Missing Nodes</h4>
<p>If you’re working with a workflow that requires additional nodes, you might encounter a “Missing Node Types” error. Go to “Manager” &gt; “Install missing custom nodes” and install the latest verisions of the required nodes. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/install%20missing%20custom%20nodes.png" alt="missing nodes" />
</figure>


 You’ll be prompted to restart ComfyUI. Click “Restart” and refresh your browser if needed.</p>
<h4 id="step-10-upload-an-image">Step 10: Upload an Image</h4>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/upload%20image.png" alt="upload image" />
</figure>


</p>
<h4 id="step-11-add-prompts">Step 11: Add Prompts</h4>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/prompts.png" alt="prompt" />
</figure>


</p>
<p><strong>Positive Prompt vs. Negative Prompt</strong> A “positive prompt” tells a model what to include in its generated output, essentially guiding it towards a specific desired element, while a “negative prompt” instructs the model what to exclude or avoid, acting as a filter to refine the content by removing unwanted aspects.</p>
<p>We will be using the following prompts to get our character to wave: Positive Prompt <em>“A portrait of a seated man, his gaze engaging the viewer with a gentle smile. One hand rests on a wide-brimmed hat in his lap, while the other lifts in a gesture of greeting.”</em></p>
<p>Negative Prompt <em>“No blurry face, no distorted hands, no extra limbs, no missing limbs, no floating hat”</em></p>
<h4 id="step-12-run-the-workflow">Step 12: Run the Workflow</h4>
<p>To run the workflow, select Queue. If you run into errors, ensure the correct files are passed into the nodes. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/queue.png" alt="queue" />
</figure>


</p>
<p>Would you look at that - we got our character to wave at us. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wan2.1/wave.png" alt="wave" />
</figure>


</p>
<p>Feel free to play around with the different parameters to see how performance is altered.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a><a href="#conclusion"></a></h2>
<p>Great work! In this tutorial, we explored the model architecture of Wan 2.1, a cutting-edge collection of video generative models. We also successfully implemented Wan 2.1’s image-to-video model using ComfyUI. This achievement underscores the rapid advancement of open-source video generation models, foreshadowing a future where AI-generated video becomes an integral tool in various industries, including media production, scientific research, and digital prototyping.</p>
<h2 id="references"><a href="#references">References</a><a href="#references"></a></h2>
<p><a href="https://www.wan-ai.org/about">https://www.wan-ai.org/about</a></p>
<p><a href="https://github.com/Wan-Video/Wan2.1">https://github.com/Wan-Video/Wan2.1</a></p>
<p><a href="https://stable-diffusion-art.com/wan-2-1/">https://stable-diffusion-art.com/wan-2-1/</a></p>
<h4 id="additional-resources">Additional Resources</h4>
<p><a href="https://www.youtube.com/watch?v=7NNxK3CqaDk">Flow Matching for Generative Modeling (Paper Explained)</a></p>
<p><a href="https://arxiv.org/abs/2210.02747">[2210.02747] Flow Matching for Generative Modeling</a></p>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/wan-video-foundation-models">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
