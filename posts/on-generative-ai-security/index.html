<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>&lt;div&gt;On Generative AI Security&lt;/div&gt;</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>&lt;div&gt;On Generative AI Security&lt;/div&gt;</h1>
			<b><time>06.02.2025 00:00</time></b>
		       

			<div>
				<p>Microsoft’s AI Red Team just published “Lessons from<br>
Red Teaming 100 Generative AI Products.” Their blog post lists “three takeaways,” but the eight lessons in the report itself are more useful:</p>
<blockquote>
<ol>
<li>Understand what the system can do and where it is applied.</li>
<li>You don’t have to compute gradients to break an AI system.</li>
<li>AI red teaming is not safety benchmarking.</li>
<li>Automation can help cover more of the risk landscape.</li>
<li>The human element of AI red teaming is crucial.</li>
<li>Responsible AI harms are pervasive but difficult to measure.</li>
<li>LLMs amplify existing security risks and introduce new ones&hellip;</li>
</ol></blockquote>
<p>Microsoft’s AI Red Team just published “Lessons from<br>
Red Teaming 100 Generative AI Products.” Their blog post lists “three takeaways,” but the eight lessons in the report itself are more useful:</p>
<blockquote>
<ol>
<li>Understand what the system can do and where it is applied.</li>
<li>You don’t have to compute gradients to break an AI system.</li>
<li>AI red teaming is not safety benchmarking.</li>
<li>Automation can help cover more of the risk landscape.</li>
<li>The human element of AI red teaming is crucial.</li>
<li>Responsible AI harms are pervasive but difficult to measure.</li>
<li>LLMs amplify existing security risks and introduce new ones.</li>
<li>The work of securing AI systems will never be complete.</li>
</ol></blockquote>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/a-visual-summary-of-sans-new2cyber-summit-2025/">A Visual Summary of SANS New2Cyber Summit 2025</a></li>
				
				<li><a href="/posts/advance-your-cybersecurity-career-how-to-use-tuition-reimbursement-for-sec406/">Advance Your Cybersecurity Career: How to Use Tuition Reimbursement for SEC406</a></li>
				
				<li><a href="/posts/continuous-penetration-testing-a-consultants-perspective/">Continuous Penetration Testing - A Consultant’s Perspective</a></li>
				
				<li><a href="/posts/desktop-4-39-smarter-ai-agent-docker-desktop-cli-in-ga-and-effortless-multi-platform-builds/">Desktop 4.39: Smarter AI Agent, Docker Desktop CLI in GA, and Effortless Multi-Platform Builds</a></li>
				
				<li><a href="/posts/docker-engine-v28-hardening-container-networking-by-default/">Docker Engine v28: Hardening Container Networking by Default</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
