<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Image Processing Using Llama 32 with Hugging Face Transformers</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Image Processing Using Llama 32 with Hugging Face Transformers</h1>
			<b><time>19.03.2025 00:00</time></b>
		       

			<div>
				<h1 id="image-processing-using-llama-32-with-hugging-face-transformers">Image Processing Using Llama 32 with Hugging Face Transformers</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Extracting insights from images has long been a challenge across industries like finance, healthcare, and law. Traditional methods, such as <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">Optical Character Recognition (OCR)</a>, have struggled with complex layouts and contextual understanding.</p>
<p><a href="https://marketplace.digitalocean.com/apps/meta-llama-llama-3-2-90b-vision-instruct-8x">Llama 3.2 Vision</a>, an advanced AI model, enhances image processing capabilities like Visual Question Answering and OCR. By integrating this model with <a href="https://cloud.digitalocean.com/">DigitalOcean’s cloud infrastructure</a>, this tutorial provides a scalable and efficient way to implement <a href="/community/tutorials/arithmetic-bitwise-and-masking-python-opencv">AI-powered image processing</a>.</p>
<p>In this tutorial, you will learn to set up Llama 3.2 Vision with DigitalOcean’s cloud infrastructure, and demonstrate how to use it for AI-powered image processing for extracting employee IDs and names from images. We will cover the installation and configuration steps, as well as provide examples of how to use the model for Visual Question Answering and OCR. By the end of this tutorial, you will have a solid understanding of how to leverage Llama 3.2 Vision for your image processing needs.</p>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<p>Before proceeding, ensure you have:</p>
<ul>
<li>A <a href="https://docs.digitalocean.com/products/droplets/how-to/gpu/">DigitalOcean GPU Droplet</a> with <a href="/community/tutorials/how-to-install-python-3-and-set-up-a-programming-environment-on-an-ubuntu-20-04-server">Python 3.10+</a> installed.</li>
<li>A <a href="https://docs.digitalocean.com/products/spaces/how-to/create/">DigitalOcean Spaces account</a> with an access key and secret key.</li>
<li>A <a href="https://docs.digitalocean.com/products/databases/mysql/">DigitalOcean Managed MySQL</a> database.</li>
<li>A <a href="https://huggingface.co/docs/hugs/en/how-to/cloud/digital-ocean">HuggingFace Token</a>.</li>
</ul>
<h2 id="step-1---set-up-the-environment"><a href="#step-1-set-up-the-environment">Step 1 - Set Up the Environment</a><a href="#step-1-set-up-the-environment"></a></h2>
<h3 id="ssh-into-your-gpu-droplet"><a href="#ssh-into-your-gpu-droplet">SSH into Your GPU Droplet</a><a href="#ssh-into-your-gpu-droplet"></a></h3>
<p>Connect to your server via SSH:</p>
<pre tabindex="0"><code>ssh root@your_server_ip
</code></pre><h3 id="install-python--create-a-virtual-environment"><a href="#install-python-create-a-virtual-environment">Install Python &amp; Create a Virtual Environment</a><a href="#install-python-create-a-virtual-environment"></a></h3>
<p>Run the following commands to set up a Python virtual environment:</p>
<pre tabindex="0"><code>apt install python3.10-venv -y
python3.10 -m venv llama-env
</code></pre><h3 id="activate-the-virtual-environment"><a href="#activate-the-virtual-environment">Activate the Virtual Environment</a><a href="#activate-the-virtual-environment"></a></h3>
<pre tabindex="0"><code>source llama-env/bin/activate
</code></pre><h2 id="step-2---install-required-dependencies"><a href="#step-2-install-required-dependencies">Step 2 - Install Required Dependencies</a><a href="#step-2-install-required-dependencies"></a></h2>
<h3 id="install-pytorch--hugging-face-cli"><a href="#install-pytorch-hugging-face-cli">Install PyTorch &amp; Hugging Face CLI</a><a href="#install-pytorch-hugging-face-cli"></a></h3>
<pre tabindex="0"><code>pip install torch torchvision torchaudio
pip install -U &#34;huggingface_hub[cli]&#34;
huggingface-cli login
</code></pre><h3 id="install-the-transformers-library"><a href="#install-the-transformers-library">Install the Transformers Library</a><a href="#install-the-transformers-library"></a></h3>
<pre tabindex="0"><code>pip install --upgrade transformers
</code></pre><h3 id="install-flask--aws-sdk-boto3"><a href="#install-flask-aws-sdk-boto3">Install Flask &amp; AWS SDK (Boto3)</a><a href="#install-flask-aws-sdk-boto3"></a></h3>
<p><a href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html">Boto3</a> is required to interact with <strong><a href="https://docs.digitalocean.com/products/spaces/how-to/create/">DigitalOcean Spaces</a></strong>, which is S3-compatible.</p>
<pre tabindex="0"><code>pip install flask boto3
</code></pre><h3 id="install-mysql-connector-for-python"><a href="#install-mysql-connector-for-python">Install MySQL Connector for Python</a><a href="#install-mysql-connector-for-python"></a></h3>
<pre tabindex="0"><code>pip install mysql-connector-python
</code></pre><h2 id="step-3---install--configure-nginx"><a href="#step-3-install-configure-nginx">Step 3 - Install &amp; Configure Nginx</a><a href="#step-3-install-configure-nginx"></a></h2>
<p>Install <strong><a href="https://nginx.org/">Nginx</a></strong> to serve your Flask application:</p>
<pre tabindex="0"><code>sudo apt install nginx -y
</code></pre><h2 id="step-4---set-up-the-flask-web-application"><a href="#step-4-set-up-the-flask-web-application">Step 4 - Set Up the Flask Web Application</a><a href="#step-4-set-up-the-flask-web-application"></a></h2>
<h3 id="application-folder-structure"><a href="#application-folder-structure">Application Folder Structure</a><a href="#application-folder-structure"></a></h3>
<p>Organize your project as follows:</p>
<pre tabindex="0"><code>llama-webapp/
├── app.py                # Main Flask app file
├── static/
│   └── styles.css        # Optional: CSS file for styling
└── templates/
    └── index.html        # HTML template for the web page
</code></pre><h3 id="python-code-for-the-application"><a href="#python-code-for-the-application">Python Code for the Application</a><a href="#python-code-for-the-application"></a></h3>
<p>Below is the Flask app (<code>app.py</code>) that loads the <strong><a href="https://marketplace.digitalocean.com/apps/meta-llama-llama-3-2-90b-vision-instruct-8x">Llama 3.2 model</a></strong>, processes uploaded images, and extracts employee details.</p>
<pre tabindex="0"><code>import os
import json
import requests
from PIL import Image
from flask import Flask, request, render_template, session
from transformers import MllamaForConditionalGeneration, AutoProcessor
import boto3
import torch
import mysql.connector
import re

# Initialize Flask app
app = Flask(__name__)
app.secret_key = &#34;your_secret_key&#34;  # Replace with a strong secret key

# Load the Llama 3.2 model and processor
model_id = &#34;meta-llama/Llama-3.2-11B-Vision-Instruct&#34;
model = MllamaForConditionalGeneration.from_pretrained(
    model_id, torch_dtype=torch.bfloat16, device_map=&#34;auto&#34;
)
processor = AutoProcessor.from_pretrained(model_id)

# DigitalOcean Spaces setup
SPACE_NAME = &#34;your_space_name&#34;
SPACE_REGION = &#34;your_region&#34;
ACCESS_KEY = &#34;your_access_key&#34;
SECRET_KEY = &#34;your_secret_key&#34;

s3 = boto3.client(
    &#34;s3&#34;,
    region_name=SPACE_REGION,
    endpoint_url=f&#34;https://{SPACE_REGION}.digitaloceanspaces.com&#34;,
    aws_access_key_id=ACCESS_KEY,
    aws_secret_access_key=SECRET_KEY
)

# DigitalOcean Managed MySQL setup
DB_HOST = &#34;your_mysql_host&#34;
DB_PORT = 25060
DB_NAME = &#34;your_database_name&#34;
DB_USER = &#34;your_username&#34;
DB_PASSWORD = &#34;your_password&#34;

# Function to establish a database connection
def get_db_connection():
    try:
        conn = mysql.connector.connect(
            host=DB_HOST, port=DB_PORT, database=DB_NAME, user=DB_USER, password=DB_PASSWORD
        )
        print(&#34;✅ Database connection successful!&#34;)
        return conn
    except Exception as e:
        print(f&#34;❌ Error connecting to the database: {e}&#34;)
        return None

# Function to extract Employee ID &amp; Name from an image
def extract_employee_details(image_path):
    &#34;&#34;&#34;Extracts Employee Name and ID using Llama 3.2 Vision AI.&#34;&#34;&#34;
    try:
        image = Image.open(image_path)
        prompt = (
            &#34;Extract the Employee ID and Name from the given image. &#34;
            &#34;Provide output in valid JSON format with keys: &#39;employee_id&#39; and &#39;employee_name&#39;.&#34;
        )
        messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [{&#34;type&#34;: &#34;image&#34;}, {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: prompt}]}]

        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)
        inputs = processor(image, input_text, return_tensors=&#34;pt&#34;).to(model.device)

        output = model.generate(**inputs, max_new_tokens=1024)
        raw_result = processor.decode(output[0])

        json_match = re.search(r&#34;\{.*?\}&#34;, raw_result, re.DOTALL)
        extracted_data = json.loads(json_match.group(0)) if json_match else {}
        employee_id = extracted_data.get(&#34;employee_id&#34;, &#34;&#34;).strip()
        employee_name = extracted_data.get(&#34;employee_name&#34;, &#34;&#34;).strip()

        return employee_id, employee_name
    except Exception as e:
        return None, None

# Flask Routes
@app.route(&#34;/&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;])
def index():
    &#34;&#34;&#34;Handles image uploads, extracts Employee Name &amp; ID, and stores it in MySQL.&#34;&#34;&#34;
    result = None
    image_url = session.get(&#34;image_url&#34;)

    if request.method == &#34;POST&#34;:
        image_file = request.files.get(&#34;image&#34;)
        if not image_file:
            return &#34;Error: Please upload an image.&#34;, 400
        filename = image_file.filename
        image_path = os.path.join(&#34;/tmp&#34;, filename)
        image_file.save(image_path)

        employee_id, employee_name = extract_employee_details(image_path)
        return render_template(&#34;index.html&#34;, result=result, image_url=image_url)

if __name__ == &#34;__main__&#34;:
    app.run(host=&#34;0.0.0.0&#34;, port=5000)
</code></pre><h2 id="step-5-run--access-the-application"><a href="#step-5-run-access-the-application"><strong>Step 5: Run &amp; Access the Application</strong></a><a href="#step-5-run-access-the-application"></a></h2>
<ol>
<li>
<p>Start the Flask application:</p>
<pre tabindex="0"><code>python app.py
</code></pre></li>
<li>
<p>Open your browser and visit:</p>
<pre tabindex="0"><code>http://your_server_ip:5000
</code></pre></li>
<li>
<p>Upload an image, extract employee details, and verify data storage in the database.</p>
</li>
</ol>
<h2 id="faqs"><a href="#faqs">FAQs</a><a href="#faqs"></a></h2>
<h3 id="1-what-is-llama-32-and-how-does-it-differ-from-previous-versions"><a href="#1-what-is-llama-3-2-and-how-does-it-differ-from-previous-versions">1. What is Llama 3.2, and how does it differ from previous versions?</a><a href="#1-what-is-llama-3-2-and-how-does-it-differ-from-previous-versions"></a></h3>
<p><a href="https://marketplace.digitalocean.com/apps/meta-llama-llama-3-2-90b-vision-instruct-8x">Llama 3.2</a> is a state-of-the-art AI model developed by <a href="https://www.meta.com/">Meta (Facebook)</a> that builds upon its predecessor, <a href="https://ai.meta.com/blog/meta-llama-3/">Llama 3</a>. It offers improved natural language understanding, better performance in multimodal tasks (including image processing), and enhanced efficiency when integrated with <a href="https://huggingface.co/">Hugging Face Transformers</a>.</p>
<h3 id="2-can-llama-32-process-images-directly"><a href="#2-can-llama-3-2-process-images-directly">2. Can Llama 3.2 process images directly?</a><a href="#2-can-llama-3-2-process-images-directly"></a></h3>
<p>Yes, Llama 3.2 introduces vision models (11B and 90B) that enable it to process and understand images directly, allowing for tasks like image captioning, object recognition, and scene interpretation</p>
<h3 id="3-what-are-some-common-use-cases-of-llama-32-in-image-processing"><a href="#3-what-are-some-common-use-cases-of-llama-3-2-in-image-processing">3. What are some common use cases of Llama 3.2 in image processing?</a><a href="#3-what-are-some-common-use-cases-of-llama-3-2-in-image-processing"></a></h3>
<p>Llama 3.2 can assist in image processing tasks such as:</p>
<ul>
<li><strong>Image Captioning:</strong> Generating descriptive text from images.</li>
<li><strong>Object Recognition:</strong> Identifying objects within an image when combined with a vision model.</li>
<li><strong>Text Extraction (OCR):</strong> Helping interpret extracted text from an image.</li>
<li><strong>Style Transfer &amp; Image Editing:</strong> Assisting in AI-powered image generation and modification.</li>
</ul>
<h3 id="4-how-do-i-set-up-hugging-face-transformers-to-work-with-llama-32"><a href="#4-how-do-i-set-up-hugging-face-transformers-to-work-with-llama-3-2">4. How do I set up Hugging Face Transformers to work with Llama 3.2?</a><a href="#4-how-do-i-set-up-hugging-face-transformers-to-work-with-llama-3-2"></a></h3>
<p>You can install the required libraries and load the model using the following steps:</p>
<pre tabindex="0"><code>pip install transformers torch torchvision
</code></pre><p>Then, load the model with:</p>
<pre tabindex="0"><code>from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = &#34;meta-llama/llama-3-2&#34;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
</code></pre><p>If working with images, you may also need transformers’s vision models like <a href="https://huggingface.co/docs/transformers/en/model_doc/clip">CLIP</a>:</p>
<pre tabindex="0"><code>from transformers import CLIPProcessor, CLIPModel

clip_model = CLIPModel.from_pretrained(&#34;openai/clip-vit-base-patch32&#34;)
processor = CLIPProcessor.from_pretrained(&#34;openai/clip-vit-base-patch32&#34;)
</code></pre><h3 id="5-how-can-i-use-llama-32-for-image-captioning"><a href="#5-how-can-i-use-llama-3-2-for-image-captioning">5. How can I use Llama 3.2 for image captioning?</a><a href="#5-how-can-i-use-llama-3-2-for-image-captioning"></a></h3>
<p>Llama 3.2’s vision model can generate high-quality captions for images:</p>
<pre tabindex="0"><code>from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image

model_name = &#34;meta-llama/llama-3.2-vision&#34;
processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForVision2Seq.from_pretrained(model_name)

image = Image.open(&#34;your_image.jpg&#34;)
inputs = processor(images=image, return_tensors=&#34;pt&#34;)
output = model.generate(**inputs)

caption = processor.batch_decode(output, skip_special_tokens=True)[0]
print(&#34;Generated Caption:&#34;, caption)
</code></pre><h3 id="6-can-i-fine-tune-llama-32-vision-on-my-own-dataset"><a href="#6-can-i-fine-tune-llama-3-2-vision-on-my-own-dataset">6. Can I fine-tune Llama 3.2 Vision on my own dataset?</a><a href="#6-can-i-fine-tune-llama-3-2-vision-on-my-own-dataset"></a></h3>
<p>Yes, you can fine-tune Llama 3.2 Vision using Hugging Face’s transformers library with LoRA (Low-Rank Adaptation).</p>
<p>Example fine-tuning setup:</p>
<pre tabindex="0"><code>from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=8, lora_alpha=32, target_modules=[&#34;q_proj&#34;, &#34;v_proj&#34;]
)
fine_tuned_model = get_peft_model(model, config)
</code></pre><p>This allows efficient fine-tuning without retraining the entire model.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a><a href="#conclusion"></a></h2>
<p>In this tutorial, you learned how to extract employee IDs and names from images using the <a href="https://marketplace.digitalocean.com/apps/meta-llama-llama-3-2-90b-vision-instruct-8x">Llama 3.2 Vision</a> model. We integrated <a href="/products/spaces">DigitalOcean Spaces</a> for storing images and used a managed <a href="https://docs.digitalocean.com/products/databases/mysql/">MySQL database</a> for structured data storage. This solution provides an automated way to process and manage employee verification data with AI-powered efficiency.</p>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/image-processing-using-llama-huggingface">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
