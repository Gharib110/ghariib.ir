<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>The Cloud Controller Manager Chicken and Egg Problem</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>The Cloud Controller Manager Chicken and Egg Problem</h1>
			<b><time>19.03.2025 00:00</time></b>
		       

			<div>
				<p>Kubernetes 1.31 completed the largest migration in Kubernetes history, removing the in-tree cloud provider. While the component migration is now done, this leaves some additional complexity for users and installer projects (for example, kOps or Cluster API) . We will go over those additional steps and failure points and make recommendations for cluster owners. This migration was complex and some logic had to be extracted from the core components, building four new subsystems.</p>
<ol>
<li><strong>Cloud controller manager</strong> (KEP-2392)</li>
<li><strong>API server network proxy</strong> (KEP-1281)</li>
<li><strong>kubelet credential provider plugins</strong> (KEP-2133)</li>
<li><strong>Storage migration to use CSI</strong> (KEP-625)</li>
</ol>
<p>The cloud controller manager is part of the control plane. It is a critical component that replaces some functionality that existed previously in the kube-controller-manager and the kubelet.</p>
<!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://kubernetes.io/images/docs/components-of-kubernetes.svg" alt="Components of Kubernetes" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>Components of Kubernetes</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>One of the most critical functionalities of the cloud controller manager is the node controller, which is responsible for the initialization of the nodes.</p>
<p>As you can see in the following diagram, when the <strong>kubelet</strong> starts, it registers the Node object with the apiserver, Tainting the node so it can be processed first by the cloud-controller-manager. The initial Node is missing the cloud-provider specific information, like the Node Addresses and the Labels with the cloud provider specific information like the Node, Region and Instance type information.</p>
<!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/ccm-chicken-egg-problem-sequence-diagram.svg" alt="Chicken and egg problem sequence diagram" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>Chicken and egg problem sequence diagram</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>This new initialization process adds some latency to the node readiness. Previously, the kubelet was able to initialize the node at the same time it created the node. Since the logic has moved to the cloud-controller-manager, this can cause a chicken and egg problem during the cluster bootstrapping for those Kubernetes architectures that do not deploy the controller manager as the other components of the control plane, commonly as static pods, standalone binaries or daemonsets/deployments with tolerations to the taints and using <code>hostNetwork</code> (more on this below)</p>
<h2 id="examples-of-the-dependency-problem">Examples of the dependency problem</h2>
<p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be unschedulable and as such the cluster will not initialize properly. The following are a few concrete examples of how this problem can be expressed and the root causes for why they might occur.</p>
<p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource (e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it will schedule properly.</p>
<h3 id="example-cloud-controller-manager-not-scheduling-due-to-uninitialized-taint">Example: Cloud controller manager not scheduling due to uninitialized taint</h3>
<p>As noted in the Kubernetes documentation, when the kubelet is started with the command line flag <code>--cloud-provider=external</code>, its corresponding <code>Node</code> object will have a no schedule taint named <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager is responsible for removing the no schedule taint, this can create a situation where a cloud-controller-manager that is being managed by a Kubernetes resource, such as a <code>Deployment</code> or <code>DaemonSet</code>, may not be able to schedule.</p>
<p>If the cloud-controller-manager is not able to be scheduled during the initialization of the control plane, then the resulting <code>Node</code> objects will all have the <code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint will not be removed as the cloud-controller-manager is responsible for its removal. If the no schedule taint is not removed, then critical workloads, such as the container network interface controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p>
<h3 id="example-cloud-controller-manager-not-scheduling-due-to-not-ready-taint">Example: Cloud controller manager not scheduling due to not-ready taint</h3>
<p>The next example would be possible in situations where the container network interface (CNI) is waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not tolerated the taint which would be removed by the CNI.</p>
<p>The Kubernetes documentation describes the <code>node.kubernetes.io/not-ready</code> taint as follows:</p>
<blockquote>
<p>&ldquo;The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.&rdquo;</p></blockquote>
<p>One of the conditions that can lead to a Node resource having this taint is when the container network has not yet been initialized on that node. As the cloud-controller-manager is responsible for adding the IP addresses to a Node resource, and the IP addresses are needed by the container network controllers to properly configure the container network, it is possible in some circumstances for a node to become stuck as not ready and uninitialized permanently.</p>
<p>This situation occurs for a similar reason as the first example, although in this case, the <code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is not able to execute, then it will not initialize the node. It will cascade into the container network controllers not being able to run properly, and the node will end up carrying both the <code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints, leaving the cluster in an unhealthy state.</p>
<h2 id="our-recommendations">Our Recommendations</h2>
<p>There is no one “correct way” to run a cloud-controller-manager. The details will depend on the specific needs of the cluster administrators and users. When planning your clusters and the lifecycle of the cloud-controller-managers please consider the following guidance:</p>
<p>For cloud-controller-managers running in the same cluster, they are managing.</p>
<ol>
<li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager will need to communicate with an API service endpoint associated with the infrastructure. Setting “hostNetwork” to true will ensure that the cloud controller is using the host networking instead of the container network and, as such, will have the same network access as the host operating system. It will also remove the dependency on the networking plugin. This will ensure that the cloud controller has access to the infrastructure endpoint (always check your networking configuration against your infrastructure provider’s instructions).</li>
<li>Use a scalable resource type. <code>Deployments</code> and <code>DaemonSets</code> are useful for controlling the lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using these primitives to control the lifecycle of your cloud controllers and running multiple replicas, you must remember to enable leader election, or else your controllers will collide with each other which could lead to nodes not being initialized in the cluster.</li>
<li>Target the controller manager containers to the control plane. There might exist other controllers which need to run outside the control plane (for example, Azure’s node manager controller). Still, the controller managers themselves should be deployed to the control plane. Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the control plane to ensure that they are running in a protected space. Cloud controllers are vital to adding and removing nodes to a cluster as they form a link between Kubernetes and the physical infrastructure. Running them on the control plane will help to ensure that they run with a similar priority as other core cluster controllers and that they have some separation from non-privileged user workloads.
<ol>
<li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running on the same host is also very useful to ensure that a single node failure will not degrade the cloud controller performance.</li>
</ol>
</li>
<li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud controller container to ensure that it will schedule to the correct nodes and that it can run in situations where a node is initializing. This means that cloud controllers should tolerate the <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any taints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code> or <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the <code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the node is not yet available for health monitoring.</li>
</ol>
<p>For cloud-controller-managers that will not be running on the cluster they manage (for example, in a hosted control plane on a separate cluster), then the rules are much more constrained by the dependencies of the environment of the cluster running the cloud-controller-manager. The advice for running on a self-managed cluster may not be appropriate as the types of conflicts and network constraints will be different. Please consult the architecture and requirements of your topology for these scenarios.</p>
<h3 id="example">Example</h3>
<p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is important to note that this is for demonstration purposes only, for production uses please consult your cloud provider’s documentation.</p>
<pre tabindex="0"><code>apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
name: cloud-controller-manager
namespace: kube-system
spec:
replicas: 2
selector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
strategy:
type: Recreate
template:
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
annotations:
kubernetes.io/description: Cloud controller manager for my infrastructure
spec:
containers: # the container details will depend on your specific cloud controller manager
- name: cloud-controller-manager
command:
- /bin/my-infrastructure-cloud-controller-manager
- --leader-elect=true
- -v=1
image: registry/my-infrastructure-cloud-controller-manager@latest
resources:
requests:
cpu: 200m
memory: 50Mi
hostNetwork: true # these Pods are part of the control plane
nodeSelector:
node-role.kubernetes.io/control-plane: &#34;&#34;
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- topologyKey: &#34;kubernetes.io/hostname&#34;
labelSelector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
tolerations:
- effect: NoSchedule
key: node-role.kubernetes.io/master
operator: Exists
- effect: NoExecute
key: node.kubernetes.io/unreachable
operator: Exists
tolerationSeconds: 120
- effect: NoExecute
key: node.kubernetes.io/not-ready
operator: Exists
tolerationSeconds: 120
- effect: NoSchedule
key: node.cloudprovider.kubernetes.io/uninitialized
operator: Exists
- effect: NoSchedule
key: node.kubernetes.io/not-ready
operator: Exists
</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple replicas of a cloud controller manager is good practice for ensuring high-availability and redundancy, but does not contribute to better performance. In general, only a single instance of a cloud controller manager will be reconciling a cluster at any given time.</p>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/apple-inc-sent-you-a-payment-request-payoneer-invoices-other-microsoft-enabled-scams/">“Apple Inc sent you a payment request” Payoneer invoices; other Microsoft-enabled scams</a></li>
				
				<li><a href="/posts/glasses-that-transcribe-text-to-audio/">“Glasses” That Transcribe Text To Audio</a></li>
				
				<li><a href="/posts/5-best-linux-centos-replacement-options-alternatives/">&lt;div&gt;5 Best Linux CentOS Replacement Options &amp; Alternatives&lt;/div&gt;</a></li>
				
				<li><a href="/posts/a-week-in-security-march-10-march-16/">&lt;div&gt;A week in security (March 10 – March 16)&lt;/div&gt;</a></li>
				
				<li><a href="/posts/a-week-in-security-march-3-march-9/">&lt;div&gt;A week in security (March 3 – March 9)&lt;/div&gt;</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
