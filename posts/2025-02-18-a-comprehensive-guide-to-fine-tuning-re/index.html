<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>A Comprehensive Guide to Fine-Tuning Reasoning Models Fine-Tuning DeepSeek-R1 on Medical CoT with DigitalOceans GPU Droplets</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>A Comprehensive Guide to Fine-Tuning Reasoning Models Fine-Tuning DeepSeek-R1 on Medical CoT with DigitalOceans GPU Droplets</h1>
			<b><time>18.02.2025 13:52</time></b>
		       

			<div>
				<h1 id="a-comprehensive-guide-to-fine-tuning-reasoning-models-fine-tuning-deepseek-r1-on-medical-cot-with-digitaloceans-gpu-droplets">A Comprehensive Guide to Fine-Tuning Reasoning Models Fine-Tuning DeepSeek-R1 on Medical CoT with DigitalOceans GPU Droplets</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Recent advances in Large Language Models (LLMs) have shown promise in systematic reasoning tasks, with open-source models like DeepSeek-R1 demonstrating impressive capabilities in breaking down complex problems into logical steps. By fine-tuning these reasoning-focused models for medical applications, we can create proof-of-concept AI assistants that could potentially support healthcare professionals in their clinical decision-making processes while maintaining transparent chains of reasoning. In this tutorial, we’ll explore how to leverage <strong>DigitalOcean’s GPU Droplets</strong> to fine-tune a <a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit"><strong>distilled quantized version of DeepSeek-R1</strong></a>, transforming it into a specialized reasoning assistant that can help analyze patient cases, suggest potential diagnoses, and provide verified structured explanations for its recommendations.</p>
<p>Shoutout to this great <a href="https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model">DataCamp tutorial</a> and the paper, <a href="https://arxiv.org/pdf/2412.18925">HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs</a>, for inspiring this tutorial.</p>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<p>Knowledge of these prerequisites will be helpful with following along with this tutorial:</p>
<ul>
<li>Python and PyTorch</li>
<li>Deep Learning fundamentals (ex: neural networks, hyperparameters, etc.)</li>
<li>Experience working with Hugging Face models and the Transformers library</li>
</ul>
<h2 id="when-should-we-use-fine-tuning"><a href="#when-should-we-use-fine-tuning">When should we use Fine-Tuning?</a><a href="#when-should-we-use-fine-tuning"></a></h2>
<p>Fine-tuning adapts a pre-trained model’s existing knowledge to perform specific tasks by training it further on a curated dataset. Fine-tuning shines in scenarios where <strong>consistent formatting</strong>, specific <strong>tone</strong> requirements, or <strong>complex instruction following</strong> are needed, as it can optimize the model’s behaviour for these particular use cases. This approach typically requires fewer computational resources and less time than training a model from scratch. Before proceeding with fine-tuning, however, it is good practice for developers to first consider the advantages of alternatives such as prompt engineering, Retrieval Augmented Generation (RAG), and even training a model from scratch.</p>
<p>Approach</p>
<p>When should we consider this?</p>
<p>Prompt Engineering</p>
<p>Prompt engineering involves crafting precise instructions to guide the model’s behaviour using existing capabilities. We have tutorials that <strong>refine system prompts for specific use-cases</strong> with DigitalOcean’s <a href="/products/ai-ml/1-click-models">1-click models</a>: <a href="/community/tutorials/llm-for-social-media-analytics#part-2-prompt-engineering-for-social-media-analytics">Getting Started with LLMs for Social Media Analytics</a> &amp; <a href="/community/tutorials/newsletter-generator-with-1click-models#part-2-newsletter-generation-with-prompt-engineering">How to Create an Email Newsletter Generator</a></p>
<p>Retrieval-Augmented Generation</p>
<p>In cases where the goal is to incorporate new or up-to-date information, Retrieval-Augmented Generation (RAG) is typically more appropriate. RAG allows the model to access external knowledge without modifying its underlying parameters.</p>
<p>Training From Scratch</p>
<p>Training a model from scratch can be beneficial in applications where model interpretability and explainability are desired. This approach gives you greater control over the model’s architecture, data, and decision-making process.</p>
<p>One can do combinations of different approaches such as fine-tuning and RAG. By combining fine-tuning to establish a robust baseline with RAG to handle dynamic updates, the system achieves both adaptability and efficiency without requiring constant re-training. It really all comes down to organizational resource constraints and desired performance.</p>
<p><strong>Monitoring whether outputs deliver to the standards of the intended utility and iterating/pivoting if not is absolutely critical.</strong></p>
<p>Once we know that fine-tuning is the approach we want to take, we need to assemble the necessary components.</p>
<h2 id="what-do-we-need-to-fine-tune-a-model"><a href="#what-do-we-need-to-fine-tune-a-model">What do we need to Fine-Tune a Model?</a><a href="#what-do-we-need-to-fine-tune-a-model"></a></h2>
<h3 id="a-pre-trained-model"><a href="#a-pre-trained-model">A pre-trained model</a><a href="#a-pre-trained-model"></a></h3>
<p>A pre-trained model is a neural network that has already been trained on a large general-purpose corpus of data. Hugging Face has a plethora of <a href="https://huggingface.co/models">open-source models</a> available for you to use.</p>
<p><strong>In this tutorial, we will be using a very popular reasoning model, DeepSeek-R1.</strong> Reasoning models excel at intricate tasks like advanced problems in math or coding. We chose <a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit">“unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit</a>” because it is distilled and pre-quantized, making it a more memory efficient and cost-effective model to perform experiments with. We were especially curious about its potential for complex tasks such as medical analysis. Note that using them for simpler tasks such as summarization or translation would be overkill due to the tendency reasoning models have towards being computationally expensive and verbose.</p>
<h3 id="dataset"><a href="#dataset">Dataset</a><a href="#dataset"></a></h3>
<p>Hugging Face has a great selection of <a href="https://huggingface.co/datasets">datasets</a>. We will be using the <a href="https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT">Medical O1 Reasoning Dataset</a>. This dataset was generated with GPT-4o by searching for solutions to verifiable medical problems and validating them through a medical verifier.</p>
<p>This dataset will be used to perform <strong>supervised fine-tuning (SFT)</strong>, where models are trained on a dataset of instructions and responses. To minimize the difference between the generated answers and ground-truth responses, SFT adjusts the weights in the LLM.</p>
<h3 id="gpus"><a href="#gpus">GPUs</a><a href="#gpus"></a></h3>
<p>GPUs aren’t always necessary to fine-tune a model. However, using a GPU (or multiple GPUs) can speed up the process significantly, especially for larger models or datasets like the ones used in this tutorial. In this article, we will show you how you can make use of <a href="/products/gpu-droplets">DigitalOcean GPU Droplets</a>.</p>
<h2 id="tools-and-frameworks"><a href="#tools-and-frameworks">Tools and Frameworks</a><a href="#tools-and-frameworks"></a></h2>
<p>Before starting this tutorial, it is recommended to familiarize yourself with the following libraries and tools:</p>
<h3 id="unsloth"><a href="#unsloth">Unsloth</a><a href="#unsloth"></a></h3>
<p><a href="https://github.com/unslothai/unsloth">Unsloth</a> is all about making LLM training faster, with a particular focus on fine-tuning. The <a href="https://github.com/unslothai/unsloth?tab=readme-ov-file#-documentation">FastLanguageModel class</a>, part of the <a href="https://docs.unsloth.ai/">Unsloth library</a>, provides a simplified abstraction for fine-tuning LLMs. This class can handle loading the trained model weights, preprocessing input text, and executing inference to generate outputs.</p>
<h3 id="transformer-reinforcement-learning-trl"><a href="#transformer-reinforcement-learning-trl">Transformer Reinforcement Learning (TRL)</a><a href="#transformer-reinforcement-learning-trl"></a></h3>
<p>The HuggingFace Library, TRL, is used to train transformer language models with Reinforcement Learning. This tutorial will utilize the <a href="https://huggingface.co/docs/trl/en/sft_trainer">SFTTrainer</a> Class.</p>
<h3 id="transformers"><a href="#transformers">Transformers</a><a href="#transformers"></a></h3>
<p><a href="https://huggingface.co/docs/transformers/en/index">Transformers</a> is also a HuggingFace Library. We will be using the <a href="https://huggingface.co/docs/transformers/v4.48.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> class to specify our desired arguments in SFTTrainer.</p>
<h3 id="weights-and-biases"><a href="#weights-and-biases">Weights and Biases</a><a href="#weights-and-biases"></a></h3>
<p>The <a href="https://wandb.ai/home">W&amp;B platform</a> will be used for experiment tracking. Specifically, <a href="https://wandb.ai/mostafaibrahim17/ml-articles/reports/A-Deep-Dive-Into-Learning-Curves-in-Machine-Learning--Vmlldzo0NjA1ODY0">loss curves</a> will be monitored.</p>
<h2 id="part-2-implementation"><a href="#part-2-implementation">Part 2: Implementation</a><a href="#part-2-implementation"></a></h2>
<h3 id="step-1-set-up-a-gpu-droplet-and-launch-jupyter-labs"><a href="#step-1-set-up-a-gpu-droplet-and-launch-jupyter-labs">Step 1: Set up a GPU Droplet and Launch Jupyter Labs</a><a href="#step-1-set-up-a-gpu-droplet-and-launch-jupyter-labs"></a></h3>
<p>Follow this tutorial, <a href="/community/tutorials/jupyter-notebooks-with-gpu-droplets">“Setting Up the GPU Droplet Environment for AI/ML Coding”</a>, to set up a GPU Droplet environment for our Jupyter Notebook.</p>
<h3 id="step-2-install-dependencies"><a href="#step-2-install-dependencies">Step 2: Install Dependencies</a><a href="#step-2-install-dependencies"></a></h3>
<pre tabindex="0"><code>%%capture
!pip install unsloth
!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
!pip install --upgrade jupyter
!pip install --upgrade ipywidgets
!pip install wandb
</code></pre><h3 id="step-3-configure-access-tokens"><a href="#step-3-configure-access-tokens">Step 3: Configure Access Tokens</a><a href="#step-3-configure-access-tokens"></a></h3>
<p>HuggingFace Tokens can be obtained from the <a href="https://huggingface.co/settings/tokens">Hugging Face Access Token page</a>. Note that you may need to create Hugging Face account.</p>
<pre tabindex="0"><code>from huggingface_hub import login 
hf_token = &#34;Replace with your actual token&#34;
login(hf_token) 
</code></pre><p>Similarly, you will also need a <a href="https://wandb.ai/home">Weights &amp; Biases</a> account to get a token for this step. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/wandb.png" alt="wandb" />
</figure>


</p>
<pre tabindex="0"><code>import wandb
wb_token = &#34;Replace with your actual token&#34;

wandb.login(key=wb_token)
run = wandb.init(
    project=&#39;Medical Assistant&#39;, 
    job_type=&#34;training&#34;, 
    anonymous=&#34;allow&#34;
)
</code></pre><pre tabindex="0"><code>from unsloth import FastLanguageModel
</code></pre><h3 id="step-4-loading-the-model-and-tokenizer"><a href="#step-4-loading-the-model-and-tokenizer">Step 4: Loading the model and tokenizer</a><a href="#step-4-loading-the-model-and-tokenizer"></a></h3>
<pre tabindex="0"><code>max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = &#34;unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit&#34;,
    max_seq_length = max_seq_length,
    load_in_4bit = True,
    dtype = None, #if using H100, will automatically default to bfloat16
    token = hf_token,
)
</code></pre><h3 id="step-5-testing-model-outputs-before-fine-tuning"><a href="#step-5-testing-model-outputs-before-fine-tuning">Step 5: Testing Model Outputs Before Fine-Tuning</a><a href="#step-5-testing-model-outputs-before-fine-tuning"></a></h3>
<h4 id="creating-a-system-prompt">Creating a System Prompt</h4>
<p>It is good practice to verify whether model outputs match your standards for format, quality, accuracy, etc. to assess if fine-tuning is necessary. Since we are interested in reasoning, we will formulate a system prompt that elicits a chain of thought.</p>
<p>Instead of writing the prompt directly in our input, let’s start by writing up a prompt template that incorporates place holders.<br>

<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/social%20network%20meme.png" alt="socialnetworkmeme" />
</figure>


</p>
<p>In this prompt template, we will specify precisely what we are looking for.</p>
<pre tabindex="0"><code>prompt_template= &#34;&#34;&#34;### Role:
You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Your responses should:
- Be evidence-based and clinically relevant
- Include differential diagnoses when appropriate
- Consider patient safety and standard of care
- Note any important limitations or uncertainties

### Question:
{question}

### Thinking Process:
{thinking}

### Clinical Assessment:
{response}

&#34;&#34;&#34;
</code></pre><p>Notice the {thinking} placeholder. The primary goal of this step is to instruct the LLM to explicitly articulate its reasoning process before providing the final answer. This is often what is referred to as <a href="https://arxiv.org/pdf/2201.11903">&ldquo;chain-of-thought prompting”</a>.</p>
<h4 id="inference-with-our-system-prompt-before-fine-tuning">Inference with our System Prompt (Before Fine-tuning)</h4>
<p>Here, we format the question using the structured prompt (prompt_template) to ensure the model follows a logical reasoning process. We will tokenize the input, return them as PyTorch tensors, and move it to the GPU (cuda) for faster inference.</p>
<pre tabindex="0"><code>question = &#34;A 58-year-old woman reports a 3-year history of urine leakage when laughing, exercising, or lifting heavy objects. She denies any nighttime incontinence or feelings of urgency. On physical exam, she demonstrates urine loss with Valsalva maneuver, and a Q-tip test shows hypermobility of the urethrovesical junction with a 45-degree excursion. What would urodynamic testing most likely show regarding her post-void residual volume and detrusor muscle activity?&#34;
FastLanguageModel.for_inference(model) #model defined in step 4
inputs = tokenizer([prompt_template.format(question,&#34;&#34;)], return_tensors=&#34;pt&#34;).to(&#34;cuda&#34;)
</code></pre><p>After, we will generate a response using the model, specifying key parameters like max_new_tokens=1200 (limits response length).</p>
<pre tabindex="0"><code>outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True,
)
</code></pre><p>To obtain the final readable answer, we will decode the output tokens back into text.</p>
<pre tabindex="0"><code>response = tokenizer.batch_decode(outputs)
print(response[0].split(&#34;### Response:&#34;)[1])
</code></pre><p>Feel free to experiment with different prompt formulations and see how they affect your outputs.</p>
<h3 id="step-6-load-the-dataset"><a href="#step-6-load-the-dataset">Step 6: Load the Dataset</a><a href="#step-6-load-the-dataset"></a></h3>
<p>The dataset, FreedomIntelligence/medical-o1-reasoning-SFT, that we’re using has three columns: Question, Complex_CoT, and Response. 
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/Mel/Dataset.png" alt="SFT dataset" />
</figure>


</p>
<p>We will create a function (<a href="https://huggingface.co/docs/trl/v0.4.7/en/sft_trainer#format-your-input-prompts">formatting_prompts_func</a>) to format the input prompts in the dataset.</p>
<pre tabindex="0"><code>def formatting_prompts_func(examples):
    inputs = examples[&#34;Question&#34;]
    cots = examples[&#34;Complex_CoT&#34;]
    outputs = examples[&#34;Response&#34;]
    texts = []
    for input, cot, output in zip(inputs, cots, outputs):
        text = prompt_template.format(input, cot, output) + tokenizer.eos_token
        texts.append(text)
    return {
        &#34;text&#34;: texts,
    }
</code></pre><pre tabindex="0"><code>from datasets import load_dataset
dataset = load_dataset(&#34;FreedomIntelligence/medical-o1-reasoning-SFT&#34;,&#34;en&#34;, split = &#34;train[0:500]&#34;,trust_remote_code=True)
dataset = dataset.map(formatting_prompts_func, batched = True,)
dataset[&#34;text&#34;][0]
</code></pre><h3 id="step-7-prepare-the-model-for-parameter-efficient-fine-tuning-peft"><a href="#step-7-prepare-the-model-for-parameter-efficient-fine-tuning-peft">Step 7: Prepare the Model for Parameter Efficient Fine-Tuning (PEFT)</a><a href="#step-7-prepare-the-model-for-parameter-efficient-fine-tuning-peft"></a></h3>
<p>Instead of updating all the parameters of the model during fine-tuning, PEFT methods typically only modify a small subset of parameters, resulting in savings in computational power and time.</p>
<p>Here is an overview of some of the parameters and arguments we will be using the .get_peft_model method of Unsloth’s FastLanguageModel class.</p>
<p>Hyperparameter</p>
<p>Possible Values</p>
<p><strong>r:</strong> <a href="https://arxiv.org/pdf/2106.09685">LoRA</a> rank. This determines the number of trainable adapters.</p>
<p>Select any number greater than 0; recommended numbers are 8, 16, 32, 64, and 128. Note that a higher rank yields more intelligent, but slower, model outputs.</p>
<p><strong>target_modules</strong>: These are the modules (layers) within the transformer architecture where LoRA will be applied.</p>
<p><strong>q_proj, k_proj, v_proj:</strong> The query, key, and value projection layers in the attention mechanism. Fine-tuning these is crucial for adapting the model’s attention to the new task. <strong>o_proj:</strong> This is the output projection layer in the attention mechanism. <strong>gate_proj, up_proj, down_proj</strong>: These are the projection layers in the feed-forward network (FFN) part of the transformer block. Fine-tuning these can help the model learn task-specific representations in the FFN.</p>
<p><strong>lora_alpha:</strong> This is a scaling factor for the LoRA updates. It helps control the magnitude of the updates applied to the original weights. It’s related to the learning rate, and tuning it can be important for performance.</p>
<p>It’s often set to a multiple of r (ex: 2r or 4r).</p>
<p><strong>lora_dropout:</strong> This is the dropout probability applied to the LoRA updates. Dropout is a regularization technique that helps prevent overfitting.</p>
<p>When set to 0, no dropout is applied. You might increase this if you observe overfitting.</p>
<p><strong>bias:</strong> The bias parameter indicates how biases, which are constants added to offset the result, are handled by the model.</p>
<p>Set as “none” if no bias is to be added. Other possible arguments include “all” or “lora_only”, specifying which layers bias is added to.</p>
<p><strong>use_gradient_checkpointing:</strong> Gradient checkpointing is a technique to reduce memory usage during training at the cost of some extra computation. It recomputes activations during the backward pass instead of storing them.</p>
<p>The “unsloth” argument can be used for an optimized implementation of gradient checkpointing for long contexts within the Unsloth library. Alternatively, this argument can be set to True for standard gradient checkpointing (to save memory at the expense of slower backward pass) or False to disable it.</p>
<p><strong>random_state:</strong> This sets the random seed for initializing the LoRA weights. Using a fixed random seed ensures reproducibility—you’ll get the same results if you run the code again with the same seed.</p>
<p>It doesn’t matter what value this is, as long as it’s consistent throughout your code.</p>
<p><strong>use_rslora:</strong> <a href="https://arxiv.org/abs/2312.03732">rsLoRA</a> introduces a scaling factor to stabilize gradients during training, addressing the issue of gradient collapse that can occur in standard LoRA as the rank increases.</p>
<p>rsLoRA is applied when set to True (sets the adapter scaling factor to lora_alpha/math.sqrt®); this is recommended for higher r values. The default value is False (default value of lora_alpha/r).</p>
<pre tabindex="0"><code>model = FastLanguageModel.get_peft_model(
    model,
    r=16,  
    target_modules=[
        &#34;q_proj&#34;,
        &#34;k_proj&#34;,
        &#34;v_proj&#34;,
        &#34;o_proj&#34;,
        &#34;gate_proj&#34;,
        &#34;up_proj&#34;,
        &#34;down_proj&#34;,
    ],
    lora_alpha=16,
    lora_dropout=0,  
    bias=&#34;none&#34;,  
    use_gradient_checkpointing=&#34;unsloth&#34;,  # True or &#34;unsloth&#34; for very long context
    random_state=522,
    use_rslora=False
    )
</code></pre><p>Now that we’ve evaluated model outputs, it is time to use our SFT dataset to fine-tune the pre-trained model.</p>
<h3 id="step-8-model-training-with-sfttrainer"><a href="#step-8-model-training-with-sfttrainer">Step 8: Model Training with SFTTrainer</a><a href="#step-8-model-training-with-sfttrainer"></a></h3>
<p><a href="https://huggingface.co/docs/trl/en/sft_trainer">Supervised Fine-tuning Trainer</a> is a class to develop supervised fine-tuned models from TRL.<br>
We will also be using</p>
<pre tabindex="0"><code>from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
</code></pre><p><a href="https://huggingface.co/docs/transformers/en/main_classes/trainer">Training</a> Arguments</p>
<p>Hyperparameter</p>
<p>Possible Values</p>
<p><strong>per_device_train_batch_size:</strong> Number of samples processed per device/GPU during training step</p>
<p>Typically powers of 2: 1, 2, 4, 7, 16, 32…</p>
<p><strong>gradient_accumulation_steps:</strong> Number of forward passes to accumulate before performing backward pass</p>
<p>Higher values allow for larger effective batch sizes. (Effective batch size = per_device_train_batch_size * gradient_accumulation_steps.)</p>
<p><strong>warmup_steps:</strong> Number of steps for the learning rate warmup phase</p>
<p>Non-negative integer, typically 5-10% of total training steps (max_steps)</p>
<p><strong>max_steps:</strong> Total number of training steps to perform</p>
<p>Positive integers, depends on dataset size and training needs</p>
<p><strong>learning_rate:</strong> Step size used for model weight updates</p>
<p>Typically between 1e-5 and 1e-3 (ex: 2e-4, 3e-4, 5e-5)</p>
<p><strong>fp16:</strong> Controls whether to use 16-bit floating point precision <strong>bf16:</strong> Controls whether to use brain floating point format</p>
<p>Enables mixed precision training (fp16 or bf16) for faster training, if supported by the hardware. Potential values include: not is_bfloat16_supported() or is_bfloat16_supported().</p>
<p><strong>logging_steps:</strong> How frequently to log training metrics</p>
<p>Positive integer value indicating interval of steps to pass before logging the training metrics. The value chosen involves striking a balance between having enough information to track training progress and keeping the overhead of logging manageable.</p>
<p><strong>optim:</strong> Optimization algorithm for training</p>
<p>adamw 8-bit performs similarly to adamw (a popular, robust optimizer), but with reduced GPU memory usage, making it a recommended choice</p>
<p><strong>weight_decay:</strong> a regularization technique to prevent overfitting where the value corresponds to the amount of weight decay to apply.</p>
<p>A float value that defaults to 0.</p>
<p><strong>lr_scheduler_type:</strong> Schedule for learning rate adjustments</p>
<p>The default and suggested value is “linear”. Other alternatives include “cosine”, “polynomial”, etc. and may be chosen to achieve faster convergence.</p>
<p><strong>seed</strong>: Random seed for reproducibility</p>
<p>It doesn’t matter what value this is, as long as it’s consistent throughout your code.</p>
<p><strong>output_dir</strong>: Location to save training outputs</p>
<p>A string of the directory path</p>
<pre tabindex="0"><code>trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field=&#34;text&#34;,
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=60,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=10,
        optim=&#34;adamw_8bit&#34;,
        weight_decay=0.01,
        lr_scheduler_type=&#34;linear&#34;,
        seed=522,
        output_dir=&#34;outputs&#34;, #saving in Response column
    ),
)
</code></pre><p>This command will start the training process.</p>
<pre tabindex="0"><code>trainer_stats = trainer.train()
</code></pre><h3 id="step-9-monitoring-experiments"><a href="#step-9-monitoring-experiments">Step 9: Monitoring Experiments</a><a href="#step-9-monitoring-experiments"></a></h3>
<p>Experiment tracking can be done with Weights and Biases. Essentially, we want to ensure that the training loss decreases over time to ensure model performance is improving with fine-tuning.</p>
<p>If model performance is degrading, it may be worth experimenting with the hyperparameter values.</p>
<h3 id="step-10-model-inference-after-fine-tuning"><a href="#step-10-model-inference-after-fine-tuning">Step 10: Model Inference After Fine-Tuning</a><a href="#step-10-model-inference-after-fine-tuning"></a></h3>
<pre tabindex="0"><code>question = &#34;A 58-year-old woman reports a 3-year history of urine leakage when laughing, exercising, or lifting heavy objects. She denies any nighttime incontinence or feelings of urgency. On physical exam, she demonstrates urine loss with Valsalva maneuver, and a Q-tip test shows hypermobility of the urethrovesical junction with a 45-degree excursion. What would urodynamic testing most likely show regarding her post-void residual volume and detrusor muscle activity?&#34;


FastLanguageModel.for_inference(model)  
inputs = tokenizer([prompt_template.format(question, &#34;&#34;)], return_tensors=&#34;pt&#34;).to(&#34;cuda&#34;)

outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True,
)
response = tokenizer.batch_decode(outputs)
print(response[0].split(&#34;### Response:&#34;)[1])
</code></pre><h3 id="step-11-saving-the-model-locally"><a href="#step-11-saving-the-model-locally">Step 11: Saving the Model Locally</a><a href="#step-11-saving-the-model-locally"></a></h3>
<pre tabindex="0"><code>new_model_local = &#34;DeepSeek-R1-Medical-COT&#34;
model.save_pretrained(new_model_local) 
tokenizer.save_pretrained(new_model_local)

model.save_pretrained_merged(new_model_local, tokenizer, save_method = &#34;merged_16bit&#34;,)
</code></pre><h3 id="step-12-pushing-the-model-to-huggingface-hub"><a href="#step-12-pushing-the-model-to-huggingface-hub">Step 12: Pushing the Model to HuggingFace Hub</a><a href="#step-12-pushing-the-model-to-huggingface-hub"></a></h3>
<p>If it is desirable to make the model accessible and beneficial to the wider AI community, we can publish the adopter, tokenizer, and model on to the Hugging Face Hub. This will allow others to easily integrate our model into their own projects and systems.</p>
<pre tabindex="0"><code>new_model_online = &#34;HuggingFaceUSERNAME/DeepSeek-R1-Medical-COT&#34;
model.push_to_hub(new_model_online)
tokenizer.push_to_hub(new_model_online)

model.push_to_hub_merged(new_model_online, tokenizer, save_method = &#34;merged_16bit&#34;)
</code></pre><h2 id="conclusion"><a href="#conclusion">Conclusion</a><a href="#conclusion"></a></h2>
<p>Fine-tuning is how smart teams transform those pre-trained models into precise, targeted tools that solve real problems. Here, we’re not reinventing the wheel, but rather aligning these wheels so that they take us where we want to go. While pre-trained models are powerful, they can be generic with outputs that may lack the structure and substance characteristic of professional-grade work.</p>
<p>We hope that through this tutorial, you gained an intuition around when to use and fine-tune reasoning models as well as some inspiration to better refine this technology for your use-case.</p>
<h2 id="references-and-additional-resources"><a href="#references-and-additional-resources">References and Additional Resources</a><a href="#references-and-additional-resources"></a></h2>
<p><a href="https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model">Fine-Tuning DeepSeek R1 (Reasoning Model) | DataCamp</a></p>
<p><a href="https://github.com/FreedomIntelligence/HuatuoGPT-o1/tree/main">HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs</a></p>
<p><a href="https://unsloth.ai/blog/r1-reasoning">Train your own R1 reasoning model locally (GRPO)</a></p>
<p><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb#scrollTo=DkIvEkIIkEyB">Unslothai Llama3.1_(8B)-GRPO.ipynb</a></p>
<p><a href="https://www.youtube.com/watch?v=baQH7tsIQQ8">Fine-Tuning Your Own Llama 3 Model</a></p>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/fine-tuning-deepseek-medical-cot">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
