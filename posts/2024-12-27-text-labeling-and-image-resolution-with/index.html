<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Text Labeling and Image Resolution with the Monkey Chat Vision Model</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Text Labeling and Image Resolution with the Monkey Chat Vision Model</h1>
			<b><time>27.12.2024 13:02</time></b>
		       

			<div>
				<h1 id="text-labeling-and-image-resolution-with-the-monkey-chat-vision-model">Text Labeling and Image Resolution with the Monkey Chat Vision Model</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Vision-language models are among the advanced artificial intelligence AI systems designed to understand and process visual and textual data together. These models are known to combine the capabilities of computer vision and natural language processing tasks. The models are trained to interpret images and generate descriptions about the image, enabling a range of applications such as image captioning, visual question answering, and text-to-image synthesis. These models are trained on large datasets and powerful neural network architectures, which helps the models to learn complex relationships. This, in turn, allows the models to perform the desired tasks. This advanced system opens up possibilities for human-computer interaction and the development of intelligent systems that can communicate similarly to humans.</p>
<p>Large Multimodal Models (LMMs) are quite powerful however they struggle with the high-resolution input and scene understanding. To address these challenges Monkey was recently introduced. Monkey, a vision-language model, processes input images by dividing the input images into uniform patches, with each patch matching the size used in its original vision encoder training (e.g., 448×448 pixels).</p>
<p>This design allows the model to handle high-resolution images. Monkey employs a two-part strategy: first, it enhances visual capture through higher resolution; second, it uses a multi-level description generation method to enrich scene-object associations, creating a more comprehensive understanding of the visual data. This approach improves learning from the data by capturing detailed visuals, enhancing descriptive text generation’s effectiveness.</p>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<ul>
<li>Basic understanding of text labeling and image processing.</li>
<li>Familiarity with using AI models for computer vision tasks.</li>
<li>Access to the Monkey Chat Vision Model and a compatible development environment.</li>
</ul>
<h2 id="monkey-architecture-overview"><a href="#monkey-architecture-overview">Monkey Architecture Overview</a><a href="#monkey-architecture-overview"></a></h2>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/Screenshot-2024-08-07-at-4.25.44-PM.png" alt="image" />
</figure>


</p>
<p>The Overall Monkey Architecture (<a href="https://arxiv.org/pdf/2311.06607">Image Source</a>)</p>
<p>Let’s break down this approach step by step.</p>
<h3 id="image-processing-with-sliding-window"><a href="#image-processing-with-sliding-window">Image Processing with Sliding Window</a><a href="#image-processing-with-sliding-window"></a></h3>
<ul>
<li>Input Image: An image (I) with dimensions (H X W X 3), where (H) and (W) are the height and width of the image, and 3 represents the color channels (RGB).</li>
<li>Sliding Window: The image is divided into smaller sections using a sliding window (W) with dimensions (H_v X W_v). This process partitions the image into local sections, which allows the model to focus on specific parts of the image.</li>
</ul>
<h3 id="lora-integration"><a href="#lora-integration">LoRA Integration</a><a href="#lora-integration"></a></h3>
<ul>
<li>LoRA (Low-Rank Adaptation): LoRA is employed within each shared encoder to handle the diverse visual elements present in different parts of the image. LoRA helps the encoders capture detail-sensitive features more effectively without significantly increasing the model’s parameters or computational load.</li>
</ul>
<h3 id="maintaining-structural-information"><a href="#maintaining-structural-information">Maintaining Structural Information</a><a href="#maintaining-structural-information"></a></h3>
<ul>
<li>Global Image Resizing: To preserve the overall structural information of the input image, the original image is resized to dimensions ((H_v, W_v)), creating a global image. This global image maintains a holistic view while the patches provide detailed perspectives.</li>
</ul>
<h3 id="processing-with-visual-encoder-and-resampler"><a href="#processing-with-visual-encoder-and-resampler">Processing with Visual Encoder and Resampler</a><a href="#processing-with-visual-encoder-and-resampler"></a></h3>
<ul>
<li>Concurrent Processing: Both the individual patches and the global image are processed through the visual encoder and resampler simultaneously.</li>
<li>Visual Resampler: Inspired by the Flamingo model, the visual resampler performs two main functions:
<ol>
<li>Summarizing Visual Information: It condenses the visual information from the image sections.</li>
<li>Obtaining Higher Semantic Representations: It transforms visual information into a language feature space for better semantic understanding.</li>
</ol>
</li>
</ul>
<h3 id="cross-attention-module"><a href="#cross-attention-module">Cross-Attention Module</a><a href="#cross-attention-module"></a></h3>
<ul>
<li>Cross-Attention Mechanism: The resampler uses a cross-attention module where trainable vectors (embeddings) act as query vectors. Image features from the visual encoder serve as keys in the cross-attention operation. This allows the model to focus on important image parts while incorporating contextual information.</li>
</ul>
<h3 id="balancing-detail-and-holistic-understanding"><a href="#balancing-detail-and-holistic-understanding">Balancing Detail and Holistic Understanding</a><a href="#balancing-detail-and-holistic-understanding"></a></h3>
<ul>
<li>Balanced Approach: This method balances the need for detailed local analysis and a holistic global image perspective. This balance enhances the model’s performance by capturing detailed features and overall structure without substantially increasing computational resources.</li>
</ul>
<p>This approach improves the model’s ability to understand complex images by combining local detail analysis with a global overview, leveraging advanced techniques like LoRA and cross-attention.</p>
<h3 id="few-key-points"><a href="#few-key-points">Few Key Points</a><a href="#few-key-points"></a></h3>
<ul>
<li>Resource-Efficient Input Resolution Increase: Monkey enhances input resolution in LMMs without requiring extensive pre-training. Instead of directly interpolating Vision Transformer (ViT) models to handle higher resolutions, it employs a sliding window method to divide high-resolution images into smaller patches. Each patch is processed by a static visual encoder with LoRA adjustments and a trainable visual resampler.</li>
<li>Maintaining Training Data Distribution: Monkey capitalizes on encoders trained on smaller resolutions (e.g., 448×448) by resizing each patch to the supported resolution. This approach maintains the original data distribution, avoiding costly training from scratch.</li>
<li>Trainable Patches Advantage: The method uses various trainable patches, enhancing resolution more effectively than traditional interpolation techniques for positional embedding.</li>
<li>Automatic Multi-Level Description Generation: Monkey incorporates multiple advanced systems (e.g., BLIP2, PPOCR, GRIT, SAM, ChatGPT) to generate high-quality captions by combining insights from these generators. This approach captures a wide spectrum of visual details through layered and contextual understanding.</li>
</ul>
<h2 id="advantages-of-monkey"><a href="#advantages-of-monkey">Advantages of Monkey</a><a href="#advantages-of-monkey"></a></h2>
<ul>
<li>High-Resolution Support: Supports resolutions up to 1344×896 without pre-training, aiding in identifying small or densely packed objects and text.</li>
<li>Improved Contextual Associations: Enhances understanding of relationships among multiple targets and leverages common knowledge for better text description generation.</li>
<li>Performance Enhancements: Shows competitive performance across various tasks, including Image Captioning and Visual Question Answering, demonstrating promising results compared to models like GPT-4V, especially in dense text question answering.</li>
</ul>
<p>Overall, Monkey offers a sophisticated way to improve resolution and description generation in LMMs by using existing models more efficiently.</p>
<h2 id="how-can-i-do-visual-qa-with-monkey"><a href="#how-can-i-do-visual-q-a-with-monkey">How can I do visual Q&amp;A with Monkey?</a><a href="#how-can-i-do-visual-q-a-with-monkey"></a></h2>
<p>To run the Monkey Model and experiment with it, we first start a notebook, or you can start up a terminal. We highly recommend using an A4000 GPU to run the model.</p>
<p>The NVIDIA A6000 GPU is a powerful graphics card that is known for its exceptional performance in various AI and machine learning applications, including visual question answering (VQA). With its memory and advanced Ampere architecture, the A4000 offers high throughput and efficiency, making it ideal for handling the complex computations required in VQA tasks.</p>
<pre tabindex="0"><code>!nvidia-smi
</code></pre><p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/Screenshot-2024-08-09-at-1.15.59-PM.png" alt="A6000-NVIDIA GPU" />
</figure>


</p>
<h3 id="setup"><a href="#setup">Setup</a><a href="#setup"></a></h3>
<p>We will run the below code cells. This will clone the repository, and install the requirements.txt file.</p>
<pre tabindex="0"><code>git clone https://github.com/Yuliang-Liu/Monkey.git
cd ./Monkey
pip install -r requirements.txt
</code></pre><p>We can run the gradio demo which is fast and easy to use.</p>
<pre tabindex="0"><code> python demo.py
</code></pre><p>or follow the code along.</p>
<pre tabindex="0"><code>from transformers import AutoModelForCausalLM, AutoTokenizer
checkpoint = &#34;echo840/Monkey-Chat&#34;
model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=&#39;cuda&#39;, trust_remote_code=True).eval()
tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)
tokenizer.padding_side = &#39;left&#39;
tokenizer.pad_token_id = tokenizer.eod_id
</code></pre><p>The code above loads the pre-trained model and tokenizer from the Hugging Face Transformers library.</p>
<p>“echo840/Monkey-Chat” is the name of the model checkpoint we will load. Next, we will load the model weights and configurations and map the device to CUDA-enabled GPU for faster computation.</p>
<pre tabindex="0"><code>img_path = &#39;/notebooks/quick_start_pytorch_images/image 2.png&#39;
question = &#34;provide a detailed caption for the image&#34;

query = f&#39;&lt;img&gt;{img_path}&lt;/img&gt; {question} Answer: &#39;
input_ids = tokenizer(query, return_tensors=&#39;pt&#39;, padding=&#39;longest&#39;)
attention_mask = input_ids.attention_mask
input_ids = input_ids.input_ids

pred = model.generate(
    input_ids=input_ids.cuda(),
    attention_mask=attention_mask.cuda(),
    do_sample=False,
    num_beams=1,
    max_new_tokens=512,
    min_new_tokens=1,
    length_penalty = 1,
    num_return_sequences=1,
    output_hidden_states=True,
    use_cache=True,
    pad_token_id=tokenizer.eod_id,
    eos_token_id=tokenizer.eod_id,
)

response = tokenizer.decode(pred[0][input_ids.size(1):].cpu(), skip_special_tokens=True).strip()
print(response)
</code></pre><p>This code will generate the detailed caption or description or any other output based on the prompt query using Monkey. We will specify the path where we have stored our image and formulating a query string that includes the image reference and the question asking for a caption. Next, the query is tokenised using the ‘tokenizer’ which converts the input texts into token IDs.</p>
<p>Parameters such as <code>do_sample=False</code> and <code>num_beams=1</code> ensure deterministic output by disabling sampling. Other parameters like <code>max_new_tokens</code>, <code>min_new_tokens</code>, and <code>length_penalty</code> control the length and nature of the generated sequence. After generation, the output tokens are decoded back into human-readable text, skipping any special tokens, to form the final response, which is a caption describing the image. Finally, we print the generated caption.</p>
<h2 id="results"><a href="#results">Results</a><a href="#results"></a></h2>
<p>We tried the model with an extremely basic image of just a screenshot, and it does fairly well in recognizing what the image is.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/Screenshot-2024-08-09-at-1.08.44-PM.png" alt="image" />
</figure>


</p>
<p>Monkey Output:- The image displays a computer interface with a blue background. The main focus is on a machine with the label “A4000” and the price “$0.76/hr”. The machine is equipped with 45 GB RAM and 8 CPUs. There are also several buttons and icons on the interface, including a blue “STOP MACHINE” button and a white “Auto-shutdown in 11 hours” button. The text “45 GB RAM” and “8 CPUs” are also visible. The overall layout suggests a user-friendly interface for managing virtual machines.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/Screenshot-2024-08-09-at-1.11.10-PM.png" alt="image" />
</figure>


</p>
<p>Monkey Output:- In the image, a golden retriever is captured in mid-air, leaping towards a yellow tennis ball that is suspended in the air. The dog’s body is stretched out, with its front paws reaching for the ball, and its tail is raised high in excitement. The background is a lush green lawn, providing a stark contrast to the dog’s white fur. The image is taken from a low angle, making the dog appear larger and more imposing. The dog’s focused expression and the dynamic pose suggest a moment of intense playfulness.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/image--1-.png" alt="image" />
</figure>


</p>
<p>Prompt:- Provide a detailed caption for the image</p>
<p>Monkey Output:- This image is a painting of a city skyline at night. The sky is filled with swirling yellow and blue colors, creating a starry effect. In the foreground, there is a tall building with a clock on top. The painting also features a large tree with a yellow light shining on it. The overall effect is one of tranquility and beauty, reminiscent of the famous “Starry Night” painting by Vincent van Gogh.</p>
<p>We are quite impressed by the detailed descriptions and captions that provide even the minutest details of the image. The AI-generated caption is truly remarkable!</p>
<p>The below image highlights Monkey’s capabilities in various VQA tasks. Monkey analyzes questions, identifies key image elements, perceives minute text, and reasons about objects, and understands visual charts. The figure also demonstrates Monkey’s impressive captioning ability, accurately describing objects and providing summaries.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/Screenshot-2024-08-09-at-12.15.00-PM.png" alt="image" />
</figure>


</p>
<p>Monkey’s results on various tasks (<a href="https://arxiv.org/pdf/2311.06607">Image Source</a>)</p>
<h2 id="comparison-results"><a href="#comparison-results">Comparison Results</a><a href="#comparison-results"></a></h2>
<p>In qualitative analysis, Monkey was compared with GPT4V and other LMMs on the task of generating detailed captions.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/Screenshot-2024-08-09-at-12.08.32-PM.png" alt="image" />
</figure>


</p>
<p>Monkey and GPT-4V identified an “Emporio Armani” store in the background, with Monkey providing additional details, such as a woman in a red coat and black pants carrying a black purse. (<a href="https://arxiv.org/pdf/2311.06607">Image Source</a>)</p>
<p>Further experiments have shown that in many cases, Monkey has demonstrated impressive performance compared to GPT4V when it comes to understanding complex text-based inquiries.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/Screenshot-2024-08-09-at-12.10.45-PM.png" alt="image" />
</figure>


</p>
<p>The VQA task comparison results in the below figure show that by scaling up the model size, Monkey achieves significant performance advantages in tasks involving dense text. It not only outperforms QwenVL-Chat [3], LLaVA-1.5 [29], and mPLUG-Owl2 [56] but also achieves promising results compared to GPT-4V [42]. This demonstrates the importance of scaling up model size for performance improvement in multimodal large models and validates our method’s effectiveness in enhancing their performance.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2024/08/Screenshot-2024-08-09-at-12.17.48-PM.png" alt="image" />
</figure>


</p>
<p>Monkey’s comparison with GPT-4V, QwenVL-Chat, LLaVA-1.5, and mPLUG-Owl2 on VQA task.</p>
<h2 id="practical-application"><a href="#practical-application">Practical Application</a><a href="#practical-application"></a></h2>
<ul>
<li>Automated Image Captioning: Generate detailed descriptions for images in various domains, such as e-commerce, social media, and digital archives.</li>
<li>Assistive Technologies: Aid visually impaired individuals by generating descriptive captions for images in real-time applications, such as screen readers and navigation aids.</li>
<li>Interactive Chatbots: Integrate with chatbots to provide detailed visual explanations and context in customer support and virtual assistants, improving user experience in various services.</li>
<li>Image-Based Search Engines: Improve image search capabilities by providing rich, context-aware descriptions that enhance search accuracy and relevance.</li>
</ul>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/text-labeling-image-resolution-monkey-chat-vision">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
