<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Text to Vision to Image Generation - Running DeepSeeks Janus Pro on DigitalOceans GPU Droplets</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Text to Vision to Image Generation - Running DeepSeeks Janus Pro on DigitalOceans GPU Droplets</h1>
			<b><time>10.02.2025 19:25</time></b>
		       

			<div>
				<h1 id="text-to-vision-to-image-generation---running-deepseeks-janus-pro-on-digitaloceans-gpu-droplets">Text to Vision to Image Generation - Running DeepSeeks Janus Pro on DigitalOceans GPU Droplets</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>More recently, DeepSeek also released their newest version of the autoregressive framework <a href="https://huggingface.co/deepseek-ai/Janus-1.3B">Janus</a>, <a href="https://huggingface.co/deepseek-ai/Janus-Pro-7B">Janus Pro</a>. Janus-Pro is a unified understanding and generation Multimodal Large Language Model that is capable of interpreting and generating both image and text data. In it does this by “by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder’s roles in understanding and generation, but also enhances the framework’s flexibility.”</p>
<p>Follow along with this article to learn how Janus Pro works, how it compares to other multimodal LLMs, and how to run Janus Pro on a <a href="/products/gpu-droplets">DigitalOcean GPU Droplet</a>.</p>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<p>Python: Experience with Python code is required to follow along Deep Learning: This article will explore advanced concepts in deep learning</p>
<h2 id="the-janus-pro-framework"><a href="#the-janus-pro-framework">The Janus Pro Framework</a><a href="#the-janus-pro-framework"></a></h2>
<p>The Janus model family is based on the autoregressive transformer, which determines the probabilistic correlation between elements in a sequence to infer the following element. The unique approach of Janus is the decoupling of the encoding methods to convert the raw inputs into features. These are then processed by an unified autoregressive transformer. In practice, this allows for the creation of a combined model for both visual understanding and image synthesis.</p>
<p>In this section, we will explore what allowed the Janus architecture and framework to achieve such great results.</p>
<h3 id="janus-pro-architecture"><a href="#janus-pro-architecture">Janus Pro Architecture</a><a href="#janus-pro-architecture"></a></h3>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/Screenshot%202025-02-05%20at%203.30.49%E2%80%AFPM.png" alt="Janus pro architecture" />
</figure>


</p>
<p>The core architecture of Janus Pro is the same as its predecessor, Janus. In Janus models, the defining characteristic of their processing is the decoupled visual encoding for multimodal vision and generation. The independent encoders are then used to interpret the features from the inputs. They are then processed by a unified autoregressive transformer.</p>
<p>For multimodal understanding, they use the SigLIP (Sigmoid Loss for Language Image Pre-Training) to extract the coarse features from the image. These features are then flattened to a 1-dimensional representation where an adaptor maps the features to the input space of the LLM.</p>
<p>For generation tasks, the VQ tokenizer converts the image features into discrete IDs and flattens the sequence to a single dimension. “They then use a generation adaptor to map the codebook embeddings for each ID into the input space of the LLM. They then concatenate these feature sequences to form a multimodal feature sequence, which is subsequently fed into the LLM for processing. The built-in prediction head of the LLM is utilized for text predictions in both the pure text understanding and multimodal understanding tasks, while a randomly initialized prediction head is used for image predictions in the visual generation task. The entire model adheres to an autoregressive framework without the need for specially designed attention masks” (<a href="https://arxiv.org/pdf/2410.13848">Source</a>).</p>
<p><strong>Info:</strong> Deploy DeepSeek R1, the open-source advanced reasoning model that excels at text generation, summarization, and translation tasks. As one of the most computationally efficient open-source LLMs available, you’ll get high performance while keeping infrastructure costs low with DigitalOcean’s GPU Droplets.</p>
<p>→<a href="https://cloud.digitalocean.com/gpus/new?region=nyc2&amp;size=gpu-h100x8-640gb&amp;fleetUuid=e31b0c76-06d2-49d4-b446-2b81a38007c7&amp;appId=177155486&amp;image=deepseek-r1-671b&amp;type=applications">Deploy your model in just one click</a></p>
<h3 id="janus-pro-training-strategy"><a href="#janus-pro-training-strategy">Janus Pro Training Strategy</a><a href="#janus-pro-training-strategy"></a></h3>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/Screenshot%202025-02-07%20at%2012.20.55%E2%80%AFPM.png" alt="janus stages" />
</figure>


</p>
<p>To achieve these results, Janus Pro used an optimized version of the three stage training process from Janus. Stage 1 trains the adaptors and image head, stage 2 is the unified pretraining of everything but the generation and understanding encoder, and stage 3 supervised finetuning of the understanding encoder. Let’s look at these in more detail.</p>
<p>In Stage 1, the goal is to train a connection between the visual and textual features in the embedding space. This functionally facilitates the LLMs to understand image elements and have the beginnings of image generation capabilities. During this stage, the model is frozen with only the understanding adaptor, generation adaptor and image head being updated. In Janus Pro, this process is extended for more training steps. More training on ImageNet allowed for model pixel dependence and superior image generation capabilities on limited categories of images. (<a href="https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf">Source</a>)</p>
<p>In Stage 2, the LLM is unfrozen and they perform unified pretraining on a multimodal corpus to allow Janus to learn and understand pure text data, multimodal understanding data, and visual generation data (<a href="https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf">source</a> ). In Janus Pro, they ignore ImageNet completely at this stage, and instead use text-to-image data to generate images based on dense descriptions. This improved both training efficiency and overall robustness of the image generation capabilities. (<a href="https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf">Source</a>)</p>
<p>In Stage 3, all parameters of the pretrained model, except the generation encoder, are fine-tuned with instruction tuning data to enhance the model’s instruction-following and dialogue capabilities. This refines its capabilities to better follow that of conventional instruction-response LLMs. To ensure consistent improvement across all modalities, the fine-tuning data consists of multimodal data, pure text data, and text to image data. In Janus Pro, they use an adjusted ratio of this data split. They found that slightly reducing the text-to-image data proportion actually improves multimodal performance without affecting generation capabilities significantly.</p>
<p>It is thanks to this tripartite training paradigm that Janus Pro is capable of such a wide variety of deep learning tasks. In our experiments, we found the model to be extremely capable for any of the tasks we gave it including instruction-response, multimodal understanding of image data, and text-to-image generation.</p>
<h2 id="janus-pro-running-on-digitalocean-gpu-droplets"><a href="#janus-pro-running-on-digitalocean-gpu-droplets">Janus Pro running on DigitalOcean GPU Droplets</a><a href="#janus-pro-running-on-digitalocean-gpu-droplets"></a></h2>
<p><!-- raw HTML omitted -->View YouTube video<!-- raw HTML omitted --></p>
<p>To get started, you will need a DigitalOcean GPU Droplet. If you haven’t created one before, we recommend following the steps shown in this <a href="/community/tutorials/getting-started-with-llama">tutorial</a>, the <a href="https://docs.digitalocean.com/products/droplets/how-to/gpu/create/">documentation</a>, or by watching the video above.</p>
<p>Once your GPU Droplet is set up, open the web console or SSH in using your local terminal. Then, paste the following code into the terminal window.</p>
<pre tabindex="0"><code>apt get install -y git-lfs pip3
git-lfs clone https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B
pip install -r requirements.txt spaces omegaconf einops timm spaces torchvision attrdict 
python app.py - -share
</code></pre><p>This will download the Janus Pro model into the HuggingFace cache, and then launch the web application run by Gradio. This can be accessed anywhere on any browser by using the shared, public link.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/Screenshot%202025-02-10%20at%206.48.38%E2%80%AFPM.png" alt="janus explaining a meme" />
</figure>


</p>
<p>To get started, upload an image to the GUI. Then ask the GUI a question about the image. For example, we found the model quite capable at interpreting memes and scientific equations. It’s also incredible for image captioning.</p>
<p>Next, tab over to the image generator and try your hand at the generation. While nowhere near the capabilities of FLUX or Stable Diffusion, we are impressed by the versatility of the model.</p>
<p>Overall, we found Janus Pro to be a very capable multimodal understanding LLM and with image generation capabilities.</p>
<h2 id="closing-thoughts"><a href="#closing-thoughts">Closing Thoughts</a><a href="#closing-thoughts"></a></h2>
<p>In conclusion, Janus Pro is an incredibly interesting model. It is capable as both an LLM, visual understanding model, and image generator. We look forward to seeing how future efforts with autoregressive models continue to advance the field.</p>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/deepseek-janus-pro-gpu-droplets">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
