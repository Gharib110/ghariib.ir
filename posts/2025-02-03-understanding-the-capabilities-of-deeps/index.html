<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Understanding the Capabilities of DeepSeek R1 Large Language Models</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Understanding the Capabilities of DeepSeek R1 Large Language Models</h1>
			<b><time>03.02.2025 14:25</time></b>
		       

			<div>
				<h1 id="understanding-the-capabilities-of-deepseek-r1-large-language-models">Understanding the Capabilities of DeepSeek R1 Large Language Models</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>In the first part of this series, we introduced <a href="/community/tutorials/deepseek-r1-gpu-droplets">DeepSeek R1 and showed how to run the model using Ollama</a>. In this follow up, we will begin with a deeper dive into what actually makes R1 so special. We will focus on analyzing model’s unique Reinforcement Learning (RL) paradigm to see how reasoning capabilities of LLMs can be incentivized purely through RL, and, afterwards, discuss how the distillation of these techniques to other models allows us to share these capabilites with existing releases. We will conclude with a short demonstration on how to setup and run DeepSeek R1 models with <a href="/products/gpu-droplets">GPU Droplets</a> using <a href="https://marketplace.digitalocean.com/apps/deepseek-r1-671b-8x">1-Click Model GPU Droplets</a>.</p>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<ul>
<li>Deep Learning: this article will cover intermediate to advanced topics related to neural network training and reinforcement learning</li>
<li>DigitalOcean account: We will specifically make use of DigitalOcean’s HuggingFace 1-Click Model GPU Droplets to test R1</li>
</ul>
<h2 id="deepseek-r1-overview"><a href="#deepseek-r1-overview">DeepSeek R1 Overview</a><a href="#deepseek-r1-overview"></a></h2>
<p>The goal of the DeepSeek R1 research project was to recreate the effective reasoning capabilities shown by powerful reasoning models, namely OpenAI’s <a href="https://openai.com/o1/">O1</a>. To achieve this, they sought to improve their existing work, <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base">DeepSeek-v3-Base</a>, using pure reinforcement learning. This lead to the emergence of DeepSeek R1 Zero, which exhibits super performance on reasoning benchmarks, but lacks human interpretability and showed some unusual behaviors like language mixing.</p>
<p>To ameliorate these problems, they proposed DeepSeek R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. R1 achieved SOTA LLM readibility and utility by fine-tuning the DeepSeek-v3-Base model on thousands of cold-start data examples, then performing another round of Reinforcement Learning, followed by performing supervised fine-tuning on a reasoning dataset, and finally finishing with a final round of Reinforcement Learning. They then distilled the technique to other models by supervised fine-tuning them on data collected from R1.</p>
<p>Follow along for a deeper dive into these stages of development, and a discussion for how these improved the model iteratively to reach the capabilities of DeepSeek R1.</p>
<h3 id="training-deepseek-r1-zero"><a href="#training-deepseek-r1-zero">Training DeepSeek R1 Zero</a><a href="#training-deepseek-r1-zero"></a></h3>
<p>To create DeepSeek R1 Zero, the baseline model from which R1 was developed, the researchers applied RL directly to the base model without any SFT data. The chosen RL paradigm they selected is called Group Relative Policy Optimization (GRPO). This process was adapted from the <a href="https://arxiv.org/pdf/2402.03300">DeepSeekMath</a> paper.</p>
<p>GRPO is similar to familiar, other RL systems, but differs in one significant way: it does not use a critic model. Instead, GRPO estimates the baseline from group scores instead. The reward modeling has two rules for this system that each rewards accuracy and format adherence to a template. The reward then acts as the source of the training signal, which then is used to modify the optimization direction of RL. This rule based system allows the RL process to iteratively modify and improve the model.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/Screenshot%202025-02-03%20at%2011.40.13%E2%80%AFAM.png" alt="template for RL training" />
</figure>


</p>
<p>The training template itself is a simple writing format that guides the base model to adhere to our specified instructions, as shown above. The model measures the responses to the adjusted “prompt” for each step of RL. “This is a noteworthy achievement, as it underscores the model’s ability to learn and generalize effectively through RL alone” (<a href="https://arxiv.org/pdf/2501.12948">Source</a>).</p>
<p>This self evolution of the model leads it to develop its powerful reasoning capabilities, including self-reflection and consideration of alternative approaches. This is further enhanced by a moment during training the research team calls the model’s “Aha moment”. “During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes” (<a href="https://arxiv.org/pdf/2501.12948">Source</a>).</p>
<p>DeepSeek R1 Zero performed extremely well across benchmarks, but suffered strongly in terms of readibility and utility compared to proper, human-adapted LLMs. The research team thus proposed DeepSeek R1 to better enhance the model for human level tasks.</p>
<h3 id="from-deepseek-r1-zero-to-deepseek-r1"><a href="#from-deepseek-r1-zero-to-deepseek-r1">From DeepSeek R1 Zero to DeepSeek R1</a><a href="#from-deepseek-r1-zero-to-deepseek-r1"></a></h3>
<p>To go from the relatively untamed DeepSeek R1 Zero to the much more functional DeepSeek R1, the researchers introduced several training stages.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/Screenshot%202025-02-03%20at%2011.48.29%E2%80%AFAM.png" alt="image" />
</figure>


</p>
<p>To start, DeepSeek-v3-Base was fine-tuned on thousands of cold-start data pieces before initiating the same RL paradigm used for DeepSeek R1 Zero with an additional reward for consistent language in outputs. In practice, this phase works to enhance the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions (<a href="https://arxiv.org/pdf/2501.12948">Source</a>).</p>
<p>When this RL stage completes, they use the resultant model to collect new data for supervised fine-tuning. “Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model’s capabilities in writing, role-playing, and other general-purpose tasks” (<a href="https://arxiv.org/pdf/2501.12948">Source</a>).</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/Screenshot%202025-02-03%20at%2011.52.50%E2%80%AFAM.png" alt="RL training R1 zero" />
</figure>


</p>
<p>Next, a second RL stage is implemented to improve the model’s “helpfulness and harmlessness while simultaneously refining its reasoning capabilities” (<a href="https://arxiv.org/pdf/2501.12948">Source</a>). By training the model further on diverse prompt distributions with reward signals, they are able to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. This helps with the models’ “human-like” responsiveness. This helps the model to evolve the incredible reasoning capabilities it is known for. Over time, this process helps the model develop its characteristic long chains of thought and reasoning.</p>
<h3 id="deepseek-r1-capabilities"><a href="#deepseek-r1-capabilities">DeepSeek R1 Capabilities</a><a href="#deepseek-r1-capabilities"></a></h3>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/Screenshot%202025-02-03%20at%2012.07.44%E2%80%AFPM.png" alt="metrics for R1 capabilities" />
</figure>


</p>
<p>Across the board, R1 demonstrates state of the art performance on reasoning benchmarks. On certain tasks, such as math, it even has shown to outperform the metrics released for O1. Overall, there is extremely high performance on stem related questions as well, which is primarily attributed to the large-scale reinforcement learning. In addition to STEM subjects, the model is highly proficient at question answering, instruction tasks, and complex reasoning. The authors argue that these improvements and enhanced capabilities are owed to the evolution of the models Chain of Thought processing through Reinforcement Learning. The long Chain of Thought data used throughout reinforcement learning and fine-tuning to encourage the model to deliver longer, more introspective outputs.</p>
<h3 id="deepseek-r1-distilled-models"><a href="#deepseek-r1-distilled-models">DeepSeek R1 Distilled models</a><a href="#deepseek-r1-distilled-models"></a></h3>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/Screenshot%202025-02-03%20at%202.11.49%E2%80%AFPM.png" alt="R1 distilled models evaluation" />
</figure>


</p>
<p>To extend the capabilities of DeepSeek R1 to smaller models, the authors collected 800000 samples from DeepSeek R1 and used those to fine-tune models like QWEN and LLAMA. They found that this relatively straight-forward distillation technique allows for the transfer of the R1 reasoning capabilities to these new models with a high-degree of success. They did this without any additional RL, showcasing the power of the original models responses to do model distillation.</p>
<h2 id="launching-deepseek-r1-on-gpu-droplets"><a href="#launching-deepseek-r1-on-gpu-droplets">Launching DeepSeek R1 on GPU Droplets</a><a href="#launching-deepseek-r1-on-gpu-droplets"></a></h2>
<p>Launching DeepSeek R1 on <a href="/products/gpu-droplets">GPU Droplets</a> is very straightforward if you already have a DigitalOcean account. Be sure to sign in before proceeding further.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/2025/James/gpu%20select.gif" alt="how to launch a DeepSeek 1-click model" />
</figure>


</p>
<p>We provide access to R1 as a 1-Click Model GPU Droplet. To launch it, simply open up the <a href="https://cloud.digitalocean.com/gpus/new">GPU Droplet console</a>, navigate to the “<a href="/products/ai-ml/1-click-models">1-Click Models</a>” tab in the template selection window, and start up the machine!</p>
<p>From there, the model will be accessible by following the HuggingFace or OpenAI methologies for communicating for the model. Use the following script to interact with your model with Python code.</p>
<pre tabindex="0"><code>import os
from huggingface_hub import InferenceClient

client = InferenceClient(base_url=&#34;http://localhost:8080&#34;, api_key=os.getenv(&#34;BEARER_TOKEN&#34;))

chat_completion = client.chat.completions.create(
    messages=[
        {&#34;role&#34;:&#34;user&#34;,&#34;content&#34;:&#34;What is Deep Learning?&#34;},
    ],
    temperature=0.7,
    top_p=0.95,
    max_tokens=128,
)
## or use OpenAI formatting
#import os
#from openai import OpenAI
#
#client = OpenAI(base_url=&#34;http://localhost:8080/v1/&#34;, api_key=os.getenv(&#34;BEARER_TOKEN&#34;))
#
#chat_completion = client.chat.completions.create(
#    model=&#34;tgi&#34;,
#    messages=[
#        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},
#        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is Deep Learning?&#34;},
#    ],
#    temperature=0.7,
#    top_p=0.95,
#    max_tokens=128,
#)
</code></pre><p>Alternatively, we have created a custom personal assistant that works on the same system. We recommend using the personal assistant for these tasks, as it abstracts much of the complication of directly interacting with the model by putting everything in a nice GUI window. To learn more about using the personal assistant script, please check out this <a href="/community/tutorials/1click-model-personal-assistant">tutorial</a>.</p>
<h2 id="closing-thoughts"><a href="#closing-thoughts">Closing Thoughts</a><a href="#closing-thoughts"></a></h2>
<p>In conclusion, R1 is an incredible step forward for the LLM development community. Their process promises to save millions of dollars on training costs while offering comparable or even better performance than state of the art closed source models. We will be watching DeepSeek closely to see how they continue to grow as their model gains international recognition.</p>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/deepseek-r1-large-language-model-capabilities">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
