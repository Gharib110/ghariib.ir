<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Fast-track generative AI security with Microsoft Purview</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Fast-track generative AI security with Microsoft Purview</h1>
			<b><time>03.02.2025 00:00</time></b>
		       

			<div>
				<p>As a data security global black belt, I help organizations secure AI solutions. They are concerned about data oversharing, data leaks, compliance, and other potential risks. Microsoft Purview is Microsoft’s solution for securing and governing data in generative AI.</p>
<p>I’m often asked how long it takes to deploy Microsoft Purview. The answer depends on the specifics of the organization and what they want to achieve. Microsoft Purview should enable a comprehensive data governance program but it can provide risk mitigation for generative AI in the short term while the program is underway.</p>
<h2 id="microsoft-purview">Microsoft Purview</h2>
<p>Secure and govern your entire data estate.</p>
<p>Explore solutions</p>
<p>
<figure>
  <img src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2025/01/CLO25-Security-Lifestyle-Getty-1084167628-1-1024x684.jpg" alt="Two colleagues collaborating at a desk." />
</figure>


</p>
<p>Organizations need AI solutions to add value for their customers and to stay competitive. They can’t wait for years to secure and govern these systems.</p>
<p>For the organizations deploying generative AI, “how long does it take to deploy Microsoft Purview?” isn’t the right question.</p>
<p>The risk mitigation Microsoft Purview provides for AI can begin on day one. This includes Microsoft AI, like Microsoft 365 Copilot, AI that an organization builds in-house, and AI from third parties like Google Gemini or ChatGPT.</p>
<p>This post will discuss ways we can secure and govern data used or generated by AI quickly, with minimal user impact, change management, and resources required.</p>
<p>These Microsoft Purview solutions are:</p>
<ul>
<li>
<p>Microsoft Purview Data Security Posture Management for AI</p>
</li>
<li>
<p>Microsoft Purview Information Protection</p>
</li>
<li>
<p>Microsoft Purview Data Loss Prevention</p>
</li>
<li>
<p>Microsoft Purview Communications Compliance</p>
</li>
<li>
<p>Microsoft Purview Insider Risk Management</p>
</li>
<li>
<p>Microsoft Purview Data Lifecycle Management</p>
</li>
<li>
<p>Microsoft Purview Audit and Microsoft Purview eDiscovery</p>
</li>
<li>
<p>Microsoft Purview Compliance Manager</p>
</li>
</ul>
<p>Here are short term steps you can take while the comprehensive data governance program is underway.</p>
<h2 id="microsoft-purview-data-security-posture-management-for-ai">Microsoft Purview Data Security Posture Management for AI</h2>
<p>Microsoft Purview Data Security Posture Management for AI (DSPM for AI) provides visibility into data security risks. It reports on:</p>
<ul>
<li>
<p>User’s interactions with AI.</p>
</li>
<li>
<p>Sensitive information in the prompts users share with the AI.</p>
</li>
<li>
<p>Whether the sensitive information users share is labeled and thus is protected by durable security policy controls.</p>
</li>
<li>
<p>Whether and how user interactions may be violating company policy including codes of conduct and attempts at jailbreak, where users manipulate the system to circumvent protections.</p>
</li>
<li>
<p>The risk level of users interacting with the system, such as inadvertent or malicious activities they may be involved in that put the organization at risk.</p>
</li>
</ul>
<p>DSPM for AI reports on this for each AI application and can drill down from the reports to the individual user activities. DSPM for AI collects and surfaces insights from the other Microsoft Purview solutions around generative AI risks in a single screen.</p>
<p>Custom sensitive information types, sensitivity labels, and information protection rules are reasoned over by DSPM for AI, but if these are not available, more than 300 out-of-the-box sensitive information types are available from day one.  </p>
<p>DSPM for AI will use these to report on risk for the organization without additional configuration. The organization’s administrators can configure policy to mitigate these risks directly from the DSPM for AI tool.</p>
<p>
<figure>
  <img src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2025/01/fig-1-DSPM-for-AI-Overview-scaled.jpg" alt="Screenshot of Data Security Posture Management for AI overview page. It shows interactions with Microsoft 365 Copilot, Enterprise Generative AI  from other providers and AI developed in-house." />
</figure>


</p>
<p><em>Figure 1. DSPM for AI shows interactions with Microsoft 365 Copilot, enterprise generative AI from other providers, and AI developed in-house.</em></p>
<p>
<figure>
  <img src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2025/01/fig-2-DSPM-for-AI-reports-scaled.jpg" alt="Screenshot of Data Security Posture Management (DSPM) for AI reports showing user interactions with sensitive data for Microsoft 365 Copilot and other generative AI.  Admins can configure policy to mitigate risks from the DSPM solution." />
</figure>


</p>
<p><em>Figure 2. DSPM for AI Reports on generative AI user interactions with sensitive data.</em></p>
<p>A big concern that organizations have in widely deploying generative AI is that it will return results that contain sensitive information that the user should not have access to. SharePoint sites have been created over the years, are unlabeled, and may be accessible to the entire organization through the AI. The “security by obscurity” that may have prevented the sensitive information from being inappropriately shared is now negated by the AI that reasons over and returns the data.</p>
<p>Data assessments, part of DSPM for AI, and currently in preview, identifies potential oversharing risks and allows the administrator to apply a sensitivity label to the SharePoint sites, the sensitive data, or initiate an Microsoft Entra ID user access review to manage group memberships.</p>
<p>The administrator can engage the business stakeholder who has knowledge of the risk posed by the data and invite them to mitigate the risk or apply the policy at scale from the Microsoft Purview administration portal.</p>
<p>
<figure>
  <img src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2025/01/fig-3-Data-Oversharing-Assessment-1024x515.webp" alt="Screenshot of Oversharing Assessment report, a feature of Data Security Posture Management for AI.  Shows the location of sensitive data and allows admins to configure policies to mitigate oversharing risks." />
</figure>


</p>
<p><em>Figure 3. Data assessment—visualize risk, review access, and deploy policy.</em></p>
<p>Learn more about Microsoft Purview Data Security Posture Management for AI</p>
<h2 id="microsoft-purview-information-protection">Microsoft Purview Information Protection</h2>
<p>The document access controls of Microsoft Purview Information Protection, including sensitivity labels, are enforced when the data is reasoned over by AI. The user is given visibility in context that they are working with sensitive information. This awareness empowers users to protect the organization. </p>
<p>The sensitivity labels that enforce scoped encryption, watermarking, and other protections travel with the document as the user interacts with the AI. When the AI creates new content based on the document, the new content inherits the most restrictive label and policy.</p>
<p>Microsoft Purview can automatically apply sensitivity labels to AI interactions based on the organization’s existing policy for email, desktop applications, and Microsoft Teams, or new policy can be deployed for the AI.</p>
<p>These can be based on out-of-the-box sensitive information types for a quick start.</p>
<p>Learn more about Microsoft Purview Information Protection</p>
<h2 id="microsoft-purview-data-loss-prevention">Microsoft Purview Data Loss Prevention</h2>
<p>The Microsoft Purview Data Loss Prevention policies that the organization currently uses for email, desktop applications, and Teams can be extended to the AI or new policy for the AI can be created. Cut and paste of sensitive information or transfer of a labeled document into the AI can be prevented or only allowed with an auditable justification from the user.</p>
<p>A rule can be configured to prevent all documents bearing a specific label from being reasoned over by the AI. Out-of-the-box sensitive information types can be used for a quick start.</p>
<p>Learn more about Microsoft Purview Data Loss Prevention</p>
<h2 id="microsoft-purview-communication-compliance">Microsoft Purview Communication Compliance</h2>
<p>Microsoft Purview Communication Compliance provides the ability to detect regulatory compliance (for example, SEC or FINRA) and business conduct violations such as sensitive or confidential information, harassing or threatening language, and sharing of adult content.</p>
<p>Out-of-the-box policies can be used to monitor user prompts or AI-generated content. It provides policy enforcement in near real time and also audit logs and reporting.</p>
<p>Learn more about Microsoft Purview Communication Compliance</p>
<h2 id="microsoft-purview-insider-risk-management">Microsoft Purview Insider Risk Management</h2>
<p>Microsoft Purview Insider Risk Management correlates signal to identify potential malicious or accidental behaviors from legitimate users. Pre-configured generative AI-specific risk detections and policy templates are now available in preview.</p>
<p>As the Insider Risk Management solution algorithms determine a user to be engaging in risky behavior, the data loss prevention (DLP) policies for that user can be made stricter using a feature called Adaptive Protection. It can be configured with out-of-the-box policies. This continuous monitoring and policy modulation mitigates risk while reducing administrator workload.</p>
<p>AI analytics can be activated from the Microsoft Purview portal to provide insights even before the Insider Risk Management solution is deployed to users. This quickly surfaces AI risks with minimal administrative workload.</p>
<p>Learn more about Microsoft Purview Insider Risk Management</p>
<h2 id="microsoft-purview-data-lifecycle-management">Microsoft Purview Data Lifecycle Management</h2>
<p>Microsoft Purview can enforce AI Data Lifecycle Management, with retention of AI prompts, prompt returns, and the documents AI creates for a specified time period. This can be done globally for every interaction with an AI solution. It can be done with out-of-the-box or custom policies. This will keep these interactions available for future investigations, for regulatory compliance, or to tune policies and inform the governance program.</p>
<p>A policy for deletion of AI interactions can be enforced so information is not over-retained.</p>
<p>Learn more about Microsoft Purview Data Lifecycle Management</p>
<h2 id="microsoft-purview-audit-and-microsoft-purview-ediscovery">Microsoft Purview Audit and Microsoft Purview eDiscovery</h2>
<p>The organization will need to support internal investigations around the use of AI. Microsoft Purview Audit logs and retains these interactions. They also need to support their legal team should they have to produce AI interactions to support litigation.</p>
<p>Learn more about Microsoft Purview Audit</p>
<p>Microsoft Purview eDiscovery can put a user’s interactions with the AI as well as their other Microsoft 365 documents and communications on hold so that their availability to support investigations is maintained. It allows them to be searched based metadata, enhancing relevancy, annotated, and produced.</p>
<p>Learn more about Microsoft Purview eDiscovery</p>
<h2 id="microsoft-purview-compliance-manager">Microsoft Purview Compliance Manager</h2>
<p>Microsoft Purview Compliance Manager has pre-built assessments for AI regulations including:</p>
<ul>
<li>
<p>EU Artificial Intelligence Act.</p>
</li>
<li>
<p>ISO/IEC 23894:2023.</p>
</li>
<li>
<p>ISO/IEC 42001:2023.</p>
</li>
<li>
<p>NIST AI Risk Management Framework (RMF) 1.0.</p>
</li>
</ul>
<p>These assessments are available to benchmark compliance over time, report on control status, and maintain and produce evidence for both Microsoft and the organization’s activities that support the regulatory compliance program.</p>
<p>Learn more about Microsoft Purview Compliance Manager</p>
<h2 id="microsoft-purview-is-an-ai-enabler">Microsoft Purview is an AI enabler</h2>
<p>Without security, governance, and compliance bases being covered, the AI program puts the organization at risk. An AI program can be blocked before it deploys if the team can’t demonstrate how it is mitigating these risks.</p>
<p>The actions suggested here can all be taken quickly, and with limited effort, to set up a generative AI deployment for success.</p>
<p>Explore the Microsoft Purview product family</p>
<h2 id="learn-more">Learn more</h2>
<p>Learn more about Microsoft Purview.</p>
<p>To learn more about Microsoft Security solutions, visit our website. Bookmark the Security blog to keep up with our expert coverage on security matters. Also, follow us on LinkedIn (Microsoft Security) and Twitter (@MSFTSecurity) for the latest news and updates on cybersecurity.</p>
<p>The post Fast-track generative AI security with Microsoft Purview appeared first on Microsoft Security Blog.</p>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-21-evolving-together-the-next-chapter-in-o/">Evolving Together The Next Chapter in Our Partner Journey</a></li>
				
				<li><a href="/posts/2025-03-21-this-week-in-security-the-github-supply/">This Week in Security The Github Supply Chain Attack Ransomware Decryption and Paragon</a></li>
				
				<li><a href="/posts/2025-03-21-modernize-your-industrial-infrastructur/">Modernize Your Industrial Infrastructure for Cybersecurity and AI Readiness with Cisco Validated Designs</a></li>
				
				<li><a href="/posts/2025-03-21-cve-2025-2590---code-projects-human-res/">CVE-2025-2590 - Code-projects Human Resource Management System Cross-Site Scripting Vulnerability</a></li>
				
				<li><a href="/posts/2025-03-21-cve-2025-2589---code-projects-human-res/">CVE-2025-2589 - Code-projects Human Resource Management System Unauthorized Access Vulnerability</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
