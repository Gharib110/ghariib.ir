<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Building a Puppeteer Web Scraper with Docker on DigitalOcean App Platform</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Building a Puppeteer Web Scraper with Docker on DigitalOcean App Platform</h1>
			<b><time>10.01.2025 19:48</time></b>
		       

			<div>
				<h1 id="building-a-puppeteer-web-scraper-with-docker-on-digitalocean-app-platform">Building a Puppeteer Web Scraper with Docker on DigitalOcean App Platform</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>This led me to build <a href="https://www.racetimeinsights.com/">Race Time Insights</a>, a tool that automatically compares race results by finding athletes who’ve completed both events. The application scrapes race results from platforms like UltraSignup and Pacific Multisports, allowing runners to input two race URLs and see how other athletes performed across both events.</p>
<p>Building this tool showed me just how powerful <a href="/products/app-platform">DigitalOcean’s App Platform</a> could be. Using Puppeteer with headless Chrome in Docker containers, I could focus on solving the problem for runners while App Platform handled all the infrastructure complexity. The result was a robust, scalable solution that helps the running community make data-driven decisions about their race goals.</p>
<p>After building Race Time Insights, I wanted to create a guide showing other developers how to leverage these same technologies—Puppeteer, Docker containers, and DigitalOcean App Platform. Of course, when working with external data, you need to be mindful of things like rate limiting and terms of service.</p>
<p>Enter Project Gutenberg. With its vast collection of public domain books and clear terms of service, it’s an ideal candidate for demonstrating these technologies. In this post, we’ll explore how to build a book search application using Puppeteer in a Docker container, deployed on App Platform, while following best practices for external data access.</p>
<h2 id="project-gutenberg-book-search"><a href="#project-gutenberg-book-search">Project Gutenberg Book Search</a><a href="#project-gutenberg-book-search"></a></h2>
<p>I’ve built and shared a web application that responsibly scrapes book information from Project Gutenberg. The app, which you can find in this <a href="https://github.com/wadewegner/doappplat-puppeteer-sample">GitHub repository</a>, allows users to search through thousands of public domain books, view detailed information about each book, and access various download formats. What makes this particularly interesting is how it demonstrates responsible web scraping practices while providing genuine value to users.</p>
<h2 id="being-a-good-digital-citizen"><a href="#being-a-good-digital-citizen">Being a Good Digital Citizen</a><a href="#being-a-good-digital-citizen"></a></h2>
<p>When building a web scraper, it’s crucial to follow good practices and respect both technical and legal boundaries. Project Gutenberg is an excellent example for learning these principles because:</p>
<ol>
<li>It has clear terms of service</li>
<li>It provides robots.txt guidelines</li>
<li>Its content is explicitly in the public domain</li>
<li>It benefits from increased accessibility to its resources</li>
</ol>
<p>Our implementation includes several best practices:</p>
<h3 id="rate-limiting"><a href="#rate-limiting">Rate Limiting</a><a href="#rate-limiting"></a></h3>
<p>For demonstration purposes, we implement a simple rate limiter that ensures at least 1 second between requests:</p>
<pre tabindex="0"><code>// A naive rate limiting implementation
const rateLimiter = {
    lastRequest: 0,
    minDelay: 1000, // 1 second between requests
    async wait() {
        const now = Date.now();
        const timeToWait = Math.max(0, this.lastRequest + this.minDelay - now);
        if (timeToWait &gt; 0) {
            await new Promise(resolve =&gt; setTimeout(resolve, timeToWait));
        }
        this.lastRequest = Date.now();
    }
};
</code></pre><p>This implementation is intentionally simplified for the example. It assumes a single application instance and stores state in memory, which wouldn’t be suitable for production use. More robust solutions might use Redis for distributed rate limiting or implement queue-based systems for better scalability.</p>
<p>This rate limiter is used before every request to Project Gutenberg:</p>
<pre tabindex="0"><code>async searchBooks(query, page = 1) {
    await this.initialize();
    await rateLimiter.wait();  // Enforce rate limit
    // ... rest of search logic
}

async getBookDetails(bookUrl) {
    await this.initialize();
    await rateLimiter.wait();  // Enforce rate limit
    // ... rest of details logic
}
</code></pre><h3 id="clear-bot-identification"><a href="#clear-bot-identification">Clear Bot Identification</a><a href="#clear-bot-identification"></a></h3>
<p>A custom User-Agent helps website administrators understand who is accessing their site and why. This transparency allows them to:</p>
<ol>
<li>Contact you if there are issues</li>
<li>Monitor and analyze bot traffic separately from human users</li>
<li>Potentially provide better access or support for legitimate scrapers</li>
</ol>
<pre tabindex="0"><code>await browserPage.setUserAgent(&#39;GutenbergScraper/1.0 (Educational Project)&#39;);
</code></pre><h3 id="efficient-resource-management"><a href="#efficient-resource-management">Efficient Resource Management</a><a href="#efficient-resource-management"></a></h3>
<p>Chrome can be memory-intensive, especially when running multiple instances. Properly closing browser pages after use prevents memory leaks and ensures your application runs efficiently, even when handling many requests:</p>
<pre tabindex="0"><code>try {
    // ... scraping logic
} finally {
    await browserPage.close();  // Free up memory and system resources
}
</code></pre><p>By following these practices, we create a scraper that’s both effective and respectful of the resources it accesses. This is particularly important when working with valuable public resources like Project Gutenberg.</p>
<h2 id="web-scraping-in-the-cloud"><a href="#web-scraping-in-the-cloud">Web Scraping in the Cloud</a><a href="#web-scraping-in-the-cloud"></a></h2>
<p>The application leverages modern cloud architecture and containerization through <a href="/products/app-platform">DigitalOcean’s App Platform</a>. This approach provides a perfect balance between development simplicity and production reliability.</p>
<h3 id="the-power-of-app-platform"><a href="#the-power-of-app-platform">The Power of App Platform</a><a href="#the-power-of-app-platform"></a></h3>
<p>App Platform streamlines the deployment process by handling:</p>
<ul>
<li>Web server configuration</li>
<li>SSL certificate management</li>
<li>Security updates</li>
<li>Load balancing</li>
<li>Resource monitoring</li>
</ul>
<p>This allows us to focus on the application code while App Platform manages the infrastructure.</p>
<h3 id="headless-chrome-in-a-container"><a href="#headless-chrome-in-a-container">Headless Chrome in a Container</a><a href="#headless-chrome-in-a-container"></a></h3>
<p>The core of our scraping functionality uses <a href="https://pptr.dev/">Puppeteer</a>, which provides a high-level API to control Chrome programmatically. Here’s how we set up and use Puppeteer in our application:</p>
<pre tabindex="0"><code>const puppeteer = require(&#39;puppeteer&#39;);

class BookService {
    constructor() {
        this.baseUrl = &#39;https://www.gutenberg.org&#39;;
        this.browser = null;
    }

    async initialize() {
        if (!this.browser) {
            // Add environment info logging for debugging
            console.log(&#39;Environment details:&#39;, {
                PUPPETEER_EXECUTABLE_PATH: process.env.PUPPETEER_EXECUTABLE_PATH,
                CHROME_PATH: process.env.CHROME_PATH,
                NODE_ENV: process.env.NODE_ENV
            });

            const options = {
                headless: &#39;new&#39;,
                args: [
                    &#39;--no-sandbox&#39;,
                    &#39;--disable-setuid-sandbox&#39;,
                    &#39;--disable-dev-shm-usage&#39;,
                    &#39;--disable-gpu&#39;,
                    &#39;--disable-extensions&#39;,
                    &#39;--disable-software-rasterizer&#39;,
                    &#39;--window-size=1280,800&#39;,
                    &#39;--user-agent=GutenbergScraper/1.0 (+https://github.com/wadewegner/doappplat-puppeteer-sample) Chromium/120.0.0.0&#39;
                ],
                executablePath: process.env.PUPPETEER_EXECUTABLE_PATH || &#39;/usr/bin/chromium-browser&#39;,
                defaultViewport: {
                    width: 1280,
                    height: 800
                }
            };

            this.browser = await puppeteer.launch(options);
        }
    }

    // Example of scraping with Puppeteer
    async searchBooks(query, page = 1) {
        await this.initialize();
        await rateLimiter.wait();

        const browserPage = await this.browser.newPage();
        try {
            // Set headers to mimic a real browser and identify our bot
            await browserPage.setExtraHTTPHeaders({
                &#39;Accept-Language&#39;: &#39;en-US,en;q=0.9&#39;,
                &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
                &#39;Connection&#39;: &#39;keep-alive&#39;,
                &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
                &#39;X-Bot-Info&#39;: &#39;GutenbergScraper - A tool for searching Project Gutenberg&#39;
            });

            const searchUrl = `${this.baseUrl}/ebooks/search/?query=${encodeURIComponent(query)}&amp;start_index=${(page - 1) * 24}`;
            await browserPage.goto(searchUrl, { waitUntil: &#39;networkidle0&#39; });
            
            // ... rest of search logic
        } finally {
            await browserPage.close();  // Always clean up
        }
    }
}
</code></pre><p>This setup allows us to:</p>
<ul>
<li>Run Chrome in headless mode (no GUI needed)</li>
<li>Execute JavaScript in the context of web pages</li>
<li>Safely manage browser resources</li>
<li>Work reliably in a containerized environment</li>
</ul>
<p>The setup also includes several important configurations for running in a containerized environment:</p>
<ol>
<li><strong>Proper Chrome Arguments</strong>: Essential flags like <code>--no-sandbox</code> and <code>--disable-dev-shm-usage</code> for running in containers</li>
<li><strong>Environment-aware Path</strong>: Uses the correct Chrome binary path from environment variables</li>
<li><strong>Resource Management</strong>: Sets viewport size and disables unnecessary features</li>
<li><strong>Professional Bot Identity</strong>: Clear user agent and HTTP headers identifying our scraper</li>
<li><strong>Error Handling</strong>: Proper cleanup of browser pages to prevent memory leaks</li>
</ol>
<p>While Puppeteer makes it easy to control Chrome programmatically, running it in a container requires proper system dependencies and configuration. Let’s look at how we set this up in our Docker environment.</p>
<h3 id="docker-ensuring-consistent-environments"><a href="#docker-ensuring-consistent-environments">Docker: Ensuring Consistent Environments</a><a href="#docker-ensuring-consistent-environments"></a></h3>
<p>One of the biggest challenges in deploying web scrapers is ensuring they work the same way in development and production. Your scraper might work perfectly on your local machine but fail in the cloud due to missing dependencies or different system configurations. Docker solves this by packaging everything the application needs - from Node.js to Chrome itself - into a single container that runs identically everywhere.</p>
<p>Our Dockerfile sets up this consistent environment:</p>
<pre tabindex="0"><code>FROM node:18-alpine

# Install Chromium and dependencies
RUN apk add --no-cache \
    chromium \
    nss \
    freetype \
    harfbuzz \
    ca-certificates \
    ttf-freefont \
    dumb-init

# Set environment variables
ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true \
    PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser \
    PUPPETEER_DISABLE_DEV_SHM_USAGE=true
</code></pre><p>The Alpine-based image keeps our container lightweight while including all necessary dependencies. When you run this container, whether on your laptop or in DigitalOcean’s App Platform, you get the exact same environment with all the right versions and configurations for running headless Chrome.</p>
<h2 id="development-to-deployment"><a href="#development-to-deployment">Development to Deployment</a><a href="#development-to-deployment"></a></h2>
<p>Let’s walk through getting this project up and running:</p>
<h3 id="1-local-development"><a href="#1-local-development">1. Local Development</a><a href="#1-local-development"></a></h3>
<p>First, fork the <a href="https://github.com/wadewegner/doappplat-puppeteer-sample">example repository</a> to your GitHub account. This gives you your own copy to work with and deploy from. Then clone your fork locally:</p>
<pre tabindex="0"><code># Clone your fork
git clone https://github.com/YOUR-USERNAME/doappplat-puppeteer-sample.git
cd doappplat-puppeteer-sample

# Build and run with Docker
docker build -t gutenberg-scraper .
docker run -p 8080:8080 gutenberg-scraper
</code></pre><h3 id="2-understanding-the-code"><a href="#2-understanding-the-code">2. Understanding the Code</a><a href="#2-understanding-the-code"></a></h3>
<p>The application is structure around three main components:</p>
<ol>
<li>
<p><strong>Book Service</strong>: Handles web scraping and data extraction</p>
<pre tabindex="0"><code>async searchBooks(query, page = 1) {
 await this.initialize();
 await rateLimiter.wait();

 const itemsPerPage = 24;
 const searchUrl = `${this.baseUrl}/ebooks/search/?query=${encodeURIComponent(query)}&amp;start_index=${(page - 1) * itemsPerPage}`;

 // ... scraping logic
}
</code></pre></li>
<li>
<p><strong>Express Server</strong>: Manages routes and renders templates</p>
<pre tabindex="0"><code>app.get(&#39;/book/:url(*)&#39;, async (req, res) =&gt; {
 try {
     const bookUrl = req.params.url;
     const bookDetails = await bookService.getBookDetails(bookUrl);
     res.render(&#39;book&#39;, { book: bookDetails, error: null });
 } catch (error) {
     // Error handling
 }
});
</code></pre></li>
<li>
<p><strong>Frontend Views</strong>: Clean, responsive UI using Bootstrap</p>
<pre tabindex="0"><code>&lt;div class=&#34;card book-card h-100&#34;&gt;
 &lt;div class=&#34;card-body&#34;&gt;
     &lt;span class=&#34;badge bg-secondary downloads-badge&#34;&gt;
         &lt;%= book.downloads.toLocaleString() %&gt; downloads
     &lt;/span&gt;
     &lt;h5 class=&#34;card-title&#34;&gt;&lt;%= book.title %&gt;&lt;/h5&gt;
     &lt;!-- ... more UI elements ... --&gt;
 &lt;/div&gt;
&lt;/div&gt;
</code></pre></li>
</ol>
<h3 id="3-deployment-to-digitalocean"><a href="#3-deployment-to-digitalocean">3. Deployment to DigitalOcean</a><a href="#3-deployment-to-digitalocean"></a></h3>
<p>Now that you have your fork of the repository, deploying to DigitalOcean App Platform is straightforward:</p>
<ol>
<li>Create a new App Platform application</li>
<li>Connect to your forked rep</li>
<li>On resources, delete the second resource (that isn’t a Dockerfile); this is auto-generated by App Platform and not needed</li>
<li>Deploy by clicking Create Resources</li>
</ol>
<p>The application will be automatically built and deployed, with App Platform handling all the infrastructure details.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a><a href="#conclusion"></a></h2>
<p>This Project Gutenberg scraper demonstrates how to build a practical web application using modern cloud technologies. By combining Puppeteer for web scraping, Docker for containerization, and DigitalOcean’s App Platform for deployment, we’ve created a solution that’s both robust and easy to maintain.</p>
<p>The project serves as a template for your own web scraping applications, showing how to handle browser automation, manage resources efficiently, and deploy to the cloud. Whether you’re building a data collection tool or just learning about containerized applications, this example provides a solid foundation to build upon.</p>
<p>Check out the <a href="https://github.com/wadewegner/doappplat-puppeteer-sample">project on GitHub</a> to learn more and deploy your own instance!</p>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/build-a-puppeteer-web-scrapper-with-docker-and-app-platform">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
