<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>How we optimized vLLM for DeepSeek-R1</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>How we optimized vLLM for DeepSeek-R1</h1>
			<b><time>19.03.2025 00:00</time></b>
		       

			<div>
				<p>DeepSeek and vLLM optimizations have been a top priority for our team and the vLLM community as a whole, and we are excited to share a deep dive into our work. In this article, we will cover the key inference improvements we have made, detail the integration of DeepSeek’s latest advancements into vLLM, and discuss how we are scaling DeepSeek-R1 for real-world deployment. Additionally, we will review the various open source contributions from DeepSeek and outline our roadmap for integrating them into vLLM.</p>
<h2 id="introduction-to-vllm">Introduction to vLLM</h2>
<p>vLLM is an open source inference server designed for efficient model serving, providing a streamlined, high-performance infrastructure for large language models (LLMs). It is licensed under Apache 2.0 and can be installed via pip or deployed as a Docker image, making it easily accessible. It supports a variety of hardware platforms, including most accelerators and CPUs, ensuring wide compatibility across different infrastructures. Check out the extensive list here.</p>
<p>Neural Magic, now part of Red Hat, is a top commercial contributor to vLLM, working extensively on model and systems optimizations to improve vLLM performance at scale. The framework supports multimodal models, embeddings, and reward modeling, and is increasingly used in reinforcement learning with human feedback (RLHF) workflows. With features such as advanced scheduling, chunk prefill, Multi-LoRA batching, and structured outputs, vLLM is optimized for both inference acceleration and enterprise-scale deployment.</p>
<h2 id="deepseek-r1-a-complex-model">DeepSeek-R1: A complex model</h2>
<p>DeepSeek-R1 has been making headlines for its exceptional reasoning capabilities and its novel architectural advancements. It introduces several technical challenges due to its complexity and scale.</p>
<p>One of the defining characteristics of DeepSeek-R1 is its sheer size. With 256 experts in its MLP layers, over 671 billion parameters, and 720GBs in size, it surpasses previous models such as Mixtral, which only had 8 experts. The model is so large that it cannot fit even on an 8x H100 node, requiring innovative techniques for inference and deployment. Additionally, DeepSeek-R1 is one of the first foundation models trained using FP8 quantization, a novel approach that requires custom support for inference.</p>
<p>Prior to DeepSeek-R1, our model and system optimization efforts focused primarily on Llama-style models. The introduction of DeepSeek-R1 has required significant modifications to accommodate its new architectural features. Over the past several weeks, we have worked on performance optimizations to enhance efficiency and reduce computational overhead. While we have made substantial progress, further work is required to refine and finalize production-ready implementations.</p>
<p>An insightful chart, shared by UnSloth, illustrates the rapid pace of development in this space (Figure 1). This chart tracks the performance of various LLM serving frameworks, including those utilizing vLLM, and demonstrates how tokens per second have grown exponentially. The progress has been driven by our integration of Multi-Token Prediction (MTP), MLA, and torch.compile, among other advancements.</p>
<!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://developers.redhat.com/sites/default/files/image3_67.png" alt="Figure 1" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>Figure 1: Performance gains in tokens per second across different LLM serving frameworks, highlighting optimizations in vLLM. Source: 2025-02-27 - vLLM Office Hours - DeepSeek and vLLM.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="open-infra-week-contributions">Open Infra Week contributions</h2>
<p>DeepSeek’s Open Infra Week in February, 2025, introduced a series of inference kernel advancements aimed at accelerating model execution. Our team has focused on integrating these optimizations into vLLM and improving their performance.</p>
<p>Key contributions from Open Infra Week include:</p>
<ul>
<li><strong>FlashMLA (Multi-Head Latent Attention):</strong> A kernel for MLA that increases speeds up batched decoding.</li>
<li><strong>DPP (Dynamic Partitioning for Parallelism):</strong> A new method to balance computational loads across distributed environments.</li>
<li><strong>Speculative decoding enhancements:</strong> Techniques that boost inference speed while maintaining accuracy.</li>
</ul>
<p>We continue to collaborate with the open source community through GitHub and Slack discussions, refining and integrating these optimizations into vLLM.</p>
<h2 id="mla-multi-token-prediction-and-parallelism-optimizations">MLA, multi-token prediction, and parallelism optimizations</h2>
<p>To fully optimize vLLM for DeepSeek-R1, we focused on 3 major areas: </p>
<ul>
<li>Multi-Head Latent Attention (MLA)</li>
<li>Multi-Token Prediction (MTP)</li>
<li>Parallelism strategies</li>
</ul>
<p>These optimizations ensure that vLLM can efficiently handle the computational demands of DeepSeek-R1.</p>
<h3 id="multi-head-latent-attention-reducing-kv-cache-bottlenecks">Multi-Head Latent Attention: Reducing KV cache bottlenecks</h3>
<p>MLA dramatically reduces KV cache size by projecting key-value heads into a compressed latent space. This optimization significantly decreases memory bandwidth usage, increasing the maximum token capacity from 67K to 650K and allowing for larger batch processing throughput.</p>
<h4 id="mla-impact-on-performance">MLA: Impact on performance</h4>
<p>Reduced KV cache size enables significantly more batching and therefore throughput at inference time, as shown in Figure 2.</p>
<!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://developers.redhat.com/sites/default/files/image_2_3.png" alt="Figure 2" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>Figure 2: Comparison of KV cache size reduction using MLA in vLLM, showing increased token capacity. Source: 2025-02-27 - vLLM Office Hours - DeepSeek and vLLM.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>While MLA provides substantial benefits, its implementation presents challenges. Unlike traditional MHA, MLA involves varying Q, K, and V head dimensions, which many existing kernels do not support. We are actively working on integrating kernel-level optimizations to fully exploit MLA’s advantages.</p>
<h3 id="multi-token-prediction-enhancing-reasoning-model-performance">Multi-Token Prediction: Enhancing reasoning model performance</h3>
<p>DeepSeek-R1’s reasoning tasks require generating long sequences, making inference efficiency critical. MTP enables faster processing by predicting multiple tokens per step rather than one at a time. This leads to significant improvements in inference speed, particularly for workloads that involve extended decoding phases.</p>
<p>The team at Meta implemented MTP over the course of the past couple of weeks. Tests show that MTP provides up to a 20% speed improvement in low QPS scenarios (see Figure 3).</p>
<!-- raw HTML omitted -->
<p>
<figure>
  <img src="https://developers.redhat.com/sites/default/files/image_3_0.png" alt="Figure 3" />
</figure>


</p>
<!-- raw HTML omitted -->
<p>Figure 3: End-to-end inference speed improvements using Multi-Token Prediction (MTP). Source: 2025-02-27 - vLLM Office Hours - DeepSeek and vLLM.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="parallelism-scaling-deepseek-r1-efficiently">Parallelism: Scaling DeepSeek-R1 efficiently</h3>
<p>DeepSeek-R1’s architecture requires parallelism strategies beyond traditional tensor parallelism. These are integrations called Expert Parallelism (EP) and Data Parallelism (DP) to improve performance.</p>
<ul>
<li><strong>Expert Parallelism (EP):</strong> Assigns specific experts to dedicated GPUs, ensuring efficient utilization and reducing redundancy.</li>
<li><strong>Data Parallelism (DP):</strong> Distributes batched sequences between GPUs for the attention layers, avoiding KV cache duplication to improve memory efficiency.</li>
</ul>
<p>These techniques allow us to effectively distribute computational loads, leading to more scalable inference. Check out the Office Hours recording on Distributed Inference with vLLM.</p>
<h2 id="future-roadmap-and-next-steps">Future roadmap and next steps</h2>
<p>Looking forward, our primary goals are to finalize key optimizations and continue refining vLLM’s performance for DeepSeek-R1. Next steps include:</p>
<ul>
<li>Finalizing V1 MLA and FlashMLA support to fully integrate DeepSeek’s attention optimizations.</li>
<li>Enhancing speculative decoding techniques to further reduce inference latency.</li>
<li>Optimizing multi-node parallelism strategies to better handle DeepSeek-R1’s immense parameter count.</li>
<li>Benchmarking additional DeepSeek inference kernels to validate efficiency improvements in real-world use cases.</li>
</ul>
<p>As part of our continued development, we will be conducting large-scale benchmarking experiments and collaborating with other teams to fine-tune vLLM’s infrastructure. By focusing on both algorithmic improvements and practical deployment strategies, we aim to make vLLM the most robust and scalable inference framework for large-scale models.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Our work optimizing vLLM for DeepSeek-R1 has been a significant effort, made possible through collaboration with Neural Magic (Red Hat), DeepSeek, Berkeley, Meta, and the broader open source community. Over the past few weeks, we have made substantial progress in improving inference performance through MLA, MTP, and advanced parallelism techniques.</p>
<p>With optimizations that reduce KV cache size, enhance multi-token prediction, and introduce scalable parallelism strategies, vLLM is well-positioned to serve next-generation AI models efficiently. Looking ahead, our continued focus will be on refining and finalizing these improvements, ensuring vLLM remains the premier open source solution for deploying large-scale models.</p>
<p>We are excited about the future and look forward to sharing further updates as we push the boundaries of inference performance for DeepSeek-R1!</p>
<p>The post How we optimized vLLM for DeepSeek-R1 appeared first on Red Hat Developer.</p>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
