<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Evaluating The Key Metrics of Deep Learning Models</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Evaluating The Key Metrics of Deep Learning Models</h1>
			<b><time>03.03.2025 17:15</time></b>
		       

			<div>
				<h1 id="evaluating-the-key-metrics-of-deep-learning-models">Evaluating The Key Metrics of Deep Learning Models</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>In computer vision, object detection is the problem of locating one or more objects in an image. Besides the traditional object detection techniques, advanced deep learning models like <a href="/community/tutorials/mask-r-cnn-in-tensorflow-2-0">R-CNN</a> and <a href="/community/tutorials/yolo-nas">YOLO</a> can achieve impressive detection over different types of objects. These models accept an image as the input and return the bounding box coordinates around each detected object.</p>
<p>This tutorial discusses the confusion matrix and how precision, recall, and accuracy are calculated. The mAP will be discussed in another tutorial. Specifically, we’ll cover:</p>
<ol>
<li>Confusion Matrix for Binary Classification</li>
<li>Confusion Matrix for Multi-Class Classification</li>
<li>Calculating the Confusion Matrix with Scikit-learn</li>
<li>Accuracy, Precision, and Recall</li>
<li>Precision or Recall?</li>
<li>Conclusion</li>
</ol>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<p>In order to follow along with this article, you will need experience with <a href="/community/tutorial-series/how-to-code-in-python-3">Python</a> code and a basic understanding of Deep Learning. We will assume that all readers have access to sufficiently powerful machines so they can run the code provided. If you do not have access to a GPU, we suggest using <a href="/products/gpu-droplets">DigitalOcean GPU Droplets</a>. For instructions on getting started with Python code, we recommend the <a href="/community/tutorial-series/how-to-code-in-python-3">Python guide for beginners</a>. This guide will help you set up your system and prepare to run beginner tutorials.</p>
<h2 id="confusion-matrix-for-binary-classification"><a href="#confusion-matrix-for-binary-classification">Confusion Matrix for Binary Classification</a><a href="#confusion-matrix-for-binary-classification"></a></h2>
<p>In binary classification, each input sample is assigned to one of two classes. Generally, these two classes are assigned labels like 1 and 0 or positive and negative. More specifically, the two class labels might be something like malignant or benign (e.g., if the problem concerns cancer classification) or success or failure (e.g., if it concerns classifying student test scores). Assume there is a binary classification problem with the classes positive and negative. Here are the labels for seven samples used to train the model. These are called the sample’s ground-truth labels.</p>
<pre tabindex="0"><code>positive, negative, negative, positive, positive, positive, negative
</code></pre><p>Note that the class labels help us humans differentiate between the different classes. The thing that is of high importance to the model is a numeric score. When feeding a single sample to the model, the model does not necessarily return a class label but rather a score. For instance, when these seven samples are fed to the model, their class scores could be:</p>
<pre tabindex="0"><code>0.6, 0.2, 0.55, 0.9, 0.4, 0.8, 0.5
</code></pre><p>Based on the scores, each sample is given a class label. How do we convert these scores into labels? We do this by using a threshold. This threshold is a hyperparameter of the model and can be defined by the user. For example, the threshold could be 0.5–then any sample above or equal to 0.5 is given the positive label. Otherwise, it is negative. Here are the predicted labels for the samples:</p>
<pre tabindex="0"><code>positive (0.6), negative (0.2), positive (0.55), positive (0.9), negative (0.4), positive (0.8), positive (0.5)
</code></pre><p>For comparison, here are both the ground truth and predicted labels. At first glance, we can see 4 correct and 3 incorrect predictions. Note that changing the threshold might give different results. For example, setting the threshold to 0.6 leaves only two incorrect predictions.</p>
<pre tabindex="0"><code>Ground-Truth: positive, negative, negative, positive, positive, positive, negative
Predicted   : positive, negative, positive, positive, negative, positive, positive
</code></pre><p>A confusion matrix is used to extract more information about model performance. It helps us visualize whether the model is “confused” in discriminating between the two classes. As seen in the next figure, it is a 2×2 matrix. The labels of the two rows and columns are Positive and Negative to reflect the two class labels. In this example, the row labels represent the ground-truth labels, while the column labels represent the predicted labels. This could be changed.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig01.jpg" alt="Fig 01" />
</figure>


</p>
<p>The 4 elements of the matrix (the items in red and green) represent the 4 metrics that count the number of correct and incorrect predictions the model made. Each element is given a label that consists of two words:</p>
<ol>
<li>True or False</li>
<li>Positive or Negative</li>
</ol>
<p>It is True when the prediction is correct (i.e., a match between the predicted and ground-truth labels) and False when there is a mismatch between the predicted and ground-truth labels. Positive or Negative refers to the predicted label.</p>
<p>In summary, the first word is False whenever the prediction is wrong. Otherwise, it is True. The goal is to maximize the metrics with the word True (True Positive and True Negative), and minimize the other two metrics (False Positive and False Negative). The four metrics in the confusion matrix are thus:</p>
<ol>
<li>Top-Left (True Positive): How many times did the model correctly classify a Positive sample as Positive?</li>
<li>Top-Right (False Negative): How often did the model incorrectly classify a Positive sample as Negative?</li>
<li>Bottom-Left (False Positive): How many times did the model incorrectly classify a Negative sample as Positive?</li>
<li>Bottom-Right (True Negative): How many times did the model correctly classify a Negative sample as Negative?</li>
</ol>
<p>We can calculate these four metrics for the seven predictions we saw previously. The resulting confusion matrix is given in the next figure.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig02.jpg" alt="Fig 02" />
</figure>


</p>
<p>This is how the confusion matrix is calculated for a binary classification problem. Now let’s see how it would be calculated for a multi-class problem.</p>
<h2 id="confusion-matrix-for-multi-class-classification"><a href="#confusion-matrix-for-multi-class-classification">Confusion Matrix for Multi-Class Classification</a><a href="#confusion-matrix-for-multi-class-classification"></a></h2>
<p>What if we have more than two classes? How do we calculate these four metrics in the confusion matrix for a multi-class classification problem? Simple!</p>
<p>Assume there are 9 samples, where each sample belongs to one of three classes: <em>White</em>, <em>Black</em>, or <em>Red</em>. Here is the ground-truth data for the 9 samples.</p>
<pre tabindex="0"><code>Red, Black, Red, White, White, Red, Black, Red, White
</code></pre><p>When the samples are fed into a model, here are the predicted labels.</p>
<pre tabindex="0"><code>Red, White, Black, White, Red, Red, Black, White, Red
</code></pre><p>For easier comparison, here they are side-by-side.</p>
<pre tabindex="0"><code>Ground-Truth: Red, Black, Red,   White, White, Red, Black, Red,   White
Predicted:    Red, White, Black, White, Red,   Red, Black, White, Red
</code></pre><p>Before calculating the confusion matrix a target class must be specified. Let’s set the <em>Red</em> class as the target. This class is marked as <em>Positive</em>, and all other classes are marked as <em>Negative</em>.</p>
<pre tabindex="0"><code>Positive, Negative, Positive, Negative, Negative, Positive, Negative, Positive, Negative
Positive, Negative, Negative, Negative, Positive, Positive, Negative, Negative, Positive
</code></pre><p>There are now only two classes again (Positive and Negative). Thus, the confusion matrix can be calculated as in the previous section. Note that this matrix is just for the <em>Red</em> class.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig03.jpg" alt="Fig 03" />
</figure>


</p>
<p>For the <em>White</em> class, replace each of its occurrences as <em>Positive</em> and all other class labels as <em>Negative</em>. After replacement, here are the ground-truth and predicted labels. The next figure shows the confusion matrix for the <em>White</em> class.</p>
<pre tabindex="0"><code>Negative, Negative, Negative, Positive, Positive, Negative, Negative, Negative, Positive
Negative, Positive, Negative, Positive, Negative, Negative, Negative, Positive, Negative
</code></pre><p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig04.jpg" alt="Fig 04" />
</figure>


</p>
<p>Similarly, here is the confusion matrix for the <em>Black</em> class.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig05.jpg" alt="Fig 05" />
</figure>


</p>
<h2 id="calculating-the-confusion-matrix-with-scikit-learn"><a href="#calculating-the-confusion-matrix-with-scikit-learn">Calculating the Confusion Matrix with Scikit-Learn</a><a href="#calculating-the-confusion-matrix-with-scikit-learn"></a></h2>
<p>The popular Scikit-learn library in Python has a module called <a href="https://scikit-learn.org/stable/api/sklearn.metrics.html"><code>metrics</code></a> that can calculate the metrics in the confusion matrix.</p>
<p>For binary-class problems the <code>confusion_matrix()</code> function is used. Among its accepted parameters, we use these two:</p>
<ol>
<li><code>y_true</code>: The ground-truth labels.</li>
<li><code>y_pred</code>: The predicted labels.</li>
</ol>
<p>The following code calculates the confusion matrix for the previously discussed binary classification example.</p>
<pre tabindex="0"><code>import sklearn.metrics

y_true = [&#34;positive&#34;, &#34;negative&#34;, &#34;negative&#34;, &#34;positive&#34;, &#34;positive&#34;, &#34;positive&#34;, &#34;negative&#34;]
y_pred = [&#34;positive&#34;, &#34;negative&#34;, &#34;positive&#34;, &#34;positive&#34;, &#34;negative&#34;, &#34;positive&#34;, &#34;positive&#34;]

r = sklearn.metrics.confusion_matrix(y_true, y_pred)
print(r)

array([[1, 2],
      [1, 3]], dtype=int64)
</code></pre><p>Note that the order of the metrics differ from that discussed previously. For example, the <em>True Positive</em> metric is at the bottom-right corner while <em>True Negative</em> is at the top-left corner. To fix that, we can flip the matrix.</p>
<pre tabindex="0"><code>import numpy

r = numpy.flip(r)
print(r)

array([[3, 1],
       [2, 1]], dtype=int64)
</code></pre><p>The multilabel_confusion_matrix () function is used to calculate the confusion matrix for a multi-class classification problem, as shown below. In addition to the y_true and y_pred parameters, a third parameter named labels accepts a list of the class labels.</p>
<pre tabindex="0"><code>import sklearn.metrics
import numpy

y_true = [&#34;Red&#34;, &#34;Black&#34;, &#34;Red&#34;,   &#34;White&#34;, &#34;White&#34;, &#34;Red&#34;, &#34;Black&#34;, &#34;Red&#34;,   &#34;White&#34;]
y_pred = [&#34;Red&#34;, &#34;White&#34;, &#34;Black&#34;, &#34;White&#34;, &#34;Red&#34;,   &#34;Red&#34;, &#34;Black&#34;, &#34;White&#34;, &#34;Red&#34;]

r = sklearn.metrics.multilabel_confusion_matrix(y_true, y_pred, labels=[&#34;White&#34;, &#34;Black&#34;, &#34;Red&#34;])
print(r)

array([
      [[4 2]
      [2 1]]

       [[6 1]
        [1 1]]

        [[3 2]
         [2 2]]], dtype=int64)
</code></pre><p>The function calculates the confusion matrix for each class and returns all the matrices. The order of the matrices match the order of the labels in the <code>labels</code> parameter. To adjust the order of the metrics in the matrices, we’ll use the <code>numpy.flip()</code> function, as before.</p>
<pre tabindex="0"><code>print(numpy.flip(r[0])) # White class confusion matrix
print(numpy.flip(r[1])) # Black class confusion matrix
print(numpy.flip(r[2])) # Red class confusion matrix

# White class confusion matrix
[[1 2]
  [2 4]]

# Black class confusion matrix
[[1 1]
  [1 6]]

# Red class confusion matrix
[[2 2]
  [2 3]]
</code></pre><p>In the rest of this tutorial, we’ll focus on just two classes. The next section discusses three key metrics calculated using the confusion matrix.</p>
<h2 id="accuracy-precision-and-recall"><a href="#accuracy-precision-and-recall">Accuracy, Precision, and Recall</a><a href="#accuracy-precision-and-recall"></a></h2>
<p>The confusion matrix offers four different and individual metrics, as we’ve already seen. Based on these four metrics, other metrics can be calculated which offer more information about how the model behaves:</p>
<ol>
<li>Accuracy</li>
<li>Precision</li>
<li>Recall</li>
</ol>
<p>The next subsections discuss each of these three metrics.</p>
<h3 id="accuracy"><a href="#accuracy">Accuracy</a><a href="#accuracy"></a></h3>
<p>Accuracy is a metric that generally describes how the model performs across all classes. It is useful when all classes are of equal importance. It is calculated as the ratio between the number of correct predictions and the total number of predictions.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig06.jpg" alt="Fig 06" />
</figure>


</p>
<p>Here is how to calculate the accuracy using <a href="/community/tutorials/python-scikit-learn-tutorial">Scikit-learn</a>, based on the confusion matrix previously calculated. The variable <em>acc</em> holds the result of dividing the sum of <em>True Positives</em> and <em>True Negatives</em> over the sum of all values in the matrix. The result is <em>0.5714</em>, which means the model is <em>57.14%</em> accurate in making a correct prediction.</p>
<pre tabindex="0"><code>import numpy
import sklearn.metrics

y_true = [&#34;positive&#34;, &#34;negative&#34;, &#34;negative&#34;, &#34;positive&#34;, &#34;positive&#34;, &#34;positive&#34;, &#34;negative&#34;]
y_pred = [&#34;positive&#34;, &#34;negative&#34;, &#34;positive&#34;, &#34;positive&#34;, &#34;negative&#34;, &#34;positive&#34;, &#34;positive&#34;]

r = sklearn.metrics.confusion_matrix(y_true, y_pred)

r = numpy.flip(r)

acc = (r[0][0] + r[-1][-1]) / numpy.sum(r)
print(acc)
</code></pre><p>0.571</p>
<p>The <em><code>sklearn.metrics</code></em> module has a function called <em><code>accuracy_score()</code></em> that can also calculate the accuracy. It accepts the ground-truth and predicted labels as arguments.</p>
<pre tabindex="0"><code>acc = sklearn.metrics.accuracy_score(y_true, y_pred)
</code></pre><p>Note that the accuracy may be deceptive. One case is when the data is imbalanced. Assume there are 600 samples, where 550 belong to the Positive class and just 50 to the Negative class. Since most of the samples belong to one class, the accuracy for that class will be higher than for the other.</p>
<p>If the model made 530/550 correct predictions for the Positive class, compared to just 5/50 for the Negative class, then the total accuracy is (530 + 5) / 600 = 0.8917. This means the model is 89.17% accurate. With that in mind, for any sample (regardless of its class), the model will likely make a correct prediction 89.17% of the time. This is not valid, especially when considering the Negative class for which the model performed badly.</p>
<h3 id="precision"><a href="#precision">Precision</a><a href="#precision"></a></h3>
<p>The precision is calculated as the ratio between the number of <em>Positive</em> samples correctly classified to the total number of samples classified as <em>Positive</em> (either correctly or incorrectly). The precision measures the model’s accuracy in classifying a sample as positive.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig07.jpg" alt="Fig 07" />
</figure>


</p>
<p>When the model makes many incorrect Positive or few correct Positive classifications, this increases the denominator and makes the precision small. On the other hand, the precision is high when:</p>
<ol>
<li>The model makes many correct Positive classifications (maximize True Positive).</li>
<li>The model makes fewer incorrect Positive classifications (minimize False Positive).</li>
</ol>
<p>Imagine a man trusted by others; when he predicts something, others believe him. The precision is like this man. When the precision is high, you can trust the model when it predicts a sample as Positive. Thus, precision helps to know how accurate the model is when it says a sample is positive.</p>
<p>Now, let us understand what Precision is.</p>
<blockquote>
<p><strong>The precision reflects how reliable the model is in classifying samples as Positive</strong>.</p></blockquote>
<p>In the next figure, the green mark means a sample is classified as <em>Positive</em> and a red mark means the sample is <em>Negative</em>. The model correctly classified two <em>Positive</em> samples, but incorrectly classified one <em>Negative</em> sample as <em>Positive</em>. Thus, the <em>True Positive</em> rate is 2 and the <em>False Positive</em> rate is 1, and the precision is <code>2/(2+1)=0.667</code>. In other words, the trustiness percentage of the model when it says that a sample is <em>Positive</em> is 66.7%.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig08.jpg" alt="Fig 08" />
</figure>


</p>
<p><strong>The goal of the precision is to classify all the <em>Positive</em> samples as <em>Positive</em>, and not misclassify a negative sample as <em>Positive</em>.</strong> According to the next figure, if all the three <em>Positive</em> samples are correctly classified but one <em>Negative</em> sample is incorrectly classified, the precision is <code>3/(3+1)=0.75</code>. Thus, the model is 75% accurate when it says that a sample is positive.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig09.jpg" alt="Fig 09" />
</figure>


</p>
<p>The only way to get 100% precision is to classify all the <em>Positive</em> samples as <em>Positive</em>, in addition to not misclassifying a <em>Negative</em> sample as <em>Positive</em>.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig10.jpg" alt="Fig 10" />
</figure>


</p>
<p>In Scikit-learn, the <code>sklearn.metrics</code> module has a function named <code>precision_score()</code> which accepts the ground-truth and predicted labels and returns the precision. The <code>pos_label</code> parameter accepts the label of the <em>Positive</em> class. It defaults to <code>1</code>.</p>
<pre tabindex="0"><code>import sklearn.metrics

y_true = [&#34;positive&#34;, &#34;positive&#34;, &#34;positive&#34;, &#34;negative&#34;, &#34;negative&#34;, &#34;negative&#34;]
y_pred = [&#34;positive&#34;, &#34;positive&#34;, &#34;negative&#34;, &#34;positive&#34;, &#34;negative&#34;, &#34;negative&#34;]

precision = sklearn.metrics.precision_score(y_true, y_pred, pos_label=&#34;positive&#34;)
print(precision)
</code></pre><p>0.6666666666666666</p>
<h3 id="recall"><a href="#recall">Recall</a><a href="#recall"></a></h3>
<p>The recall is calculated as the ratio between the number of <em>Positive</em> samples correctly classified as <em>Positive</em> to the total number of <em>Positive</em> samples. The recall measures the model’s ability to detect <em>Positive</em> samples. The higher the recall, the more positive samples detected.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig11.jpg" alt="Fig 11" />
</figure>


</p>
<p>The recall cares only about how the positive samples are classified. This is independent of how the negative samples are classified, e.g. for the precision. When the model classifies all the positive samples as <em>Positive</em>, then the recall will be 100% even if all the negative samples were incorrectly classified as <em>Positive</em>. Let’s look at some examples.</p>
<p>In the next figure, there are 4 different cases (A to D) and all have the same recall which is <code>0.667</code>. Each case differs only in how the negative samples are classified. For example, case A has all the negative samples correctly classified as Negative, but case D misclassifies all the negative samples as Positive. Independently of how the negative samples are classified, the recall only cares about the positive samples.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig12.jpg" alt="Fig 12" />
</figure>


</p>
<p>Out of the 4 cases shown above, only 2 positive samples are classified correctly as positive. Thus, the <em>True Positive</em> rate is 2. The <em>False Negative</em> rate is 1 because just a single positive sample is classified as negative. As a result, the recall is <code>2/(2+1)=2/3=0.667</code>.</p>
<p>Because it does not matter whether the negative samples are classified as positive or negative, it is better to neglect the negative samples altogether as shown in the next figure. You only need to consider the positive samples when calculating the recall.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig13.jpg" alt="Fig 13" />
</figure>


</p>
<p>What does it mean when the recall is high or low? When the recall is high, it means the model can classify all the positive samples correctly as <em>Positive</em>. Thus, the model can be trusted in its ability to detect positive samples.</p>
<p>In the next figure the recall is 1.0 because all the positive samples were correctly classified as <em>Positive</em>. The <em>True Positive</em> rate is 3, and the <em>False Negative</em> rate is 0. Thus, the recall is equal to <code>3/(3+0)=1</code>. This means the model detected <em>all</em> the positive samples. Because the recall neglects how the negative samples are classified, there could still be many negative samples classified as positive (i.e. a high <em>False Positive</em> rate). The recall doesn’t take this into account.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig14.jpg" alt="Fig 14" />
</figure>


</p>
<p>On the other hand, the recall is 0.0 when it fails to detect any positive sample. In the next figure all the positive samples are <strong>incorrectly</strong> classified as <em>Negative</em>. This means the model detected 0% of the positive samples. The <em>True Positive</em> rate is 0, and the <em>False Negative</em> rate is 3. Thus, the recall is equal to <code>0/(0+3)=0</code>.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2020/09/Fig15.jpg" alt="Fig 15" />
</figure>


</p>
<p>When the recall has a value between 0.0 and 1.0, this value reflects the percentage of positive samples the model correctly classified as <em>Positive</em>. For example, if there are 10 positive samples and the recall is 0.6, this means the model correctly classified 60% of the positive samples (i.e. <code>0.6*10=6</code> positive samples are correctly classified).</p>
<p>Similar to the <code>precision_score()</code> function, the <code>recall_score()</code> function in the <code>sklearn.metrics</code> module calculates the recall. The next block of code shows an example.</p>
<pre tabindex="0"><code>import sklearn.metrics

y_true = [&#34;positive&#34;, &#34;positive&#34;, &#34;positive&#34;, &#34;negative&#34;, &#34;negative&#34;, &#34;negative&#34;]
y_pred = [&#34;positive&#34;, &#34;positive&#34;, &#34;negative&#34;, &#34;positive&#34;, &#34;negative&#34;, &#34;negative&#34;]

recall = sklearn.metrics.recall_score(y_true, y_pred, pos_label=&#34;positive&#34;)
print(recall)
</code></pre><p>0.6666666666666666</p>
<p>After defining both the precision and the recall, let’s have a quick recap:</p>
<ol>
<li>The precision measures the model’s trustworthiness in classifying positive samples, and the recall measures how many positive samples the model correctly classified.</li>
<li>The precision considers how the positive and negative samples were classified, but the recall only considers the positive samples in its calculations. In other words, the precision depends on both the negative and positive samples, but the recall is dependent only on the positive samples (and independent of the negative samples).</li>
<li>The precision considers when a sample is classified as Positive, but it does not consider correctly classifying all positive samples. The recall cares about correctly classifying all positive samples, but it does not care if a negative sample is classified as positive.</li>
<li>When a model has high recall but low precision, it classifies most of the positive samples correctly but has many false positives (i.e., it classifies many Negative samples as Positive). When a model has high precision but low recall, it is accurate when it classifies a sample as Positive but can only classify a few positive samples.</li>
</ol>
<p>Here are some questions to test your understanding:</p>
<ol>
<li>If the recall is 1.0 and the dataset has 5 positive samples, how many positive samples were correctly classified by the model? (5)</li>
<li>Given that the recall is 0.3 when the dataset has 30 positive samples, how many positive samples were correctly classified by the model? (0.3*30=9 samples)</li>
<li>If the recall is 0.0 and the dataset has 14 positive samples, how many positive samples were correctly classified by the model? (0)</li>
</ol>
<h2 id="precision-or-recall"><a href="#precision-or-recall">Precision or Recall?</a><a href="#precision-or-recall"></a></h2>
<p>The decision of whether to use precision or recall depends on the type of problem being solved. If the goal is to detect all the positive samples (without caring whether negative samples would be misclassified as positive), then use recall. Use precision if the problem is sensitive to classifying a sample as Positive in general, i.e., including Negative samples that were falsely classified as Positive.</p>
<p>Imagine being given an image and asked to detect all the cars within it. Which metric do you use? The goal is to detect all the cars and use recall. This may misclassify some objects as cars, but it eventually will work towards detecting all the target objects.</p>
<p>Now, say you’re given a mammography image, and you are asked to detect whether there is cancer or not. Which metric do you use? Because it is sensitive to incorrectly identifying an image as cancerous, we must be sure when classifying an image as Positive (i.e., has cancer). Thus, precision is the preferred metric.</p>
<h2 id="faqs"><a href="#faq-s">FAQ’s</a><a href="#faq-s"></a></h2>
<p><strong>1. What is a confusion matrix in deep learning?</strong> A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted and actual labels.</p>
<p><strong>2. How is accuracy calculated in a classification model?</strong> Accuracy = (True Positives + True Negatives) / (Total Samples).</p>
<p>*<strong>3. What is the difference between precision and recall?</strong> Precision measures correctness among positive predictions, while recall measures how many actual positives were identified.</p>
<p><strong>4. When should I prioritize precision over recall?</strong> Prioritize precision when false positives are costly, such as in medical diagnosis or fraud detection.</p>
<p><strong>5. How do I calculate the confusion matrix in Python using Scikit-learn?</strong> Use from sklearn.metrics import confusion_matrix; confusion_matrix(y_true, y_pred).</p>
<p><strong>6. How does the confusion matrix apply to multi-class classification?</strong> It extends to multi-class problems by creating an NxN matrix where N is the number of classes.</p>
<p><strong>7.What are true positives, false positives, true negatives, and false negatives?</strong></p>
<ul>
<li>True Positive (TP): Correctly predicted positive cases.</li>
<li>False Positive (FP): Incorrectly predicted positive cases.</li>
<li>True Negative (TN): Correctly predicted negative cases.</li>
<li>False Negative (FN): Incorrectly predicted negative cases.</li>
</ul>
<p><strong>8. How do accuracy, precision, and recall impact model performance?</strong> They determine how well a model balances correct predictions, minimizing false positives and false negatives.</p>
<p><strong>9. Why is accuracy sometimes misleading as an evaluation metric?</strong> Accuracy can be misleading in imbalanced datasets where one class dominates predictions.</p>
<p><strong>10. How do I interpret a confusion matrix for an object detection model?</strong> It helps assess false positives, false negatives, and correct detections, influencing IoU-based performance metrics.</p>
<p><strong>11. What are common challenges when evaluating deep learning models?</strong> Handling class imbalance, overfitting, data quality issues, and selecting the right evaluation metrics.</p>
<p><strong>12. How can I improve the precision and recall of my model?</strong> Use better data preprocessing, adjust classification thresholds, tune hyperparameters, and apply data augmentation.</p>
<p><strong>13. What is the role of a confusion matrix in object detection tasks?</strong> It helps analyze detection errors, including misclassifications, missed detections, and false alarms.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a><a href="#conclusion"></a></h2>
<p>This tutorial discussed the confusion matrix and how to calculate its 4 metrics (true/false positive/negative) in both binary and multiclass classification problems. Using the metrics module in Scikit-learn, we saw how to calculate the confusion matrix in Python. Based on these four metrics, we discussed accuracy, precision, and recall. Each metric is defined based on several examples, and the sklearn.metrics module calculates each one.</p>
<h2 id="references"><a href="#references">References</a><a href="#references"></a></h2>
<ol>
<li>Understanding Model Evaluation Metrics - <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">Scikit-learn Documentation</a></li>
<li>Accuracy, Precision, and Recall Explained - <a href="/community/tutorials/deep-learning-metrics-precision-recall-accuracy">DigitalOcean Blog</a></li>
<li>Confusion Matrix for Model Performance Analysis - <a href="https://towardsdatascience.com/">Towards DataScience Blog</a></li>
<li>Optimizing Deep Learning Model Performance - <a href="/community/conceptual-articles/optimizing-deep-learning-pipelines">DigitalOcean Blog</a></li>
<li>Hyperparameter Tuning for Better Model Metrics - <a href="/community/tutorials/hyperparameter-optimization-with-keras-tuner">DigitalOcean Blog</a></li>
<li>Measuring Object Detection Model Performance - <a href="https://paperswithcode.com/">Papers With Code</a></li>
</ol>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/deep-learning-metrics-precision-recall-accuracy">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
