<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>DeepSeek-R1 : internals made easy üêã</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>DeepSeek-R1 : internals made easy üêã</h1>
			<b><time>26.01.2025 00:00</time></b>
		       

			<div>
				<p>
<figure>
  <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fazx69go6rieenz2glovg.jpg" alt="This open source AI crushes everything - DeepSeek R1 - YouTube" />
</figure>


<br>
Well, This week has been all about DeepSeek-R1 making the headlines. So, in this post, let&rsquo;s understand right from <code>What</code> is DeepSeek-R1 model and it&rsquo;s <code>working internals</code> in depth.</p>
<h2 id="firstly-whats-deepseek-r1">Firstly, What&rsquo;s DeepSeek-R1?</h2>
<p>DeepSeek-R1 is an open-source reasoning model developed by DeepSeek, a Chinese AI company which can work on tasks that require logical inference, mathematical problem-solving, and real-time decision-making.</p>
<p>What sets reasoning models like DeepSeek-R1 and OpenAI‚Äôs o1 apart from traditional large language models (LLMs) is their ability to show how they arrived at a conclusion.</p>
<p>
<figure>
  <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F11q7ophb6w01b1hk4c6w.png" alt="Batman DeepSeek" />
</figure>


</p>
<p>As you can see in the image above, with DeepSeek-R1, you see what steps it follows for reasoning for a prompt which makes it easier to understand and if necessary, challenge its output. This capability gives reasoning models an edge in fields where outcomes need to be explainable, like research or complex decision-making.</p>
<p>Also, this model challenges the industry&rsquo;s reliance on <code>supervised fine-tuning</code> (SFT) by showing that <code>reinforcement learning</code> (RL) can improve reasoning capabilities. But again, apart from the things I mentioned above, what makes this <code>revolutionary?</code></p>
<ul>
<li><em>Autonomous Skill Emergence</em>: Unlike GPT-4 or Claude 3.5 Sonnet, which requires human-curated reasoning examples, <code>R1-Zero</code> develops skills like self-verification and multi-step planning <em>through pure RL</em>.</li>
<li><em>Cost</em>: Distilled 7B models outperform <em>GPT-4o at</em> 1/100th the training cost.</li>
<li><strong>Open Source</strong>: Full release of model weights, training code.</li>
</ul>
<h2 id="technical-architecture-">Technical Architecture :</h2>
<h3 id="base-model-foundation-">Base Model Foundation :</h3>
<p>It&rsquo;s built on top of <code>DeepSeek-V3-Base</code> model which is - a 671B parameter Mixture-of-Experts model (MoE = <em>integrating multiple specialized models, or &ldquo;experts,&rdquo; to solve complex problems more effectively</em>) with :</p>
<ul>
<li><em>16 Expert Networks</em>: which are each specialized submodels for math, code, logic, etc</li>
<li><em>Dynamic Activation</em>: 37B parameters activated per token through learned routing.</li>
<li><em>Pre-Training</em>: 4.8T (yes, <strong>Trillion</strong>) tokens spanning 52 languages and technical domains which includes STEM papers, Github repositories.</li>
</ul>
<h3 id="the-r1-variants-">The R1 Variants :</h3>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Parameters</th>
          <th>Training Approach</th>
          <th>Key Innovation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>R1-Zero</td>
          <td>671B MoE</td>
          <td>Pure RL (No SFT)</td>
          <td>Autonomous reasoning discovery</td>
      </tr>
      <tr>
          <td>R1</td>
          <td>671B MoE</td>
          <td>Multi-stage SFT+RL</td>
          <td>Human-aligned CoT generation</td>
      </tr>
      <tr>
          <td>R1-Distill</td>
          <td>1.5B‚Äì70B</td>
          <td>SFT on R1 outputs</td>
          <td>Cost-efficient deployment</td>
      </tr>
  </tbody>
</table>
<h2 id="deepseek-internals-in-depth">DeepSeek Internals in Depth:</h2>
<p>
<figure>
  <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fmiro.medium.com%2Fv2%2Fresize%3Afit%3A1400%2F0%2AUkWMT4LF0Jh0PuJm" alt="DeepSeek-R1: Affordable, Efficient, and State-of-the-Art AI Reasoning | by  LM Po | Jan, 2025 | Medium" />
</figure>


</p>
<h3 id="1-reinforcement-learning-at-its-core-">1. Reinforcement Learning at it&rsquo;s Core :</h3>
<p>DeepSeek-R1‚Äôs most groundbreaking feature is its reliance on <strong>reinforcement learning (RL)</strong> to develop reasoning capabilities. Unlike traditional LLMs that depend on <strong>supervised fine-tuning (SFT)</strong> with human-curated examples, DeepSeek-R1 uses RL to autonomously discover reasoning patterns. Here‚Äôs how it works:</p>
<h4 id="a-group-relative-policy-optimization-grpo">A. Group Relative Policy Optimization (GRPO)</h4>
<p>This is a <code>critic-free</code> RL framework that reduces the compute costs by <strong>40%</strong> when used instead of Proximal Policy Optimization (PPO).<br>
The way that this algorithm works is :</p>
<ol>
<li><strong><em>Group Sampling</em></strong> : For each prompt, the model generates G = 16 responses using the current policy. These responses form a group which lateron is used to compute rewards and advantages.</li>
<li><strong><em>Reward Normalization</em></strong>: Each response in the group is assigned a reward based on the accuracy, format and on is language consistency and Advantage Ai is calculated. This normalization helps stabilize training by reducing variance in group statistics.</li>
<li><strong><em>Policy Update</em></strong> : Maximization of advantage while constraining KL divergence. (<em>Kullback-Leibler (KL) divergence is a statistical metric that measures the difference between two probability distributions</em>). In the equation below, Œ≤=0.01 controls the strength of the KL penalty, ensuring the policy doesn‚Äôt deviate too far from the reference.</li>
</ol>
<p>
<figure>
  <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp4xbblnj8l7nreuhoj7z.jpeg" alt="Handwritten Equations" />
</figure>


</p>
<h4 id="b-hybrid-reward-engineering-">B. Hybrid Reward Engineering :</h4>
<p>This is a three-tiered reward system that prevents <code>reward hacking</code>. (<em>Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.)</em></p>
<table>
  <thead>
      <tr>
          <th>Reward Type</th>
          <th>Calculation Method</th>
          <th>Weight (Œª)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Accuracy (r_acc)</td>
          <td>Binary (1 if final answer correct)</td>
          <td>1.0</td>
      </tr>
      <tr>
          <td>Format (r_fmt)</td>
          <td>Cosine similarity to &lt;<strong>think&gt;/&lt;answer</strong>&gt; template</td>
          <td>0.3</td>
      </tr>
      <tr>
          <td>Language (r_lang)</td>
          <td>% of tokens in target language</td>
          <td>0.2</td>
      </tr>
  </tbody>
</table>
<p>Total Reward: r_total = r_acc + Œª1r_fmt + Œª2r_lang</p>
<h3 id="2-cold-start-supervised-fine-tuning-sft-">2. Cold-Start Supervised Fine-Tuning (SFT) :</h3>
<p>Before applying RL, DeepSeek-R1 goes through a cold-start SFT phase which helps in <code>seeding</code> the model with basic reasoning patterns. Now, this phase involves:</p>
<h4 id="a-curated-dataset">A. Curated Dataset</h4>
<ul>
<li>~1,000 high-quality Chain-of-Thought (CoT) examples are manually curated.</li>
<li>Each example follows a strict XML-style template:</li>
</ul>
<p>
<figure>
  <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftlbbutosbeffrfaw6qny.png" alt="vim" />
</figure>


</p>
<h4 id="b-template-enforcement-">B. Template Enforcement :</h4>
<ul>
<li>
<p>The model is fine-tuned to generate responses in the <code>&lt;think&gt;</code>/<code>&lt;answer&gt;</code> format.</p>
</li>
<li>
<p>This ensures that the reasoning process is structured and interpretable.</p>
</li>
</ul>
<h3 id="3-rejection-sampling-for-high-quality-data">3. Rejection Sampling for High-Quality Data:</h3>
<p>After the RL process, DeepSeek-R1 generates <strong>600K high-quality reasoning samples</strong> through <strong>rejection sampling</strong>. The way that it works is :</p>
<ol>
<li>
<p><em>Sample Generation</em> :</p>
<ul>
<li>The RL Model generates multiple responses for each prompt.</li>
<li>Only <em>those</em> responses that pass the <strong>rule-based</strong> checks are retained.</li>
</ul>
</li>
<li>
<p><em>Semantic Filtering</em> :</p>
<ul>
<li>Responses with low semantic coherence or incorrect reasoning are discarded.</li>
</ul>
</li>
<li>
<p><em>Final Dataset</em> :</p>
<ul>
<li>The filtered dataset is used for further fine-tuning and distillation.</li>
</ul>
</li>
</ol>
<h3 id="4-distillation-to-smaller-models">4. Distillation to Smaller Models</h3>
<p>DeepSeek-R1‚Äôs reasoning capabilities are distilled into smaller models (1.5B‚Äì70B parameters) for cost-efficient deployment. The distillation process involves:</p>
<ol>
<li>
<p><em>Dataset Creation</em> :</p>
<ul>
<li>800k samples are generated from the RL-trained model.</li>
<li>These samples include both reasoning (600k) and general tasks (200k).</li>
</ul>
</li>
<li>
<p><em>Fine-Tuning</em> :</p>
<ul>
<li>Smaller models (like Qwen-7B, Llama-70B) are fine-tuned on the distilled dataset.</li>
<li>No RL is applied during distillation, which makes it computationally efficient.</li>
</ul>
</li>
<li>
<p><em>Performance</em> :</p>
<ul>
<li>The distilled 7B model achieves <strong>55.5% pass@1</strong> on AIME 2024, outperforming GPT-4o (9.3%) at a fraction of the cost.</li>
</ul>
</li>
</ol>
<h2 id="performance-analysis-benchmarks"><strong>Performance Analysis: Benchmarks</strong></h2>
<h3 id="mathematical-reasoning">Mathematical Reasoning</h3>
<table>
  <thead>
      <tr>
          <th>Benchmark</th>
          <th>R1</th>
          <th>R1-Zero</th>
          <th>GPT-40</th>
          <th>Human Expert</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>AIME 2024 (pass@1)</td>
          <td>79.8%</td>
          <td>71.0%</td>
          <td>9.3%</td>
          <td>85%</td>
      </tr>
      <tr>
          <td>MATH-500 (pass@1)</td>
          <td>97.3%</td>
          <td>95.9%</td>
          <td>74.6%</td>
          <td>98%</td>
      </tr>
      <tr>
          <td>IMO Problem Formalization</td>
          <td>81%</td>
          <td>N/A</td>
          <td>22%</td>
          <td>89%</td>
      </tr>
  </tbody>
</table>
<p>Key Insight: R1 achieves near-human performance on Olympiad-level problems through:</p>
<ul>
<li>Step Recycling: Reusing partial solutions across similar problems</li>
<li>Symbolic-Statistical Fusion: Combining neural intuition with algebraic simplifications</li>
</ul>
<h3 id="coding--software-engineering">Coding &amp; Software Engineering</h3>
<table>
  <thead>
      <tr>
          <th>Task</th>
          <th>R1</th>
          <th>GPT-40</th>
          <th>SWE Human</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>LiveCodeBench (pass@1)</td>
          <td>65.9%</td>
          <td>32.9%</td>
          <td>72%</td>
      </tr>
      <tr>
          <td>Codeforces Elo</td>
          <td>2029</td>
          <td>759</td>
          <td>2100 (95th percentile)</td>
      </tr>
      <tr>
          <td>SWE-Bench Resolved</td>
          <td>49.2%</td>
          <td>38.8%</td>
          <td>58%</td>
      </tr>
  </tbody>
</table>
<p>Breakthroughs:</p>
<ul>
<li>Debugging Chains: Automatically generates test cases to validate code patches</li>
<li>Cross-Language Transfer: Solves Python problems then ports solutions to Rust</li>
</ul>
<p>
<figure>
  <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F6188a0i5rygu88modu4a.jpg" alt="Understanding DeepSeek R1: How Reinforcement Learning Reshapes Language  Model Reasoning? ‚Ä¢ Tech Explorer üöÄ" />
</figure>


</p>
<p>Go to Source</p>

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-19-risky-bulletin-google-buys-wiz-for-32-b/">Risky Bulletin Google buys Wiz for 32 billion</a></li>
				
				<li><a href="/posts/apple-inc-sent-you-a-payment-request-payoneer-invoices-other-microsoft-enabled-scams/">‚ÄúApple Inc sent you a payment request‚Äù Payoneer invoices; other Microsoft-enabled scams</a></li>
				
				<li><a href="/posts/glasses-that-transcribe-text-to-audio/">‚ÄúGlasses‚Äù That Transcribe Text To Audio</a></li>
				
				<li><a href="/posts/5-best-linux-centos-replacement-options-alternatives/">&lt;div&gt;5 Best Linux CentOS Replacement Options &amp; Alternatives&lt;/div&gt;</a></li>
				
				<li><a href="/posts/a-week-in-security-march-10-march-16/">&lt;div&gt;A week in security (March 10 ‚Äì March 16)&lt;/div&gt;</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
