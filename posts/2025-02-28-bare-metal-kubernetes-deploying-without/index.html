<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Bare Metal Kubernetes Deploying Without Virtualization</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>Bare Metal Kubernetes Deploying Without Virtualization</h1>
			<b><time>28.02.2025 00:00</time></b>
		       

			<div>
				<h1 id="bare-metal-kubernetes-deploying-without-virtualization">Bare Metal Kubernetes Deploying Without Virtualization</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>A bare metal Kubernetes deployment is the process of setting up a Kubernetes cluster directly on <a href="/community/conceptual-articles/what-is-bare-metal-hypervisor">bare metal</a> servers. This approach provides unparalleled performance and control, eliminating the overhead associated with virtualization. It is ideal for workloads that demand high efficiency, such as <a href="/products/ai-ml">AI/ML applications</a> and high-performance computing tasks. In this tutorial, you will learn to set up a Kubernetes cluster on bare metal infrastructure, covering prerequisites, installation steps, networking configurations, monitoring, and best practices.</p>
<h2 id="bare-metal-kubernetes-vs-vm-based-kubernetes"><a href="#bare-metal-kubernetes-vs-vm-based-kubernetes">Bare-Metal Kubernetes vs VM-based Kubernetes</a><a href="#bare-metal-kubernetes-vs-vm-based-kubernetes"></a></h2>
<p>Feature</p>
<p>Bare-Metal Kubernetes</p>
<p>VM-based Kubernetes</p>
<p>Performance</p>
<p>High</p>
<p>Moderate</p>
<p>Overhead</p>
<p>Low</p>
<p>High</p>
<p>Flexibility</p>
<p>High</p>
<p>Moderate</p>
<p>Resource Utilization</p>
<p>Efficient</p>
<p>Inefficient</p>
<p>Scalability</p>
<p>High</p>
<p>Moderate</p>
<p>Cost</p>
<p>Low</p>
<p>High</p>
<p>Bare-Metal <a href="/products/kubernetes">Kubernetes</a> is a deployment approach that sets up a Kubernetes cluster directly on <a href="/products/bare-metal-gpu">bare metal servers</a>. This approach provides high performance, low overhead, and high flexibility. It efficiently utilizes resources and is highly scalable, all at a low cost.</p>
<p>On the other hand, VM-based Kubernetes deploys a Kubernetes cluster on virtual machines. This approach provides moderate performance, high overhead, and moderate flexibility. It utilizes resources inefficiently and is moderately scalable, all at a high cost.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/006Community/Bare-metal-k8s-tutorial/bare-metal-k8s.png" alt="Difference between Bare-metal K8s and VM-Based K8s" />
</figure>


</p>
<h2 id="advantages-of-bare-metal-kubernetes-deployment"><a href="#advantages-of-bare-metal-kubernetes-deployment">Advantages of Bare Metal Kubernetes Deployment</a><a href="#advantages-of-bare-metal-kubernetes-deployment"></a></h2>
<p>Before diving into the deployment process, itâ€™s essential to understand the advantages of running Kubernetes on bare metal:</p>
<ol>
<li>
<p><strong>Enhanced Performance:</strong> Direct access to hardware resources ensures minimal latency and maximized throughput.</p>
</li>
<li>
<p><strong>Resource Efficiency</strong>: Eliminating the hypervisor layer reduces overhead, allowing applications to utilize the full potential of the hardware.</p>
</li>
<li>
<p><strong>Cost Savings</strong>: By optimizing resource utilization, organizations can achieve better performance-per-dollar compared to virtualized environments.</p>
</li>
<li>
<p><strong>Flexibility</strong>: Full control over hardware configurations enables customization tailored to specific workload requirements.</p>
</li>
</ol>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<p>Ensure the following prerequisites are met before proceeding:</p>
<p><strong>Hardware Requirements:</strong></p>
<ol>
<li>
<p><strong>Master Node</strong>: At least 4 CPUs, 16GB RAM, and 100GB SSD storage.</p>
</li>
<li>
<p><strong>Worker Nodes</strong>: At least 2 CPUs, 8GB RAM, and 100GB SSD storage per node.</p>
</li>
</ol>
<p><strong>Operating System</strong>: Ubuntu 24.04 LTS(or above) or CentOS 9 Stream installed on all nodes.</p>
<p><strong>Network Configuration:</strong></p>
<ol>
<li>
<p>Static IP addresses are assigned to each node.</p>
</li>
<li>
<p>Proper DNS settings configured.</p>
</li>
</ol>
<p><strong>Access:</strong></p>
<ol>
<li><a href="/community/tutorial-collections/how-to-set-up-ssh-keys">SSH access</a> with root or sudo privileges on all nodes.</li>
</ol>
<h2 id="step-1---prepare-the-nodes"><a href="#step-1-prepare-the-nodes">Step 1 - Prepare the Nodes</a><a href="#step-1-prepare-the-nodes"></a></h2>
<p>You need to follow these steps on all master and worker nodes.</p>
<h3 id="1-update-system-packages"><a href="#1-update-system-packages">1. Update System Packages</a><a href="#1-update-system-packages"></a></h3>
<p>Run the following command on all nodes to ensure they are up-to-date:</p>
<pre tabindex="0"><code>sudo apt update &amp;&amp; sudo apt upgrade -y
</code></pre><h3 id="2-set-hostnames-and-hosts-file"><a href="#2-set-hostnames-and-hosts-file">2. Set Hostnames and Hosts File</a><a href="#2-set-hostnames-and-hosts-file"></a></h3>
<p>Assign a unique hostname to each node. On each node, run:</p>
<pre tabindex="0"><code>sudo hostnamectl set-hostname &lt;node-name&gt;
</code></pre><p>Edit <code>/etc/hosts</code> on all nodes to include the IP addresses and hostnames of all other nodes:</p>
<pre tabindex="0"><code>192.168.1.100 master-node
192.168.1.101 worker-node1
192.168.1.102 worker-node2
</code></pre><p>Please Note that, if the nodes are on the same private network (e.g., in the same VPC or subnet), use their private IP addresses instead for better security and performance. You can use the public IPs instead of private IPs if nodes are on different networks.</p>
<h3 id="3-disable-swap"><a href="#3-disable-swap">3. Disable Swap</a><a href="#3-disable-swap"></a></h3>
<p><a href="https://phoenixnap.com/kb/swap-memory#:~:text=Swap%20memory%2C%20also%20known%20as,preventing%20system%20slowdowns%20or%20crashes.">Swap</a> is a space on a disk that is used when the amount of physical RAM memory is full. When a Linux system runs out of RAM, inactive pages are moved from the RAM to the swap space. Disabling swap is recommended for Kubernetes as it can cause issues with the container runtime. To disable the swap, run the following commands:</p>
<pre tabindex="0"><code>sudo swapoff -a
sudo sed -i &#39;/ swap / s/^/#/&#39; /etc/fstab
</code></pre><h3 id="4-load-necessary-kernel-modules"><a href="#4-load-necessary-kernel-modules">4. Load Necessary Kernel Modules</a><a href="#4-load-necessary-kernel-modules"></a></h3>
<p>Run the following on all nodes to enable the required networking modules:</p>
<pre tabindex="0"><code>sudo modprobe br_netfilter
sudo tee /etc/modules-load.d/k8s.conf &lt;&lt;EOF
br_netfilter
EOF
sudo tee /etc/sysctl.d/k8s.conf &lt;&lt;EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system
</code></pre><h2 id="step-2---install-container-runtime"><a href="#step-2-install-container-runtime">Step 2 - Install Container Runtime</a><a href="#step-2-install-container-runtime"></a></h2>
<p>A <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">container runtime</a> is a software that is responsible for running containers. It is a crucial component of any containerized environment. Some examples of famous container runtimes are <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#docker">Docker</a>, <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd">containerd</a>, and <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o">CRI-O</a>. On all master and worker nodes, you will need to install the container runtime.</p>
<p>Follow the below steps on all the master and worker nodes:</p>
<p>In this tutorial, you will use the <code>containerd</code> container runtime.</p>
<pre tabindex="0"><code>sudo apt install -y containerd
</code></pre><pre tabindex="0"><code>sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd
</code></pre><h2 id="step-3---install-kubernetes-components"><a href="#step-3-install-kubernetes-components">Step 3 - Install Kubernetes Components</a><a href="#step-3-install-kubernetes-components"></a></h2>
<p>On all the master and worker nodes, follow these steps to install the Kubernetes components:</p>
<pre tabindex="0"><code># Download the Google Cloud public signing key
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add the Kubernetes repository
echo &#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /&#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Update apt package index
sudo apt-get update

# Install kubelet, kubeadm and kubectl
sudo apt-get install -y kubelet kubeadm kubectl

# Pin their version
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre><p>Once the installation is complete, verify the installation by checking the versions:</p>
<pre tabindex="0"><code>kubectl version --client
kubeadm version
</code></pre><p>Check the status of the kubelet service:</p>
<pre tabindex="0"><code>sudo systemctl status kubelet
</code></pre><p>If the service is not active, start it:</p>
<pre tabindex="0"><code>sudo systemctl start kubelet
sudo systemctl enable kubelet
</code></pre><p>Thatâ€™s it! You have now installed the Kubernetes components on your bare metal Ubuntu machines.</p>
<h2 id="step-4---initialize-the-kubernetes-cluster-run-this-only-on-the-master-node"><a href="#step-4-initialize-the-kubernetes-cluster-run-this-only-on-the-master-node">Step 4 - Initialize the Kubernetes Cluster (Run this only on the master node)</a><a href="#step-4-initialize-the-kubernetes-cluster-run-this-only-on-the-master-node"></a></h2>
<p>On the master node, initialize the Kubernetes cluster using the following command:</p>
<pre tabindex="0"><code>sudo kubeadm init --pod-network-cidr=192.168.0.0/16
</code></pre><p><strong>Note:</strong> If you notice the following error while initializing the k8s cluster using the above command on the master node:</p>
<pre tabindex="0"><code>I0227 09:37:20.755567    4052 version.go:256] remote version is much newer: v1.32.2; falling back to: stable-1.29
[init] Using Kubernetes version: v1.29.14
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
</code></pre><p>The error message indicates that the <code>ip_forward</code> setting in your system is not enabled. This setting is necessary for Kubernetes to allow network traffic to be forwarded between pods and nodes. To fix this error, you need to enable IP forwarding.</p>
<p><strong>Enable IP Forwarding Temporarily:</strong></p>
<pre tabindex="0"><code>sudo sysctl -w net.ipv4.ip_forward=1
</code></pre><p>This command will initialize the Kubernetes control plane and generate a <code>kubeconfig</code> file. It will also print the next steps and the command to join the worker nodes to the cluster.</p>
<p>After the initialization is complete, you will see a message that looks like this:</p>
<pre tabindex="0"><code>Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/networking/

&lt;^&gt;Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 192.168.0.100:6443 --token 9vz3zv.3x3z3z3z3z3z3z3z \&lt;^&gt;
</code></pre><p><strong>Note:</strong> In the above output of a successful Kubernetes control plane deployment, please make a note of <code>kubeadm join</code> command and the <code>--token</code> and <code>--discovery-token-ca-cert-hash sha256</code> values as you will need it in the upcoming steps when joining worker nodes to the Kubernetes cluster.</p>
<p>To start using your cluster, you need to run the following as a regular user:</p>
<pre tabindex="0"><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre><p>Alternatively, if you are the <code>root</code> user, you can run:</p>
<pre tabindex="0"><code>export KUBECONFIG=/etc/kubernetes/admin.conf
</code></pre><p>You should now deploy a pod network to the cluster.</p>
<h2 id="step-5---open-kubernetes-ports-and-deploy-a-pod-network"><a href="#step-5-open-kubernetes-ports-and-deploy-a-pod-network">Step 5 - Open Kubernetes Ports and Deploy a Pod Network</a><a href="#step-5-open-kubernetes-ports-and-deploy-a-pod-network"></a></h2>
<p>To deploy a pod network to your Kubernetes cluster, you can use <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">network plugins</a> like Calico, Flannel, or Weave. Here, you will use Flannel.</p>
<p>This command should be run on the <strong>master node.</strong></p>
<h3 id="open-kubernetes-ports"><a href="#open-kubernetes-ports">Open Kubernetes Ports</a><a href="#open-kubernetes-ports"></a></h3>
<p>Before installing the Pod Network, you need to ensure that the required Kubernetes ports are open. These ports are used by various Kubernetes services:</p>
<ul>
<li><code>Port 6443</code>: This is the default secure port for the <strong><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">Kubernetes API server</a></strong>.</li>
<li><code>Port 10250</code>: This is the default port for the <strong><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">Kubelet API server</a>.</strong></li>
<li><code>Ports 2379-2380</code>: These ports are used by the <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/"><code>etcd</code> server</a>.</li>
</ul>
<p>To open these ports, you can use the following commands:</p>
<pre tabindex="0"><code>sudo ufw allow 6443/tcp
sudo ufw allow 10250/tcp
sudo ufw allow 2379:2380/tcp
</code></pre><h3 id="deploy-pod-network"><a href="#deploy-pod-network">Deploy Pod Network</a><a href="#deploy-pod-network"></a></h3>
<p>Now, letâ€™s install the Pod network using the <a href="https://github.com/flannel-io/flannel#deploying-flannel-manually">Flannel</a> network plugin.</p>
<pre tabindex="0"><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre><p>A successful pod network deployment will give the below output:</p>
<pre tabindex="0"><code>Outputnamespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
</code></pre><p>This command will deploy the <a href="https://github.com/flannel-io/flannel">Flannel</a> pod network to your cluster. You can verify the deployment by running:</p>
<pre tabindex="0"><code>kubectl get pods --all-namespaces
</code></pre><p>You should see the <strong>Flannel</strong> pods running in the <code>kube-system</code> namespace.</p>
<pre tabindex="0"><code>Outputkube-flannel   kube-flannel-ds-bs6df                       1/1     Running             1 (82s ago)      94s
kube-system    coredns-76f75df574-md5b2                    0/1     ContainerCreating   0                105s
kube-system    coredns-76f75df574-wdpsd                    0/1     ContainerCreating   0                105s
kube-system    etcd-master-node-anish                      1/1     Running             16 (2m48s ago)   2m49s
kube-system    kube-apiserver-master-node-anish            1/1     Running             21 (2m18s ago)   2m15s
kube-system    kube-controller-manager-master-node-anish   1/1     Running             1 (2m48s ago)    80s
kube-system    kube-proxy-vsr5m                            1/1     Running             3 (83s ago)      105s
kube-system    kube-scheduler-master-node-anish            1/1     Running             21 (2m48s ago)   2m50s
</code></pre><p>If you want to get your Kubernetees cluster details, you can use the following command:</p>
<pre tabindex="0"><code>kubectl cluster-info
</code></pre><p>This should give you the following output:</p>
<pre tabindex="0"><code>OutputKubernetes control plane is running at https://64.227.157.182:6443
CoreDNS is running at https://64.227.157.182:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
</code></pre><h2 id="step-6---join-worker-nodes"><a href="#step-6-join-worker-nodes">Step 6 - Join Worker Nodes</a><a href="#step-6-join-worker-nodes"></a></h2>
<p>To join the worker nodes to the Kubernetes cluster, you will need to run the following <code>kubectl join</code> command on each worker node.</p>
<pre tabindex="0"><code>sudo kubeadm join &lt;MASTER_NODE_IP&gt;:6443 --token &lt;TOKEN&gt; --discovery-token-ca-cert-hash sha256:&lt;HASH&gt;
</code></pre><p>Replace <code>&lt;MASTER_NODE_IP&gt;</code> with the IP address of your master node IP, <code>&lt;TOKEN&gt;</code> with the token generated during the master node setup, and <code>&lt;HASH&gt;</code> with the hash generated during the master node setup.</p>
<p>Once you have run this command on all worker nodes, you can verify that they have joined the cluster by running:</p>
<pre tabindex="0"><code>kubectl get nodes
</code></pre><p>You should see all worker nodes listed and in the <code>Ready</code> state.</p>
<pre tabindex="0"><code>Outputmaster-node-anish   Ready    control-plane   13m     v1.29.14
worker-node-anish   Ready    &lt;none&gt;          7m27s   v1.29.14
</code></pre><h2 id="step-7---deploy-a-sample-application"><a href="#step-7-deploy-a-sample-application">Step 7 - Deploy a Sample Application</a><a href="#step-7-deploy-a-sample-application"></a></h2>
<p>In this step, you will deploy a sample application, specifically an <code>nginx</code> server. This can be done using the following command:</p>
<pre tabindex="0"><code>kubectl create deployment nginx --image=nginx
</code></pre><p>This command will create a deployment named <code>nginx</code> and use the <code>nginx</code> image.</p>
<p>Next, we will expose the deployment <code>nginx</code> to the outside world on port <code>80</code>. The <code>--type=NodePort</code> flag specifies that the service should be exposed on a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">NodePort</a>.</p>
<pre tabindex="0"><code>kubectl expose deployment nginx --port=80 --type=NodePort
</code></pre><p>After running these commands, you can access the <code>nginx</code> server by using the IP address of any of your worker nodes and the NodePort that was assigned to the <code>nginx</code> service. You can find the NodePort by running the following command:</p>
<pre tabindex="0"><code>kubectl get svc
</code></pre><p>You should see the newly created <code>nginx</code> service listed, along with its NodePort.</p>
<pre tabindex="0"><code>OutputNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        18m
nginx        NodePort    10.111.19.83   &lt;none&gt;        80:32224/TCP   8s
</code></pre><h3 id="access-the-application"><a href="#access-the-application">Access the Application</a><a href="#access-the-application"></a></h3>
<p>To access the <code>nginx</code> service, you can use the IP address of any of your <strong>worker nodes</strong> and the <code>NodePort</code> that was assigned to the <code>nginx</code> service.</p>
<p>You can find the <code>NodePort</code> by running the following command:</p>
<pre tabindex="0"><code>kubectl get svc
</code></pre><p>You should see the newly created <code>nginx</code> service listed, along with its NodePort.</p>
<pre tabindex="0"><code>OutputNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        18m
nginx        NodePort    10.111.19.83   &lt;none&gt;        80:32224/TCP   8s
</code></pre><p>In this example, the NodePort for the <code>nginx</code> service is <code>32224</code>. You can access the <code>nginx</code> server by using the IP address of any of your worker nodes and the NodePort. For example, if the IP address of your worker node is <code>10.111.19.83</code>, you can access the <code>nginx</code> server at <code>http://10.111.19.83:32224</code>.</p>
<h2 id="step-8---monitoring-kubernetes-on-bare-metal"><a href="#step-8-monitoring-kubernetes-on-bare-metal">Step 8 - Monitoring Kubernetes on Bare Metal</a><a href="#step-8-monitoring-kubernetes-on-bare-metal"></a></h2>
<p>Kubernetes provides a rich set of monitoring tools and integrations. Some of the popular ones are:</p>
<ul>
<li><strong>Prometheus</strong>: A monitoring system and time series database.</li>
<li><strong>Grafana</strong>: A visualization tool that works with Prometheus to create and display dashboards.</li>
<li><strong>Kube-state-metrics</strong>: A service that listens to the Kubernetes API server and generates metrics about the state of the objects.</li>
</ul>
<p>To set up monitoring for your Kubernetes cluster, you can follow this tutorial on <a href="/community/tutorials/how-to-set-up-digitalocean-kubernetes-cluster-monitoring-with-helm-and-prometheus-operator">How to Set Up DigitalOcean Kubernetes Cluster Monitoring with Helm and Prometheus Operator</a>.</p>
<h2 id="faqs"><a href="#faqs">FAQs</a><a href="#faqs"></a></h2>
<h3 id="1-can-kubernetes-be-installed-on-bare-metal"><a href="#1-can-kubernetes-be-installed-on-bare-metal">1. Can Kubernetes be installed on bare metal?</a><a href="#1-can-kubernetes-be-installed-on-bare-metal"></a></h3>
<p>Yes, <a href="/products/kubernetes">Kubernetes</a> can be installed on physical servers without virtualization, providing direct access to hardware resources.</p>
<h3 id="2-what-is-the-simplest-way-to-deploy-kubernetes"><a href="#2-what-is-the-simplest-way-to-deploy-kubernetes">2. What is the simplest way to deploy Kubernetes?</a><a href="#2-what-is-the-simplest-way-to-deploy-kubernetes"></a></h3>
<p>The simplest way to deploy Kubernetes on bare metal is by using <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/"><code>kubeadm</code></a>, which automates the <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">cluster setup process</a>.</p>
<h3 id="3-is-it-possible-to-run-kubernetes-without-docker"><a href="#3-is-it-possible-to-run-kubernetes-without-docker">3. Is it possible to run Kubernetes without Docker?</a><a href="#3-is-it-possible-to-run-kubernetes-without-docker"></a></h3>
<p>Yes, <a href="/products/kubernetes">Kubernetes</a> supports other container runtimes like <code>containerd</code> and <code>CRI-O</code>. Docker is no longer required since Kubernetes deprecated Docker support in v1.20.</p>
<h3 id="4-is-docker-better-on-bare-metal-or-vm"><a href="#4-is-docker-better-on-bare-metal-or-vm">4. Is Docker better on bare metal or VM?</a><a href="#4-is-docker-better-on-bare-metal-or-vm"></a></h3>
<p>Docker performs better on bare metal because it eliminates the overhead of a hypervisor, allowing direct access to system resources. However, VMs provide better isolation and security.</p>
<h3 id="5-can-you-use-kubernetes-without-helm"><a href="#5-can-you-use-kubernetes-without-helm">5. Can you use Kubernetes without helm?</a><a href="#5-can-you-use-kubernetes-without-helm"></a></h3>
<p>Yes, you can manually deploy applications using Kubernetes manifests (<code>kubectl apply -f</code>). However, Helm simplifies package management and application deployment.</p>
<h3 id="6-what-is-bare-metal-kubernetes-and-why-use-it"><a href="#6-what-is-bare-metal-kubernetes-and-why-use-it">6. What is bare metal Kubernetes, and why use it?</a><a href="#6-what-is-bare-metal-kubernetes-and-why-use-it"></a></h3>
<p><a href="/products/bare-metal-gpu">Bare metal</a> Kubernetes refers to running Kubernetes directly on physical machines instead of virtualized environments. It is used for enhanced performance, reduced latency, and better resource efficiency, making it ideal for AI/ML and high-performance workloads.</p>
<h3 id="7-how-does-kubernetes-deployment-on-bare-metal-differ-from-cloud-setups"><a href="#7-how-does-kubernetes-deployment-on-bare-metal-differ-from-cloud-setups">7. How does Kubernetes deployment on bare metal differ from cloud setups?</a><a href="#7-how-does-kubernetes-deployment-on-bare-metal-differ-from-cloud-setups"></a></h3>
<p>In cloud setups, Kubernetes nodes run on virtual machines managed by a cloud provider. Bare metal deployments require manual setup and configuration, but offer greater flexibility, cost efficiency, and performance.</p>
<h3 id="8-what-tools-are-required-to-deploy-kubernetes-on-bare-metal"><a href="#8-what-tools-are-required-to-deploy-kubernetes-on-bare-metal">8. What tools are required to deploy Kubernetes on bare metal?</a><a href="#8-what-tools-are-required-to-deploy-kubernetes-on-bare-metal"></a></h3>
<p>You need:</p>
<ul>
<li>
<p><code>kubeadm</code>, <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"><code>kubelet</code></a>, and <a href="https://kubernetes.io/docs/reference/kubectl/kubectl/"><code>kubectl</code></a> for cluster setup and management.</p>
</li>
<li>
<p>A <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">container runtime</a> like <a href="https://containerd.io/">containerd</a> or <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o">CRI-O</a>.</p>
</li>
<li>
<p>A networking plugin such as <a href="https://www.tigera.io/project-calico/">Calico</a> or <a href="https://github.com/flannel-io/flannel#deploying-flannel-manually">Flannel</a>.</p>
</li>
<li>
<p>Load balancers like <a href="https://metallb.io/">MetalLB</a> for service exposure.</p>
</li>
</ul>
<h3 id="9-can-bare-metal-kubernetes-clusters-scale-like-cloud-environments"><a href="#9-can-bare-metal-kubernetes-clusters-scale-like-cloud-environments">9. Can bare metal Kubernetes clusters scale like cloud environments?</a><a href="#9-can-bare-metal-kubernetes-clusters-scale-like-cloud-environments"></a></h3>
<p>Yes, but scaling requires manual provisioning of additional nodes and network configurations, unlike cloud environments where resources are automatically allocated.</p>
<p>For additional guidance, check out <a href="/blog/digitalocean-doks-managed-kubernetes-networking">DigitalOcean DOKS Managed Kubernetes Networking</a>.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a><a href="#conclusion"></a></h2>
<p>In this comprehensive tutorial, you learned how to deploy Kubernetes on bare metal infrastructure. We covered the key aspects of setting up a Kubernetes cluster on physical machines, including the use of <code>kubeadm</code> for automation, the importance of container runtimes like <code>containerd</code> and <code>CRI-O</code>, and the role of networking plugins such as Calico and Flannel.</p>
<p>By following this tutorial, you should now have a solid understanding of how to deploy and manage a Kubernetes cluster on bare metal, taking advantage of the performance, control, and efficiency it offers.</p>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/bare-metal-kubernetes">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
