<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>PyTorch 101 Memory Management and Using Multiple GPUs</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	
</head>
<body>
	<header>
	==========================<br>
	== <a href="https://ghariib.ir/">Gharib Personal Blog</a> ==<br>
	==========================
	<div style="float: right;">A Techi Personal Blog</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		<article>
			<h1>PyTorch 101 Memory Management and Using Multiple GPUs</h1>
			<b><time>18.03.2025 09:33</time></b>
		       

			<div>
				<h1 id="pytorch-101-memory-management-and-using-multiple-gpus">PyTorch 101 Memory Management and Using Multiple GPUs</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>When working with deep learning models that use <a href="/community/tutorials/pytorch-101-understanding-graphs-and-automatic-differentiation">PyTorch</a>, efficiently managing GPUs can make a huge difference in performance. Whether you’re training large models or running complex computations, using multiple GPUs can significantly speed up the process. However, handling multiple GPUs properly requires understanding different parallelism techniques, automating GPU selection, and troubleshooting memory issues.</p>
<p>In this article, we’ll explore:</p>
<ul>
<li>How do you use multiple GPUs for your network, whether through data parallelism (splitting data across GPUs) or model parallelism (distributing model layers across GPUs)?</li>
<li>How to automate GPU selection so PyTorch assigns available GPUs to new objects.</li>
<li>How to diagnose and fix memory issues, ensuring smooth training and inference without running into out-of-memory errors.</li>
</ul>
<p>By the end of this guide, you will understand how to optimize GPU usage in PyTorch.</p>
<h2 id="prerequisites"><a href="#prerequisites">Prerequisites</a><a href="#prerequisites"></a></h2>
<p>Before diving into PyTorch 101: Memory Management and Using Multiple GPUs, ensure you have the following:</p>
<ul>
<li>Basic understanding of Python and <a href="/community/tutorials/pytorch-vs-tensorflow">PyTorch</a>.</li>
<li>PyTorch is installed on your system.</li>
<li>Access to a <a href="/community/tutorials/intro-to-cuda">CUDA-enabled GPU</a> or multiple GPUs for testing (optional but recommended).</li>
<li>Familiarity with GPU memory management concepts (optional but beneficial).</li>
<li>pip for installing any additional packages.</li>
</ul>
<h2 id="moving-tensors-around-cpu--gpus"><a href="#moving-tensors-around-cpu-gpus">Moving tensors around CPU / GPUs</a><a href="#moving-tensors-around-cpu-gpus"></a></h2>
<p>Every Tensor in PyTorch has a <code>to()</code> member function. It’s job is to put the tensor on which it’s called to a certain device whether it be the CPU or a certain GPU. Input to the <code>to</code> function is a <code>torch.device</code> object which can initialised with either of the following inputs.</p>
<ol>
<li><code>cpu</code> for CPU</li>
<li><code>cuda:0</code> for putting it on GPU number 0. Similarly, if you want to put the tensors on</li>
</ol>
<p>Generally, whenever you initialise a Tensor, it’s put on the CPU. You can move it to the GPU then. You can check whether a GPU is available or not by invoking the <code>torch.cuda.is_available</code> function.</p>
<pre tabindex="0"><code>if torch.cuda.is_available():
        dev = &#34;cuda:0&#34;
else:
        dev = &#34;cpu&#34;

device = torch.device(dev)

a = torch.zeros(4,3)   
a = a.to(device)       #alternatively, a.to(0)
</code></pre><p>You can also move a tensor to a certain GPU by giving it’s index as the argument to <code>to</code> function.</p>
<p>Importantly, the above piece of code is device agnostic, that is, you don’t have to separately change it for it to work on both <a href="/resources/articles/cpu-vs-gpu">GPU and the CPU</a>.</p>
<h2 id="cuda-function"><a href="#cuda-function">cuda() function</a><a href="#cuda-function"></a></h2>
<p>One way to transfer tensors to GPUs is by using the <code>cuda(n)</code> function, where <code>n</code> specifies the index of the GPU. If you call <code>cuda</code> without arguments, the tensor will be placed on GPU 0 by default.</p>
<p>Additionally, the <code>torch.nn.Module</code> class provides <code>to</code> and <code>cuda</code> methods that can move the entire neural network to a specific device. Unlike tensors, when you use the <code>to</code> method on an <code>nn.Module</code> object, it’s sufficient to call the function directly; you do not need to assign the returned value.</p>
<pre tabindex="0"><code>clf = myNetwork()
clf.to(torch.device(&#34;cuda:0&#34;)    # or clf = clf.cuda() 
</code></pre><h2 id="automatic-selection-of-gpu"><a href="#automatic-selection-of-gpu">Automatic selection of GPU</a><a href="#automatic-selection-of-gpu"></a></h2>
<p>It’s beneficial to explicitly choose which GPU a tensor is assigned to; however, we typically create many tensors during operations. We want these tensors to be automatically created on a specific device to minimize cross-device transfers that can slow down our code. PyTorch offers functionality to help us achieve this.</p>
<p>One useful function is <code>torch.get_device</code>. This function is only supported for GPU tensors and returns the index of the GPU on which a tensor is located. Using this function, we can determine the tensor device and automatically move any newly created tensor to that device.</p>
<pre tabindex="0"><code>#making sure t2 is on the same device as t2

a = t1.get_device()
b = torch.tensor(a.shape).to(dev)
</code></pre><p>We can also call <code>cuda(n)</code> while creating new Tensors. By default all tensors created by <code>cuda</code> call are put on GPU 0, but this can be changed by the following statement.</p>
<pre tabindex="0"><code>torch.cuda.set_device(0)   # or 1,2,3
</code></pre><p>If a tensor results from an operation between two operands on the same device, the resultant tensor will also be on that device. If the operands are on different devices, an error will occur.</p>
<h2 id="new_-functions"><a href="#new_-functions">new_* functions</a><a href="#new_-functions"></a></h2>
<p>One can also use the new functions that made their way to PyTorch in version 1.0. When a function like new_ones is called on a Tensor, it returns a new tensor of the same data type and on the same device as the tensor on which the new_ones function was invoked.</p>
<pre tabindex="0"><code>ones = torch.ones((2,)).cuda(0)

# Create a tensor of ones of size (3,4) on same device as of &#34;ones&#34;
newOnes = ones.new_ones((3,4)) 

randTensor = torch.randn(2,4)
</code></pre><p>A detailed list of <code>new_</code> functions can be found in <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a> docs.</p>
<h2 id="using-multiple-gpus"><a href="#using-multiple-gpus">Using Multiple GPUs</a><a href="#using-multiple-gpus"></a></h2>
<p>There are two ways we could make use of multiple GPUs.</p>
<ol>
<li><a href="/community/tutorials/parallel-computing-gpu-vs-cpu-with-cuda">Data Parallelism</a>, where we divide batches into smaller batches and process these smaller batches in parallel on multiple GPUs.</li>
<li>Model Parallelism involves breaking the neural network into smaller subnetworks and then executing these subnetworks on different GPUs.</li>
</ol>
<h2 id="data-parallelism"><a href="#data-parallelism">Data Parallelism</a><a href="#data-parallelism"></a></h2>
<p>Data Parallelism in PyTorch is achieved through the <code>nn.DataParallel</code> class. You initialize a nn.DataParallel object with a <code>nn.Module</code> object representing your network and a list of GPU IDs, across which the batches must be parallelized.</p>
<pre tabindex="0"><code>parallel_net = nn.DataParallel(myNet, gpu_ids = [0,1,2])
</code></pre><p>Now, you can simply execute the <code>nn.DataParallel</code> object just like a <code>nn.Module</code> .</p>
<pre tabindex="0"><code>predictions = parallel_net(inputs)    # Forward pass on multi-GPUs
loss = loss_function(predictions, labels)     # Compute loss function
loss.mean().backward()                        # Average GPU-losses + backward pass
optimizer.step()            
</code></pre><p>However, there are a few things I want to clarify. Although our data has to be parallelized over multiple GPUs, we have to initially store it on a single GPU.</p>
<p>We also need to make sure the DataParallel object is on that particular GPU. The syntax remains similar to what we did earlier with nn.Module.</p>
<pre tabindex="0"><code>input = input.to(0)
parallel_net = parellel_net.to(0)
</code></pre><p>In effect, the following diagram describes how <code>nn.DataParallel</code> works.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2019/04/image-4.png" alt="image" />
</figure>


</p>
<p>Working of nn.DataParallel. <a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">Source</a></p>
<p><code>DataParallel</code> takes the input, splits it into smaller batches, replicates the neural network across all the devices, executes the pass and then collects the output back on the original GPU.</p>
<p>One issue with <code>DataParallel</code> can be that it can put asymmetrical load on one GPU (the main node). There are generally two ways to circumvent these problem.</p>
<ol>
<li>First, is to compute the loss during the forward pass. This makes sure at least the loss calculation phase is parallelised.</li>
<li>Another way is to implement a parallel loss function layer. This is beyond the scope of this article. However, for those interested I have given a link to a medium article detailing implementation of such a layer at the end of this article.</li>
</ol>
<h2 id="model-parallelism"><a href="#model-parallelism">Model Parallelism</a><a href="#model-parallelism"></a></h2>
<p>Model parallelism means breaking your network into smaller subnetworks that you put on different GPUs. The main motivation is that your network might be too large to fit inside a single GPU.</p>
<p>Note that model parallelism is often slower than data parallelism. Splitting a single network into multiple GPUs introduces dependencies between GPUs, which prevents them from running in a truly parallel way. The advantage one derives from model parallelism is not about speed but about the ability to run networks whose size is too large to fit on a single GPU.</p>
<p>As Figure B shows, Subnet 2 waits for Subnet 1 during the forward pass, while Subnet 1 waits for Subnet 2 during the backward pass.</p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2019/04/model_parallel_steps-1.png" alt="image" />
</figure>


</p>
<p>Model Parallelism with Dependencies Implementing Model parallelism in PyTorch is pretty easy as long as you remember two things.</p>
<ol>
<li>The input and the network should always be on the same device.</li>
<li><code>to</code> and <code>cuda</code> functions have autograd support, so your gradients can be copied from one GPU to another during backward pass. We will use the following piece of code to understand this better.</li>
</ol>
<pre tabindex="0"><code>class model_parallel(nn.Module):
        def __init__(self):
                super().__init__()
                self.sub_network1 = ...
                self.sub_network2 = ...
                self.sub_network1.cuda(0)
                self.sub_network2.cuda(1)

        def forward(x):
                x = x.cuda(0)
                x = self.sub_network1(x)
                x = x.cuda(1)
                x = self.sub_network2(x)
                return x
</code></pre><p>In the <code>init</code> function we have put the sub-networks on GPUs 0 and 1 respectively.</p>
<p>Notice in the <code>forward</code> function, we transfer the intermediate output from <code>sub_network1</code> to GPU 1 before feeding it to <code>sub_network2</code>. Since <code>cuda</code> has autograd support, the loss backpropagated from <code>sub_network2</code> will be copied to buffers of <code>sub_network1</code> for further backpropagation.</p>
<h2 id="troubleshooting-out-of-memory-errors"><a href="#troubleshooting-out-of-memory-errors">Troubleshooting Out of Memory Errors</a><a href="#troubleshooting-out-of-memory-errors"></a></h2>
<p>This section will discuss how to diagnose memory issues and explore potential solutions if your network is utilizing more memory than necessary.</p>
<p>While going out of memory may necessitate reducing batch size, one can do certain checks to ensure that memory usage is optimal.</p>
<h2 id="tracking-memory-usage-with-gputil"><a href="#tracking-memory-usage-with-gputil">Tracking Memory Usage with GPUtil</a><a href="#tracking-memory-usage-with-gputil"></a></h2>
<p>One way to track GPU usage is by monitoring memory usage in a console with the nvidia-smi command. The problem with this approach is that peak <a href="/community/tutorials/monitoring-gpu-utilization-in-real-time">GPU</a> usage and out-of-memory happen so fast that you can’t quite pinpoint which part of your code is causing the memory overflow.</p>
<p>For this, we will use an extension called GPUtil, which you can install with pip by running the following command.</p>
<pre tabindex="0"><code>pip install GPUtil
</code></pre><p>The usage is pretty simple.</p>
<pre tabindex="0"><code>import GPUtil
GPUtil.showUtilization()
</code></pre><p>Just put the second line wherever you want to see the GPU Utilization. By placing this statement in different places in the code, you can figure out what part is causing the network to go OOM.</p>
<h2 id="dealing-with-memory-losses-using-del-keyword"><a href="#dealing-with-memory-losses-using-del-keyword">Dealing with Memory Losses using del keyword</a><a href="#dealing-with-memory-losses-using-del-keyword"></a></h2>
<p>PyTorch has a pretty aggressive garbage collector. The garbage collection will free it as soon as a variable goes out of scope.</p>
<p>It’s important to note that Python does not enforce scoping rules as strictly as languages like C or C++. A variable remains in memory as long as references (or pointers) exist. This behavior is influenced by the fact that variables in Python do not need to be explicitly declared.</p>
<p>As a result, memory occupied by tensors holding your <code>input</code>, <code>output</code> tensors can still not be freed even once you are out of the training loop. Consider the following chunk of code.</p>
<pre tabindex="0"><code>for x in range(10):
        i = x

print(i)   # 9 is printed
</code></pre><p>When executing the code snippet above, you’ll notice that the value of ‘i’ persists even after we exit the loop where it was initialized. Similarly, the tensors that store loss and output can remain in memory beyond the training loop. To properly release the memory occupied by these tensors, we should use the ‘del’ keyword.</p>
<pre tabindex="0"><code>del out, loss
</code></pre><p>In fact, as a general rule of thumb, if you are done with a tensor, you should del as it won’t be garbage collected unless there is no reference to it left.</p>
<h2 id="using-python-data-types-instead-of-1-d-tensors"><a href="#using-python-data-types-instead-of-1-d-tensors">Using Python Data Types Instead of 1-D Tensors</a><a href="#using-python-data-types-instead-of-1-d-tensors"></a></h2>
<p>In our training loop, we frequently aggregate values to calculate metrics, with the most common example being the update of the running loss during each iteration. However, if this is not done carefully in PyTorch, it can result in excessive memory usage beyond what is necessary.</p>
<p>Consider the following snippet of code.</p>
<pre tabindex="0"><code>total_loss = 0

for x in range(10):
  # assume loss is computed 
        iter_loss = torch.randn(3,4).mean()
        iter_loss.requires_grad = True     # losses are supposed to differentiable
        total_loss += iter_loss            # use total_loss += iter_loss.item) instead
</code></pre><p>We expect that in subsequent iterations, the reference to <code>iter_loss</code> will be reassigned to the new <code>iter_loss,</code> and the object representing <code>iter_loss</code> from the earlier representation will be freed. However, this doesn’t happen.</p>
<p>Why is that?</p>
<p>Since <code>iter_loss</code> is differentiable, the line <code>total_loss += iter_loss</code> creates a computation graph that includes an <code>AddBackward</code> function node. During subsequent iterations, additional <code>AddBackward</code> nodes are added to this graph, and no objects holding the values of <code>iter_loss</code> are released. Typically, the memory allocated for a computation graph is freed when <code>backward</code> is called on it; however, in this case, there is no opportunity to call <code>backward.</code></p>
<p>
<figure>
  <img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2019/04/loss_add_graph-1.png" alt="image" />
</figure>


</p>
<p>The computation graph is formed when you keep adding the loss tensor to the variable <code>total_loss.</code> To prevent the creation of any computation graph, the solution is to add a Python data type instead of a tensor to <code>total_loss.</code> We merely replace the line total_loss += iter_loss with total_loss += iter_loss.item(). item returns the Python data type from a tensor containing single values.</p>
<h2 id="emptying-cuda-cache"><a href="#emptying-cuda-cache">Emptying Cuda Cache</a><a href="#emptying-cuda-cache"></a></h2>
<p>While PyTorch efficiently manages memory usage, it may not return memory to the operating system (OS) even after you delete your tensors. Instead, this memory is cached to facilitate the quick allocation of new tensors without requesting additional memory from the OS.</p>
<p>This behavior can pose a problem when using more than two processes in your workflow. If the first process completes its tasks but retains the GPU memory, it can lead to out-of-memory (OOM) errors when the second process is initiated.</p>
<p>To address this issue, you can include a specific command at the end of your code to ensure that memory is released properly.</p>
<pre tabindex="0"><code>torch.cuda.empy_cache()
</code></pre><p>This will make sure that the space held by the process is released.</p>
<pre tabindex="0"><code>import torch
from GPUtil import showUtilization as gpu_usage

print(&#34;Initial GPU Usage&#34;)
gpu_usage()                             

tensorList = []
for x in range(10):
        tensorList.append(torch.randn(10000000,10).cuda())   # reduce the size of tensor if you are getting OOM
  
  

print(&#34;GPU Usage after allcoating a bunch of Tensors&#34;)
gpu_usage()

del tensorList

print(&#34;GPU Usage after deleting the Tensors&#34;)
gpu_usage()  

print(&#34;GPU Usage after emptying the cache&#34;)
torch.cuda.empty_cache()
gpu_usage()
</code></pre><p>The following output is produced when this code is executed on a Tesla K80</p>
<pre tabindex="0"><code>Initial GPU Usage
| ID | GPU | MEM |
------------------
|  0 |  0% |  5% |
GPU Usage after allcoating a bunch of Tensors
| ID | GPU | MEM |
------------------
|  0 |  3% | 30% |
GPU Usage after deleting the Tensors
| ID | GPU | MEM |
------------------
|  0 |  3% | 30% |
GPU Usage after emptying the cache
| ID | GPU | MEM |
------------------
|  0 |  3% |  5% |
</code></pre><h2 id="using-torchno_grad-for-inference"><a href="#using-torch-no_grad-for-inference">Using torch.no_grad() for Inference</a><a href="#using-torch-no_grad-for-inference"></a></h2>
<p>By default, PyTorch creates a computational graph during the forward pass. This process involves allocating buffers to store gradients and intermediate values necessary for computing the gradient during the backward pass.</p>
<p>During the backward pass, most of these buffers are freed, except for those allocated for leaf variables. However, during inference, since there is no backward pass, these buffers remain allocated, which can lead to excessive memory usage over time.</p>
<p>To prevent this memory buildup when executing code that does not require backpropagation, always place it within a <code>torch.no_grad()</code> context manager.</p>
<pre tabindex="0"><code>with torch.no_grad()
# your code 
</code></pre><h2 id="using-cudnn-backend"><a href="#using-cudnn-backend">Using CuDNN Backend</a><a href="#using-cudnn-backend"></a></h2>
<p>You can utilize the <code>cuDNN</code> benchmark instead of the standard benchmark. <a href="/community/tutorials/install-cuda-cudnn-for-gpu">CuDNN</a> offers significant optimizations that can reduce your memory usage, especially when the input size to your neural network is fixed. To enable the cuDNN benchmark, add the following lines at the beginning of your code.</p>
<pre tabindex="0"><code>torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True
</code></pre><h2 id="using-16-bit-floats"><a href="#using-16-bit-floats">Using 16-bit Floats</a><a href="#using-16-bit-floats"></a></h2>
<p>The new RTX and Volta cards by nVidia support both 16-bit training and inference.</p>
<pre tabindex="0"><code>model = model.half()    # convert a model to 16-bit
input = input.half()    # convert a model to 16-bit
</code></pre><p>However, the 16-bit training options must be taken with a pinch of salt.</p>
<p>While 16-bit tensors can cut your GPU usage by almost half, they have a few issues.</p>
<p>In PyTorch, batch normalization layers can have convergence issues when using half-precision floats. If you encounter this problem, ensure that the batch normalization layers are set to <code>float32.</code></p>
<pre tabindex="0"><code>model.half()  # convert to half precision
for layer in model.modules():
        if isinstance(layer, nn.BatchNorm2d):
                layer.float()
</code></pre><p>Also, you need to make sure when the output is passed through different layers in the <code>forward</code> function, the input to the batch norm layer is converted from <code>float16</code> to <code>float32</code> and then the output needs to be converted back to <code>float16</code></p>
<p>One can find a good discussion of 16-bit training in PyTorch <a href="https://discuss.pytorch.org/t/training-with-half-precision/11815">here</a>.</p>
<p>Additionally, it is important to ensure that when the output is processed through various layers in the forward function, the input to the batch normalization layer is converted from float16 to float32. After processing, the output should be converted back to float16.</p>
<p>Be cautious of overflow issues when using 16-bit floats. I once encountered an overflow while trying to store the union area of two bounding boxes (for calculating Intersection over Union, or IoU) in a <code>float16.</code> It’s important to ensure that the value you intend to save in a float16 is realistic. Nvidia has recently released a <a href="https://github.com/NVIDIA/apex">PyTorch extension called Apex</a> that facilitates numerically safe mixed precision training in PyTorch. I have provided the link to that at the end of the article.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a><a href="#conclusion"></a></h2>
<p>Managing memory efficiently in PyTorch, especially with multiple GPUs, is crucial for getting the best performance out of deep learning models. Understanding memory allocation, caching, and gradient accumulation can prevent out-of-memory errors and make the most of your hardware.</p>
<p>When scaling up to multiple GPUs, using <code>DataParallel</code> or <code>DistributedDataParallel</code> helps distribute workloads effectively. However, each approach has trade-offs, so choosing the right one depends on your use case. Keeping an eye on memory usage with tools like <code>torch.cuda.memory_summary()</code> and clearing unused tensors with <code>torch.cuda.empty_cache()</code> can go a long way in avoiding performance bottlenecks. Ultimately, optimizing memory in PyTorch isn’t just about freeing up space—it’s about ensuring smooth, efficient training so your models run faster and more reliably. With the right practices, you can push the limits of your hardware while keeping things stable and efficient.</p>
<h2 id="further-reading"><a href="#further-reading">Further Reading</a><a href="#further-reading"></a></h2>
<ol>
<li><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.new_tensor">PyTorch <code>new</code> functions</a></li>
<li>Parallelised Loss Layer: <a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups</a></li>
<li><a href="https://github.com/anderskm/gputil">GPUtil Github page</a></li>
<li><a href="https://discuss.pytorch.org/t/training-with-half-precision/11815">A discussion on half precision training in PyTorch</a></li>
<li><a href="https://github.com/nvidia/apex">Nvidia Apex Github page</a></li>
<li><a href="https://medium.com/the-artificial-impostor/use-nvidia-apex-for-easy-mixed-precision-training-in-pytorch-46841c6eed8c">Nvidia Apex tutorial</a></li>
</ol>
<h4 id="source"><a href="https://www.digitalocean.com/community/tutorials/pytorch-memory-multi-gpu-debugging">Source</a></h4>
<!-- raw HTML omitted -->

			</div>
		</article>
	</main>
<aside>
	<div>
		<div>
			<h3>LATEST POSTS</h3>
		</div>
		<div>
			<ul>
				
				<li><a href="/posts/2025-03-20-laser-harp-sets-the-tone/">Laser Harp Sets the Tone</a></li>
				
				<li><a href="/posts/2025-03-20-arduino-device-helps-split-the-g-on-a-p/">Arduino device helps split the G on a pint of Guinness</a></li>
				
				<li><a href="/posts/2025-03-20-the-70-best-early-amazon-spring-sale-ga/">The 70 best early Amazon Spring Sale gaming deals 2025</a></li>
				
				<li><a href="/posts/2025-03-20-tomorrow-and-tomorrow-and-tomorrow-info/">Tomorrow and tomorrow and tomorrow Information security and the Baseball Hall of Fame</a></li>
				
				<li><a href="/posts/2025-03-20-i-found-an-android-phone-that-can-convi/">I found an Android phone that can convince iPhone users to make the switch - and its not a flagship</a></li>
				
			</ul>
		</div>
	</div>
</aside>


	<footer>
	<p>&copy; 2025 <a href="https://ghariib.ir/"><b>Alireza Gharib. All right reserved</b></a>.
	<a href="https://github.com/Gharib110"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
